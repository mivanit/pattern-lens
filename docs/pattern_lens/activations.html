<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 16.0.0"/>
    <title>pattern_lens.activations API documentation</title>
<link rel="stylesheet" href="../resources/css/bootstrap-reboot.min.css"><link rel="stylesheet" href="../resources/css/syntax-highlighting.css"><link rel="stylesheet" href="../resources/css/theme.css"><link rel="stylesheet" href="../resources/css/layout.css"><link rel="stylesheet" href="../resources/css/content.css"><link rel="stylesheet" href="../resources/css/custom.css"><script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    /* Re-invoke MathJax when DOM content changes, for example during search. */
    document.addEventListener("DOMContentLoaded", () => {
        new MutationObserver(() => MathJax.typeset()).observe(
            document.querySelector("main.pdoc").parentNode,
            {childList: true}
        );
    })
</script>
<style>
    mjx-container {
        overflow-x: auto;
        overflow-y: hidden;
    }
</style><style>
    .pdoc .mermaid-pre {
        border: none;
        background: none;
    }
</style>
<script type="module" defer>
    import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs";

    /* Re-invoke Mermaid when DOM content changes, for example during search. */
    document.addEventListener("DOMContentLoaded", () => {
        new MutationObserver(() => mermaid.run()).observe(
            document.querySelector("main.pdoc").parentNode,
            {childList: true}
        );
    })
</script></head>
<body>
<div class="package-version">
    docs for <a href="https://github.com/mivanit/pattern-lens">pattern_lens</a> v0.6.0<br>
</div>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button">
            <img src="../resources/svg/navtoggle.svg" alt="Toggle navigation"> 
        </label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../pattern_lens.html">
                <img src="../resources/svg/box-arrow-in-left.svg" alt="Back to parent module"/>
                &nbsp;pattern_lens</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>

            <h2>Contents</h2>
            <ul>
  <li><a href="#usage">Usage:</a></li>
</ul>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="function" href="#compute_activations">compute_activations</a>
            </li>
            <li>
                    <a class="function" href="#compute_activations_batched">compute_activations_batched</a>
            </li>
            <li>
                    <a class="function" href="#get_activations">get_activations</a>
            </li>
            <li>
                    <a class="variable" href="#DEFAULT_DEVICE">DEFAULT_DEVICE</a>
            </li>
            <li>
                    <a class="function" href="#activations_main">activations_main</a>
            </li>
            <li>
                    <a class="function" href="#main">main</a>
            </li>
    </ul>


    <hr/>
    
    <div>
        <a href="../coverage/html/index.html" class="pdoc-button" title="View test coverage report">
            Coverage
        </a>
        <a href="../other/todo-inline.html" class="pdoc-button" title="Table of TODOs scraped from source code, with links to create issues from them">
            TODOs
        </a>
        <a href="../other/lmcat.txt" class="pdoc-button" title="a view of the repo contents made for LLMs, using https://miv.name/lmcat">
            lmcat
        </a>
    </div>


        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span>
            <img src="../resources/svg/pdoc-logo.svg" alt="pdoc logo"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                        <a class="pdoc-button git-button" href="https://github.com/mivanit/pattern-lens/blob/0.6.0activations.py">View Source on GitHub</a>
                    <h1 class="modulename">
<a href="./../pattern_lens.html">pattern_lens</a><wbr>.activations    </h1>

                        <div class="docstring"><p>computing and saving activations given a model and prompts</p>

<h1 id="usage">Usage:</h1>

<p>from the command line:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>python<span class="w"> </span>-m<span class="w"> </span><a href="">pattern_lens.activations</a><span class="w"> </span>--model<span class="w"> </span>&lt;model_name&gt;<span class="w"> </span>--prompts<span class="w"> </span>&lt;prompts_path&gt;<span class="w"> </span>--save-path<span class="w"> </span>&lt;save_path&gt;<span class="w"> </span>--min-chars<span class="w"> </span>&lt;min_chars&gt;<span class="w"> </span>--max-chars<span class="w"> </span>&lt;max_chars&gt;<span class="w"> </span>--n-samples<span class="w"> </span>&lt;n_samples&gt;
</code></pre>
</div>

<p>from a script:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn"><a href="">pattern_lens.activations</a></span><span class="w"> </span><span class="kn">import</span> <span class="n">activations_main</span>
<span class="n">activations_main</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">,</span>
        <span class="n">save_path</span><span class="o">=</span><span class="s2">&quot;demo/&quot;</span>
        <span class="n">prompts_path</span><span class="o">=</span><span class="s2">&quot;data/pile_1k.jsonl&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre>
</div>
</div>

                        <input id="mod-activations-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <div class="source-button-container">
            <label class="pdoc-button view-source-button" for="mod-activations-view-source"><span>View Source</span></label>
            <div class="github-button-wrapper">
                <a class="pdoc-button github-link-button" href="https://github.com/mivanit/pattern-lens/blob/0.6.0activations.py#L0-L887" target="_blank">
                    <span>View on GitHub</span>
                </a>
            </div>
        </div>

                <br/>
                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="sd">&quot;&quot;&quot;computing and saving activations given a model and prompts</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="sd"># Usage:</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="sd">from the command line:</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="sd">```bash</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="sd">python -m pattern_lens.activations --model &lt;model_name&gt; --prompts &lt;prompts_path&gt; --save-path &lt;save_path&gt; --min-chars &lt;min_chars&gt; --max-chars &lt;max_chars&gt; --n-samples &lt;n_samples&gt;</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="sd">```</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="sd">from a script:</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a><span class="sd">```python</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="sd">from pattern_lens.activations import activations_main</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="sd">activations_main(</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="sd">	model_name=&quot;gpt2&quot;,</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="sd">	save_path=&quot;demo/&quot;</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="sd">	prompts_path=&quot;data/pile_1k.jsonl&quot;,</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a><span class="sd">)</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="sd">```</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">asdict</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">overload</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a><span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">jaxtyping</span><span class="w"> </span><span class="kn">import</span> <span class="n">Float</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">muutils.json_serialize</span><span class="w"> </span><span class="kn">import</span> <span class="n">json_serialize</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">muutils.misc.numerical</span><span class="w"> </span><span class="kn">import</span> <span class="n">shorten_numerical_to_str</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a><span class="c1"># custom utils</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">muutils.spinner</span><span class="w"> </span><span class="kn">import</span> <span class="n">SpinnerContext</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformer_lens</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore[import-untyped]</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a>	<span class="n">ActivationCache</span><span class="p">,</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a>	<span class="n">HookedTransformer</span><span class="p">,</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a>	<span class="n">HookedTransformerConfig</span><span class="p">,</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="p">)</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="c1"># pattern_lens</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">pattern_lens.consts</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a>	<span class="n">ATTN_PATTERN_REGEX</span><span class="p">,</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a>	<span class="n">DATA_DIR</span><span class="p">,</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a>	<span class="n">DIVIDER_S1</span><span class="p">,</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a>	<span class="n">DIVIDER_S2</span><span class="p">,</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a>	<span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a>	<span class="n">ActivationCacheNp</span><span class="p">,</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a>	<span class="n">ReturnCache</span><span class="p">,</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a><span class="p">)</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">pattern_lens.indexes</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a>	<span class="n">generate_models_jsonl</span><span class="p">,</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a>	<span class="n">generate_prompts_jsonl</span><span class="p">,</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a>	<span class="n">write_html_index</span><span class="p">,</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="p">)</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">pattern_lens.load_activations</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a>	<span class="n">ActivationsMissingError</span><span class="p">,</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a>	<span class="n">activations_exist</span><span class="p">,</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a>	<span class="n">augment_prompt_with_hash</span><span class="p">,</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a>	<span class="n">load_activations</span><span class="p">,</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a><span class="p">)</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">pattern_lens.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_text_data</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a><span class="k">def</span><span class="w"> </span><span class="nf">_rel_path</span><span class="p">(</span><span class="n">p</span><span class="p">:</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a><span class="w">	</span><span class="sd">&quot;&quot;&quot;Return path relative to cwd if possible, otherwise absolute.&quot;&quot;&quot;</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a>	<span class="k">try</span><span class="p">:</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>		<span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">relative_to</span><span class="p">(</span><span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">())</span><span class="o">.</span><span class="n">as_posix</span><span class="p">()</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>	<span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>		<span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">as_posix</span><span class="p">()</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a><span class="c1"># return nothing, but `stack_heads` still affects how we save the activations</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a><span class="nd">@overload</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_activations</span><span class="p">(</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a>	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">ATTN_PATTERN_REGEX</span><span class="p">,</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a>	<span class="n">stack_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span> <span class="o">...</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a><span class="c1"># return stacked heads in numpy or torch form</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a><span class="nd">@overload</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_activations</span><span class="p">(</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a>	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">ATTN_PATTERN_REGEX</span><span class="p">,</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;torch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span><span class="p">,</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>	<span class="n">stack_heads</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;n_layers n_heads n_ctx n_ctx&quot;</span><span class="p">]]:</span> <span class="o">...</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a><span class="nd">@overload</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_activations</span><span class="p">(</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a>	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">ATTN_PATTERN_REGEX</span><span class="p">,</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;numpy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;numpy&quot;</span><span class="p">,</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a>	<span class="n">stack_heads</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="n">Float</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s2">&quot;n_layers n_heads n_ctx n_ctx&quot;</span><span class="p">]]:</span> <span class="o">...</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a><span class="c1"># return dicts in numpy or torch form</span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a><span class="nd">@overload</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_activations</span><span class="p">(</span>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a>	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">ATTN_PATTERN_REGEX</span><span class="p">,</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;numpy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;numpy&quot;</span><span class="p">,</span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a>	<span class="n">stack_heads</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="n">ActivationCacheNp</span><span class="p">]:</span> <span class="o">...</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a><span class="nd">@overload</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_activations</span><span class="p">(</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a>	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">ATTN_PATTERN_REGEX</span><span class="p">,</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;torch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span><span class="p">,</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a>	<span class="n">stack_heads</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="n">ActivationCache</span><span class="p">]:</span> <span class="o">...</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a><span class="c1"># actual function body</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_activations</span><span class="p">(</span>  <span class="c1"># noqa: PLR0915</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a>	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">ATTN_PATTERN_REGEX</span><span class="p">,</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="n">ReturnCache</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span><span class="p">,</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a>	<span class="n">stack_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a>	<span class="n">Path</span><span class="p">,</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>	<span class="n">ActivationCacheNp</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>	<span class="o">|</span> <span class="n">ActivationCache</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>	<span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s2">&quot;n_layers n_heads n_ctx n_ctx&quot;</span><span class="p">]</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>	<span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;n_layers n_heads n_ctx n_ctx&quot;</span><span class="p">]</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>	<span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a><span class="p">]:</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a><span class="w">	</span><span class="sd">&quot;&quot;&quot;compute activations for a single prompt and save to disk</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a><span class="sd">	always runs a forward pass -- does NOT load from disk cache.</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a><span class="sd">	for cache-aware loading, use `get_activations` which tries disk first.</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a><span class="sd">	# Parameters:</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a><span class="sd">	- `prompt : dict | None`</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a><span class="sd">		(defaults to `None`)</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a><span class="sd">	- `model : HookedTransformer`</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a><span class="sd">	- `save_path : Path`</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a><span class="sd">		(defaults to `Path(DATA_DIR)`)</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a><span class="sd">	- `names_filter : Callable[[str], bool]|re.Pattern`</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a><span class="sd">		a filter for the names of the activations to return. if an `re.Pattern`, will use `lambda key: names_filter.match(key) is not None`</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a><span class="sd">		(defaults to `ATTN_PATTERN_REGEX`)</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a><span class="sd">	- `return_cache : Literal[None, &quot;numpy&quot;, &quot;torch&quot;]`</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a><span class="sd">		will return `None` as the second element if `None`, otherwise will return the cache in the specified tensor format. `stack_heads` still affects whether it will be a dict (False) or a single tensor (True)</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a><span class="sd">		(defaults to `None`)</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a><span class="sd">	- `stack_heads : bool`</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a><span class="sd">		whether the heads should be stacked in the output. this causes a number of changes:</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a><span class="sd">	- `npy` file with a single `(n_layers, n_heads, n_ctx, n_ctx)` tensor saved for each prompt instead of `npz` file with dict by layer</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a><span class="sd">	- `cache` will be a single `(n_layers, n_heads, n_ctx, n_ctx)` tensor instead of a dict by layer if `return_cache` is `True`</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a><span class="sd">		will assert that everything in the activation cache is only attention patterns, and is all of the attention patterns. raises an exception if not.</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a><span class="sd">	# Returns:</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a><span class="sd">	```</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a><span class="sd">	tuple[</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a><span class="sd">		Path,</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a><span class="sd">		Union[</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a><span class="sd">			None,</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a><span class="sd">			ActivationCacheNp, ActivationCache,</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a><span class="sd">			Float[np.ndarray, &quot;n_layers n_heads n_ctx n_ctx&quot;], Float[torch.Tensor, &quot;n_layers n_heads n_ctx n_ctx&quot;],</span>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a><span class="sd">		]</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a><span class="sd">	]</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a><span class="sd">	```</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a><span class="sd">	&quot;&quot;&quot;</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a>	<span class="c1"># check inputs</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a>	<span class="k">assert</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;model must be passed&quot;</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a>	<span class="k">assert</span> <span class="s2">&quot;text&quot;</span> <span class="ow">in</span> <span class="n">prompt</span><span class="p">,</span> <span class="s2">&quot;prompt must contain &#39;text&#39; key&quot;</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a>	<span class="n">prompt_str</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a>	<span class="c1"># compute or get prompt metadata</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>	<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a>	<span class="n">prompt_tokenized</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a>		<span class="s2">&quot;tokens&quot;</span><span class="p">,</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">prompt_str</span><span class="p">),</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a>	<span class="p">)</span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a>	<span class="c1"># n_tokens counts subword tokens (no BOS); attention patterns include BOS</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a>	<span class="c1"># so have dim n_tokens+1. see also compute_activations_batched Phase B.</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a>	<span class="n">prompt</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a>		<span class="nb">dict</span><span class="p">(</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>			<span class="n">n_tokens</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokenized</span><span class="p">),</span>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a>			<span class="n">tokens</span><span class="o">=</span><span class="n">prompt_tokenized</span><span class="p">,</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a>		<span class="p">),</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a>	<span class="p">)</span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a>	<span class="c1"># save metadata</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a>	<span class="n">prompt_dir</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">save_path</span> <span class="o">/</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">model_name</span> <span class="o">/</span> <span class="s2">&quot;prompts&quot;</span> <span class="o">/</span> <span class="n">prompt</span><span class="p">[</span><span class="s2">&quot;hash&quot;</span><span class="p">]</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>	<span class="n">prompt_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a>	<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">prompt_dir</span> <span class="o">/</span> <span class="s2">&quot;prompt.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a>		<span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a>	<span class="c1"># set up names filter</span>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a>	<span class="n">names_filter_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a>	<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names_filter</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span><span class="p">):</span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a>		<span class="n">names_filter_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">key</span><span class="p">:</span> <span class="n">names_filter</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># noqa: E731</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>		<span class="n">names_filter_fn</span> <span class="o">=</span> <span class="n">names_filter</span>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a>	<span class="c1"># compute activations</span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a>	<span class="c1"># NOTE: no padding_side kwarg here -- it&#39;s only meaningful for multi-sequence</span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a>	<span class="c1"># batches where padding is needed. single-string input has no padding.</span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a>	<span class="c1"># see compute_activations_batched for the batched path that passes padding_side=&quot;right&quot;.</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a>	<span class="n">cache_torch</span><span class="p">:</span> <span class="n">ActivationCache</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a>	<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a>		<span class="n">_</span><span class="p">,</span> <span class="n">cache_torch</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">run_with_cache</span><span class="p">(</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a>			<span class="n">prompt_str</span><span class="p">,</span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a>			<span class="n">names_filter</span><span class="o">=</span><span class="n">names_filter_fn</span><span class="p">,</span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a>			<span class="n">return_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a>		<span class="p">)</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>	<span class="n">activations_path</span><span class="p">:</span> <span class="n">Path</span>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a>	<span class="c1"># saving and returning</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a>	<span class="k">if</span> <span class="n">stack_heads</span><span class="p">:</span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a>		<span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_layers</span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a>		<span class="n">key_pattern</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;blocks.</span><span class="si">{i}</span><span class="s2">.attn.hook_pattern&quot;</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a>		<span class="c1"># NOTE: this only works for stacking heads at the moment</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a>		<span class="c1"># activations_specifier: str = key_pattern.format(i=f&#39;0-{n_layers}&#39;)</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>		<span class="n">activations_specifier</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">key_pattern</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a>		<span class="n">activations_path</span> <span class="o">=</span> <span class="n">prompt_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;activations-</span><span class="si">{</span><span class="n">activations_specifier</span><span class="si">}</span><span class="s2">.npy&quot;</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a>		<span class="c1"># check the keys are only attention heads</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a>		<span class="n">head_keys</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">key_pattern</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a>		<span class="n">cache_torch_keys_set</span><span class="p">:</span> <span class="nb">set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">cache_torch</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a>		<span class="k">assert</span> <span class="n">cache_torch_keys_set</span> <span class="o">==</span> <span class="nb">set</span><span class="p">(</span><span class="n">head_keys</span><span class="p">),</span> <span class="p">(</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a>			<span class="sa">f</span><span class="s2">&quot;unexpected keys!</span><span class="se">\n</span><span class="si">{</span><span class="nb">set</span><span class="p">(</span><span class="n">head_keys</span><span class="p">)</span><span class="o">.</span><span class="n">symmetric_difference</span><span class="p">(</span><span class="n">cache_torch_keys_set</span><span class="p">)</span><span class="w"> </span><span class="si">= }</span><span class="se">\n</span><span class="si">{</span><span class="n">cache_torch_keys_set</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="nb">set</span><span class="p">(</span><span class="n">head_keys</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>		<span class="p">)</span>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a>		<span class="c1"># stack heads</span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a>		<span class="n">patterns_stacked</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;n_layers n_heads n_ctx n_ctx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a>			<span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">cache_torch</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">head_keys</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a>		<span class="p">)</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a>		<span class="c1"># check shape</span>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a>		<span class="n">pattern_shape_no_ctx</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">patterns_stacked</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a>		<span class="k">assert</span> <span class="n">pattern_shape_no_ctx</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span> <span class="p">(</span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a>			<span class="sa">f</span><span class="s2">&quot;unexpected shape: </span><span class="si">{</span><span class="n">patterns_stacked</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span><span class="w"> </span><span class="si">= }</span><span class="s2"> (</span><span class="si">{</span><span class="n">pattern_shape_no_ctx</span><span class="w"> </span><span class="si">= }</span><span class="s2">), expected </span><span class="si">{</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">n_layers</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span><span class="w"> </span><span class="si">= }</span><span class="s2">&quot;</span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a>		<span class="p">)</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a>		<span class="n">patterns_stacked_np</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s2">&quot;n_layers n_heads n_ctx n_ctx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>			<span class="n">patterns_stacked</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a>		<span class="p">)</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a>		<span class="c1"># save</span>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a>		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">activations_path</span><span class="p">,</span> <span class="n">patterns_stacked_np</span><span class="p">)</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a>		<span class="c1"># return</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a>		<span class="k">match</span> <span class="n">return_cache</span><span class="p">:</span>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a>			<span class="k">case</span> <span class="s2">&quot;numpy&quot;</span><span class="p">:</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="n">patterns_stacked_np</span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a>			<span class="k">case</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="n">patterns_stacked</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a>			<span class="k">case</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>			<span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a>				<span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;invalid return_cache: </span><span class="si">{</span><span class="n">return_cache</span><span class="w"> </span><span class="si">= }</span><span class="s2">&quot;</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a>				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a>		<span class="n">activations_path</span> <span class="o">=</span> <span class="n">prompt_dir</span> <span class="o">/</span> <span class="s2">&quot;activations.npz&quot;</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a>		<span class="c1"># save</span>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a>		<span class="n">cache_np</span><span class="p">:</span> <span class="n">ActivationCacheNp</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a>			<span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cache_torch</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a>		<span class="p">}</span>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a>		<span class="n">np</span><span class="o">.</span><span class="n">savez_compressed</span><span class="p">(</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a>			<span class="n">activations_path</span><span class="p">,</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a>			<span class="o">**</span><span class="n">cache_np</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a>		<span class="p">)</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a>		<span class="c1"># return</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a>		<span class="k">match</span> <span class="n">return_cache</span><span class="p">:</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a>			<span class="k">case</span> <span class="s2">&quot;numpy&quot;</span><span class="p">:</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="n">cache_np</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a>			<span class="k">case</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="n">cache_torch</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a>			<span class="k">case</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a>			<span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a>				<span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;invalid return_cache: </span><span class="si">{</span><span class="n">return_cache</span><span class="w"> </span><span class="si">= }</span><span class="s2">&quot;</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a>				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_activations_batched</span><span class="p">(</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a>	<span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span><span class="p">,</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a>	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">ATTN_PATTERN_REGEX</span><span class="p">,</span>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a>	<span class="n">seq_lens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Path</span><span class="p">]:</span>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a><span class="w">	</span><span class="sd">&quot;&quot;&quot;compute and save activations for a batch of prompts in a single forward pass</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a>
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a><span class="sd">	Batched companion to `compute_activations` -- instead of one forward pass per</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a><span class="sd">	prompt, this runs a single `model.run_with_cache(list_of_strings)` call for the</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a><span class="sd">	whole batch. TransformerLens tokenizes and right-pads automatically. Each prompt&#39;s</span>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a><span class="sd">	attention patterns are then trimmed to their actual (unpadded) size and saved</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a><span class="sd">	individually, producing files identical to the single-prompt path.</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a><span class="sd">	Does not support `stack_heads` or `return_cache` -- this function is intended for</span>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a><span class="sd">	the bulk processing path in `activations_main`, not for interactive use. Use</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a><span class="sd">	`compute_activations` directly for single-prompt use cases that need those features.</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a><span class="sd">	## Why right-padding makes trimming correct without an explicit attention mask</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a><span class="sd">	With right-padding, pad tokens sit at positions seq_len, seq_len+1, ...,</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a><span class="sd">	max_seq_len-1 (higher than any real token). The causal attention mask prevents</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a><span class="sd">	position i from attending to any j &gt; i. So for real tokens at positions</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a><span class="sd">	0..seq_len-1, they can only attend to 0..i -- all real tokens. The softmax is computed over the same set of positions</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a><span class="sd">	as in single-prompt inference, producing identical attention patterns.</span>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a><span class="sd">	We explicitly pass `padding_side=&quot;right&quot;` to `run_with_cache` to guarantee this</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a><span class="sd">	regardless of the model&#39;s default padding side.</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a><span class="sd">	# Parameters:</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a><span class="sd">	- `prompts : list[dict]`</span>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a><span class="sd">		each prompt must contain &#39;text&#39; and &#39;hash&#39; keys. call</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a><span class="sd">		`augment_prompt_with_hash` on each prompt before passing them here.</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a><span class="sd">	- `model : HookedTransformer`</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a><span class="sd">		the model to compute activations with</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a><span class="sd">	- `save_path : Path`</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a><span class="sd">		path to save the activations to</span>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a><span class="sd">		(defaults to `Path(DATA_DIR)`)</span>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a><span class="sd">	- `names_filter : Callable[[str], bool] | re.Pattern`</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a><span class="sd">		filter for which activations to save. must only match activations with</span>
</span><span id="L-336"><a href="#L-336"><span class="linenos">336</span></a><span class="sd">		4D shape `[batch, n_heads, seq, seq]` (e.g. attention patterns).</span>
</span><span id="L-337"><a href="#L-337"><span class="linenos">337</span></a><span class="sd">		non-attention activations will cause incorrect trimming.</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos">338</span></a><span class="sd">		(defaults to `ATTN_PATTERN_REGEX`)</span>
</span><span id="L-339"><a href="#L-339"><span class="linenos">339</span></a><span class="sd">	- `seq_lens : list[int] | None`</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos">340</span></a><span class="sd">		pre-computed model sequence lengths per prompt (from `model.to_tokens`).</span>
</span><span id="L-341"><a href="#L-341"><span class="linenos">341</span></a><span class="sd">		if `None`, will be computed internally. pass this to avoid redundant</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos">342</span></a><span class="sd">		tokenization when lengths are already known (e.g. from length-sorting).</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos">343</span></a><span class="sd">		**important**: these must be from `model.to_tokens()` (includes BOS),</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos">344</span></a><span class="sd">		NOT from `model.tokenizer.tokenize()` (excludes BOS).</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos">345</span></a><span class="sd">		(defaults to `None`)</span>
</span><span id="L-346"><a href="#L-346"><span class="linenos">346</span></a>
</span><span id="L-347"><a href="#L-347"><span class="linenos">347</span></a><span class="sd">	# Returns:</span>
</span><span id="L-348"><a href="#L-348"><span class="linenos">348</span></a><span class="sd">	- `list[Path]`</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos">349</span></a><span class="sd">		paths to the saved activations files, one per prompt</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos">350</span></a>
</span><span id="L-351"><a href="#L-351"><span class="linenos">351</span></a><span class="sd">	# Modifies:</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos">352</span></a><span class="sd">	each prompt dict in `prompts` -- adds/overwrites `n_tokens` and `tokens` keys</span>
</span><span id="L-353"><a href="#L-353"><span class="linenos">353</span></a><span class="sd">	with tokenization metadata (same mutation as `compute_activations`).</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos">354</span></a><span class="sd">	&quot;&quot;&quot;</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos">355</span></a>	<span class="k">assert</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;model must be passed&quot;</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos">356</span></a>	<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;prompts must not be empty&quot;</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos">357</span></a>	<span class="k">assert</span> <span class="s2">&quot;text&quot;</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;prompt must contain &#39;text&#39; key: </span><span class="si">{</span><span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-358"><a href="#L-358"><span class="linenos">358</span></a>	<span class="k">assert</span> <span class="s2">&quot;hash&quot;</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos">359</span></a>		<span class="sa">f</span><span class="s2">&quot;prompt must contain &#39;hash&#39; key (call augment_prompt_with_hash first): </span><span class="si">{</span><span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-360"><a href="#L-360"><span class="linenos">360</span></a>	<span class="p">)</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos">361</span></a>
</span><span id="L-362"><a href="#L-362"><span class="linenos">362</span></a>	<span class="c1"># --- Phase A: get actual model sequence lengths ---</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos">363</span></a>	<span class="c1"># model.to_tokens() includes BOS if applicable, matching the attention pattern dims</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos">364</span></a>	<span class="c1"># model.tokenizer.tokenize() gives subword strings WITHOUT BOS, used for metadata</span>
</span><span id="L-365"><a href="#L-365"><span class="linenos">365</span></a>	<span class="c1"># these differ by 1 when BOS is prepended -- using the wrong one for trimming</span>
</span><span id="L-366"><a href="#L-366"><span class="linenos">366</span></a>	<span class="c1"># would silently truncate or include garbage</span>
</span><span id="L-367"><a href="#L-367"><span class="linenos">367</span></a>	<span class="k">if</span> <span class="n">seq_lens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos">368</span></a>		<span class="n">seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
</span><span id="L-369"><a href="#L-369"><span class="linenos">369</span></a>	<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">),</span> <span class="p">(</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos">370</span></a>		<span class="sa">f</span><span class="s2">&quot;seq_lens length mismatch: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">)</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos">371</span></a>	<span class="p">)</span>
</span><span id="L-372"><a href="#L-372"><span class="linenos">372</span></a>
</span><span id="L-373"><a href="#L-373"><span class="linenos">373</span></a>	<span class="c1"># --- Phase B: save prompt metadata (mirrors compute_activations&#39;s metadata logic) ---</span>
</span><span id="L-374"><a href="#L-374"><span class="linenos">374</span></a>	<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos">375</span></a>	<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos">376</span></a>		<span class="n">prompt_str</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
</span><span id="L-377"><a href="#L-377"><span class="linenos">377</span></a>		<span class="n">prompt_tokenized</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos">378</span></a>			<span class="s2">&quot;tokens&quot;</span><span class="p">,</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos">379</span></a>			<span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">prompt_str</span><span class="p">),</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos">380</span></a>		<span class="p">)</span>
</span><span id="L-381"><a href="#L-381"><span class="linenos">381</span></a>		<span class="c1"># n_tokens counts subword tokens (no BOS); attention patterns include BOS so have dim n_tokens+1</span>
</span><span id="L-382"><a href="#L-382"><span class="linenos">382</span></a>		<span class="n">p</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos">383</span></a>			<span class="nb">dict</span><span class="p">(</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos">384</span></a>				<span class="n">n_tokens</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokenized</span><span class="p">),</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos">385</span></a>				<span class="n">tokens</span><span class="o">=</span><span class="n">prompt_tokenized</span><span class="p">,</span>
</span><span id="L-386"><a href="#L-386"><span class="linenos">386</span></a>			<span class="p">),</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos">387</span></a>		<span class="p">)</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos">388</span></a>		<span class="n">prompt_dir</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">save_path</span> <span class="o">/</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">model_name</span> <span class="o">/</span> <span class="s2">&quot;prompts&quot;</span> <span class="o">/</span> <span class="n">p</span><span class="p">[</span><span class="s2">&quot;hash&quot;</span><span class="p">]</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos">389</span></a>		<span class="n">prompt_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-390"><a href="#L-390"><span class="linenos">390</span></a>		<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">prompt_dir</span> <span class="o">/</span> <span class="s2">&quot;prompt.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos">391</span></a>			<span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos">392</span></a>
</span><span id="L-393"><a href="#L-393"><span class="linenos">393</span></a>	<span class="c1"># --- Phase C: batched forward pass ---</span>
</span><span id="L-394"><a href="#L-394"><span class="linenos">394</span></a>	<span class="n">names_filter_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos">395</span></a>	<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names_filter</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span><span class="p">):</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos">396</span></a>		<span class="n">names_filter_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">key</span><span class="p">:</span> <span class="n">names_filter</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># noqa: E731</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos">397</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos">398</span></a>		<span class="n">names_filter_fn</span> <span class="o">=</span> <span class="n">names_filter</span>
</span><span id="L-399"><a href="#L-399"><span class="linenos">399</span></a>
</span><span id="L-400"><a href="#L-400"><span class="linenos">400</span></a>	<span class="n">texts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
</span><span id="L-401"><a href="#L-401"><span class="linenos">401</span></a>	<span class="n">cache_torch</span><span class="p">:</span> <span class="n">ActivationCache</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos">402</span></a>	<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos">403</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="L-404"><a href="#L-404"><span class="linenos">404</span></a>		<span class="n">_</span><span class="p">,</span> <span class="n">cache_torch</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">run_with_cache</span><span class="p">(</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos">405</span></a>			<span class="n">texts</span><span class="p">,</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos">406</span></a>			<span class="n">names_filter</span><span class="o">=</span><span class="n">names_filter_fn</span><span class="p">,</span>
</span><span id="L-407"><a href="#L-407"><span class="linenos">407</span></a>			<span class="n">return_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-408"><a href="#L-408"><span class="linenos">408</span></a>			<span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span>
</span><span id="L-409"><a href="#L-409"><span class="linenos">409</span></a>		<span class="p">)</span>
</span><span id="L-410"><a href="#L-410"><span class="linenos">410</span></a>
</span><span id="L-411"><a href="#L-411"><span class="linenos">411</span></a>	<span class="c1"># --- Phase D: split, trim padding, and save per-prompt ---</span>
</span><span id="L-412"><a href="#L-412"><span class="linenos">412</span></a>	<span class="c1"># For each prompt i with actual sequence length seq_len_i:</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos">413</span></a>	<span class="c1">#   v[i : i+1, :, :seq_len_i, :seq_len_i]</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos">414</span></a>	<span class="c1">#     ^^^^^^^                               i:i+1 not i -- keeps batch dim [1,...] for</span>
</span><span id="L-415"><a href="#L-415"><span class="linenos">415</span></a>	<span class="c1">#                                           format compatibility with compute_activations</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos">416</span></a>	<span class="c1">#              ^^                           all attention heads</span>
</span><span id="L-417"><a href="#L-417"><span class="linenos">417</span></a>	<span class="c1">#                  ^^^^^^^^^^  ^^^^^^^^^^   trim both query and key dims to actual length,</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos">418</span></a>	<span class="c1">#                                           discarding meaningless padding positions</span>
</span><span id="L-419"><a href="#L-419"><span class="linenos">419</span></a>	<span class="n">paths</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos">420</span></a>	<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)):</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos">421</span></a>		<span class="n">prompt_dir</span> <span class="o">=</span> <span class="n">save_path</span> <span class="o">/</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">model_name</span> <span class="o">/</span> <span class="s2">&quot;prompts&quot;</span> <span class="o">/</span> <span class="n">prompt</span><span class="p">[</span><span class="s2">&quot;hash&quot;</span><span class="p">]</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos">422</span></a>		<span class="n">activations_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">prompt_dir</span> <span class="o">/</span> <span class="s2">&quot;activations.npz&quot;</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos">423</span></a>		<span class="n">cache_np</span><span class="p">:</span> <span class="n">ActivationCacheNp</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="L-424"><a href="#L-424"><span class="linenos">424</span></a>		<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cache_torch</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos">425</span></a>			<span class="k">assert</span> <span class="n">v</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span>  <span class="c1"># noqa: PLR2004</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos">426</span></a>				<span class="sa">f</span><span class="s2">&quot;expected 4D attention pattern tensor for </span><span class="si">{</span><span class="n">k</span><span class="si">!r}</span><span class="s2">, &quot;</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos">427</span></a>				<span class="sa">f</span><span class="s2">&quot;got shape </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. names_filter must only match &quot;</span>
</span><span id="L-428"><a href="#L-428"><span class="linenos">428</span></a>				<span class="sa">f</span><span class="s2">&quot;attention pattern activations [batch, n_heads, seq, seq]&quot;</span>
</span><span id="L-429"><a href="#L-429"><span class="linenos">429</span></a>			<span class="p">)</span>
</span><span id="L-430"><a href="#L-430"><span class="linenos">430</span></a>			<span class="n">cache_np</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span><span id="L-431"><a href="#L-431"><span class="linenos">431</span></a>
</span><span id="L-432"><a href="#L-432"><span class="linenos">432</span></a>		<span class="n">np</span><span class="o">.</span><span class="n">savez_compressed</span><span class="p">(</span>
</span><span id="L-433"><a href="#L-433"><span class="linenos">433</span></a>			<span class="n">activations_path</span><span class="p">,</span>
</span><span id="L-434"><a href="#L-434"><span class="linenos">434</span></a>			<span class="o">**</span><span class="n">cache_np</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
</span><span id="L-435"><a href="#L-435"><span class="linenos">435</span></a>		<span class="p">)</span>
</span><span id="L-436"><a href="#L-436"><span class="linenos">436</span></a>		<span class="n">paths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activations_path</span><span class="p">)</span>
</span><span id="L-437"><a href="#L-437"><span class="linenos">437</span></a>
</span><span id="L-438"><a href="#L-438"><span class="linenos">438</span></a>	<span class="k">return</span> <span class="n">paths</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos">439</span></a>
</span><span id="L-440"><a href="#L-440"><span class="linenos">440</span></a>
</span><span id="L-441"><a href="#L-441"><span class="linenos">441</span></a><span class="nd">@overload</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos">442</span></a><span class="k">def</span><span class="w"> </span><span class="nf">get_activations</span><span class="p">(</span>
</span><span id="L-443"><a href="#L-443"><span class="linenos">443</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="L-444"><a href="#L-444"><span class="linenos">444</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos">445</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-446"><a href="#L-446"><span class="linenos">446</span></a>	<span class="n">allow_disk_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-447"><a href="#L-447"><span class="linenos">447</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="L-448"><a href="#L-448"><span class="linenos">448</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span> <span class="o">...</span>
</span><span id="L-449"><a href="#L-449"><span class="linenos">449</span></a><span class="nd">@overload</span>
</span><span id="L-450"><a href="#L-450"><span class="linenos">450</span></a><span class="k">def</span><span class="w"> </span><span class="nf">get_activations</span><span class="p">(</span>
</span><span id="L-451"><a href="#L-451"><span class="linenos">451</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="L-452"><a href="#L-452"><span class="linenos">452</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-453"><a href="#L-453"><span class="linenos">453</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-454"><a href="#L-454"><span class="linenos">454</span></a>	<span class="n">allow_disk_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-455"><a href="#L-455"><span class="linenos">455</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;torch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span><span class="p">,</span>
</span><span id="L-456"><a href="#L-456"><span class="linenos">456</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="n">ActivationCache</span><span class="p">]:</span> <span class="o">...</span>
</span><span id="L-457"><a href="#L-457"><span class="linenos">457</span></a><span class="nd">@overload</span>
</span><span id="L-458"><a href="#L-458"><span class="linenos">458</span></a><span class="k">def</span><span class="w"> </span><span class="nf">get_activations</span><span class="p">(</span>
</span><span id="L-459"><a href="#L-459"><span class="linenos">459</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="L-460"><a href="#L-460"><span class="linenos">460</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-461"><a href="#L-461"><span class="linenos">461</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-462"><a href="#L-462"><span class="linenos">462</span></a>	<span class="n">allow_disk_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-463"><a href="#L-463"><span class="linenos">463</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;numpy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;numpy&quot;</span><span class="p">,</span>
</span><span id="L-464"><a href="#L-464"><span class="linenos">464</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="n">ActivationCacheNp</span><span class="p">]:</span> <span class="o">...</span>
</span><span id="L-465"><a href="#L-465"><span class="linenos">465</span></a><span class="k">def</span><span class="w"> </span><span class="nf">get_activations</span><span class="p">(</span>
</span><span id="L-466"><a href="#L-466"><span class="linenos">466</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="L-467"><a href="#L-467"><span class="linenos">467</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-468"><a href="#L-468"><span class="linenos">468</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="L-469"><a href="#L-469"><span class="linenos">469</span></a>	<span class="n">allow_disk_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="L-470"><a href="#L-470"><span class="linenos">470</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="n">ReturnCache</span> <span class="o">=</span> <span class="s2">&quot;numpy&quot;</span><span class="p">,</span>
</span><span id="L-471"><a href="#L-471"><span class="linenos">471</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="n">ActivationCacheNp</span> <span class="o">|</span> <span class="n">ActivationCache</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]:</span>
</span><span id="L-472"><a href="#L-472"><span class="linenos">472</span></a><span class="w">	</span><span class="sd">&quot;&quot;&quot;given a prompt and a model, save or load activations</span>
</span><span id="L-473"><a href="#L-473"><span class="linenos">473</span></a>
</span><span id="L-474"><a href="#L-474"><span class="linenos">474</span></a><span class="sd">	# Parameters:</span>
</span><span id="L-475"><a href="#L-475"><span class="linenos">475</span></a><span class="sd">	- `prompt : dict`</span>
</span><span id="L-476"><a href="#L-476"><span class="linenos">476</span></a><span class="sd">		expected to contain the &#39;text&#39; key</span>
</span><span id="L-477"><a href="#L-477"><span class="linenos">477</span></a><span class="sd">	- `model : HookedTransformer | str`</span>
</span><span id="L-478"><a href="#L-478"><span class="linenos">478</span></a><span class="sd">		either a `HookedTransformer` or a string model name, to be loaded with `HookedTransformer.from_pretrained`</span>
</span><span id="L-479"><a href="#L-479"><span class="linenos">479</span></a><span class="sd">	- `save_path : Path`</span>
</span><span id="L-480"><a href="#L-480"><span class="linenos">480</span></a><span class="sd">		path to save the activations to (and load from)</span>
</span><span id="L-481"><a href="#L-481"><span class="linenos">481</span></a><span class="sd">		(defaults to `Path(DATA_DIR)`)</span>
</span><span id="L-482"><a href="#L-482"><span class="linenos">482</span></a><span class="sd">	- `allow_disk_cache : bool`</span>
</span><span id="L-483"><a href="#L-483"><span class="linenos">483</span></a><span class="sd">		whether to allow loading from disk cache</span>
</span><span id="L-484"><a href="#L-484"><span class="linenos">484</span></a><span class="sd">		(defaults to `True`)</span>
</span><span id="L-485"><a href="#L-485"><span class="linenos">485</span></a><span class="sd">	- `return_cache : Literal[None, &quot;numpy&quot;, &quot;torch&quot;]`</span>
</span><span id="L-486"><a href="#L-486"><span class="linenos">486</span></a><span class="sd">		whether to return the cache, and in what format</span>
</span><span id="L-487"><a href="#L-487"><span class="linenos">487</span></a><span class="sd">		(defaults to `&quot;numpy&quot;`)</span>
</span><span id="L-488"><a href="#L-488"><span class="linenos">488</span></a>
</span><span id="L-489"><a href="#L-489"><span class="linenos">489</span></a><span class="sd">	# Returns:</span>
</span><span id="L-490"><a href="#L-490"><span class="linenos">490</span></a><span class="sd">	- `tuple[Path, ActivationCacheNp | ActivationCache | None]`</span>
</span><span id="L-491"><a href="#L-491"><span class="linenos">491</span></a><span class="sd">		the path to the activations and the cache if `return_cache is not None`</span>
</span><span id="L-492"><a href="#L-492"><span class="linenos">492</span></a>
</span><span id="L-493"><a href="#L-493"><span class="linenos">493</span></a><span class="sd">	&quot;&quot;&quot;</span>
</span><span id="L-494"><a href="#L-494"><span class="linenos">494</span></a>	<span class="c1"># add hash to prompt</span>
</span><span id="L-495"><a href="#L-495"><span class="linenos">495</span></a>	<span class="n">augment_prompt_with_hash</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</span><span id="L-496"><a href="#L-496"><span class="linenos">496</span></a>
</span><span id="L-497"><a href="#L-497"><span class="linenos">497</span></a>	<span class="c1"># get the model</span>
</span><span id="L-498"><a href="#L-498"><span class="linenos">498</span></a>	<span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="L-499"><a href="#L-499"><span class="linenos">499</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">model_name</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">HookedTransformer</span><span class="p">)</span> <span class="k">else</span> <span class="n">model</span>
</span><span id="L-500"><a href="#L-500"><span class="linenos">500</span></a>	<span class="p">)</span>
</span><span id="L-501"><a href="#L-501"><span class="linenos">501</span></a>
</span><span id="L-502"><a href="#L-502"><span class="linenos">502</span></a>	<span class="c1"># from cache</span>
</span><span id="L-503"><a href="#L-503"><span class="linenos">503</span></a>	<span class="k">if</span> <span class="n">allow_disk_cache</span><span class="p">:</span>
</span><span id="L-504"><a href="#L-504"><span class="linenos">504</span></a>		<span class="k">if</span> <span class="n">return_cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-505"><a href="#L-505"><span class="linenos">505</span></a>			<span class="c1"># fast path: check file existence without loading data into memory.</span>
</span><span id="L-506"><a href="#L-506"><span class="linenos">506</span></a>			<span class="c1"># activations_exist just calls .exists() on two paths, whereas</span>
</span><span id="L-507"><a href="#L-507"><span class="linenos">507</span></a>			<span class="c1"># load_activations would decompress the full .npz into numpy arrays</span>
</span><span id="L-508"><a href="#L-508"><span class="linenos">508</span></a>			<span class="c1"># only for us to discard them immediately.</span>
</span><span id="L-509"><a href="#L-509"><span class="linenos">509</span></a>			<span class="k">if</span> <span class="n">activations_exist</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">save_path</span><span class="p">):</span>
</span><span id="L-510"><a href="#L-510"><span class="linenos">510</span></a>				<span class="n">prompt_dir</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">save_path</span> <span class="o">/</span> <span class="n">model_name</span> <span class="o">/</span> <span class="s2">&quot;prompts&quot;</span> <span class="o">/</span> <span class="n">prompt</span><span class="p">[</span><span class="s2">&quot;hash&quot;</span><span class="p">]</span>
</span><span id="L-511"><a href="#L-511"><span class="linenos">511</span></a>				<span class="k">return</span> <span class="n">prompt_dir</span> <span class="o">/</span> <span class="s2">&quot;activations.npz&quot;</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="L-512"><a href="#L-512"><span class="linenos">512</span></a>		<span class="k">else</span><span class="p">:</span>
</span><span id="L-513"><a href="#L-513"><span class="linenos">513</span></a>			<span class="k">try</span><span class="p">:</span>
</span><span id="L-514"><a href="#L-514"><span class="linenos">514</span></a>				<span class="n">path</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">load_activations</span><span class="p">(</span>
</span><span id="L-515"><a href="#L-515"><span class="linenos">515</span></a>					<span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
</span><span id="L-516"><a href="#L-516"><span class="linenos">516</span></a>					<span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
</span><span id="L-517"><a href="#L-517"><span class="linenos">517</span></a>					<span class="n">save_path</span><span class="o">=</span><span class="n">save_path</span><span class="p">,</span>
</span><span id="L-518"><a href="#L-518"><span class="linenos">518</span></a>				<span class="p">)</span>
</span><span id="L-519"><a href="#L-519"><span class="linenos">519</span></a>			<span class="k">except</span> <span class="n">ActivationsMissingError</span><span class="p">:</span>
</span><span id="L-520"><a href="#L-520"><span class="linenos">520</span></a>				<span class="k">pass</span>
</span><span id="L-521"><a href="#L-521"><span class="linenos">521</span></a>			<span class="k">else</span><span class="p">:</span>
</span><span id="L-522"><a href="#L-522"><span class="linenos">522</span></a>				<span class="k">return</span> <span class="n">path</span><span class="p">,</span> <span class="n">cache</span>
</span><span id="L-523"><a href="#L-523"><span class="linenos">523</span></a>
</span><span id="L-524"><a href="#L-524"><span class="linenos">524</span></a>	<span class="c1"># compute them</span>
</span><span id="L-525"><a href="#L-525"><span class="linenos">525</span></a>	<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="L-526"><a href="#L-526"><span class="linenos">526</span></a>		<span class="n">model</span> <span class="o">=</span> <span class="n">HookedTransformer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span><span id="L-527"><a href="#L-527"><span class="linenos">527</span></a>
</span><span id="L-528"><a href="#L-528"><span class="linenos">528</span></a>	<span class="k">return</span> <span class="n">compute_activations</span><span class="p">(</span>  <span class="c1"># type: ignore[return-value]</span>
</span><span id="L-529"><a href="#L-529"><span class="linenos">529</span></a>		<span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
</span><span id="L-530"><a href="#L-530"><span class="linenos">530</span></a>		<span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
</span><span id="L-531"><a href="#L-531"><span class="linenos">531</span></a>		<span class="n">save_path</span><span class="o">=</span><span class="n">save_path</span><span class="p">,</span>
</span><span id="L-532"><a href="#L-532"><span class="linenos">532</span></a>		<span class="n">return_cache</span><span class="o">=</span><span class="n">return_cache</span><span class="p">,</span>
</span><span id="L-533"><a href="#L-533"><span class="linenos">533</span></a>	<span class="p">)</span>
</span><span id="L-534"><a href="#L-534"><span class="linenos">534</span></a>
</span><span id="L-535"><a href="#L-535"><span class="linenos">535</span></a>
</span><span id="L-536"><a href="#L-536"><span class="linenos">536</span></a><span class="n">DEFAULT_DEVICE</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>
</span><span id="L-537"><a href="#L-537"><span class="linenos">537</span></a>	<span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
</span><span id="L-538"><a href="#L-538"><span class="linenos">538</span></a><span class="p">)</span>
</span><span id="L-539"><a href="#L-539"><span class="linenos">539</span></a>
</span><span id="L-540"><a href="#L-540"><span class="linenos">540</span></a>
</span><span id="L-541"><a href="#L-541"><span class="linenos">541</span></a><span class="k">def</span><span class="w"> </span><span class="nf">activations_main</span><span class="p">(</span>  <span class="c1"># noqa: C901, PLR0912, PLR0915</span>
</span><span id="L-542"><a href="#L-542"><span class="linenos">542</span></a>	<span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-543"><a href="#L-543"><span class="linenos">543</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Path</span><span class="p">,</span>
</span><span id="L-544"><a href="#L-544"><span class="linenos">544</span></a>	<span class="n">prompts_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="L-545"><a href="#L-545"><span class="linenos">545</span></a>	<span class="n">raw_prompts</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-546"><a href="#L-546"><span class="linenos">546</span></a>	<span class="n">min_chars</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-547"><a href="#L-547"><span class="linenos">547</span></a>	<span class="n">max_chars</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-548"><a href="#L-548"><span class="linenos">548</span></a>	<span class="n">force</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-549"><a href="#L-549"><span class="linenos">549</span></a>	<span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="L-550"><a href="#L-550"><span class="linenos">550</span></a>	<span class="n">no_index_html</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="L-551"><a href="#L-551"><span class="linenos">551</span></a>	<span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-552"><a href="#L-552"><span class="linenos">552</span></a>	<span class="n">stacked_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="L-553"><a href="#L-553"><span class="linenos">553</span></a>	<span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">DEFAULT_DEVICE</span><span class="p">,</span>
</span><span id="L-554"><a href="#L-554"><span class="linenos">554</span></a>	<span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
</span><span id="L-555"><a href="#L-555"><span class="linenos">555</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-556"><a href="#L-556"><span class="linenos">556</span></a><span class="w">	</span><span class="sd">&quot;&quot;&quot;main function for computing activations</span>
</span><span id="L-557"><a href="#L-557"><span class="linenos">557</span></a>
</span><span id="L-558"><a href="#L-558"><span class="linenos">558</span></a><span class="sd">	# Parameters:</span>
</span><span id="L-559"><a href="#L-559"><span class="linenos">559</span></a><span class="sd">	- `model_name : str`</span>
</span><span id="L-560"><a href="#L-560"><span class="linenos">560</span></a><span class="sd">		name of a model to load with `HookedTransformer.from_pretrained`</span>
</span><span id="L-561"><a href="#L-561"><span class="linenos">561</span></a><span class="sd">	- `save_path : str | Path`</span>
</span><span id="L-562"><a href="#L-562"><span class="linenos">562</span></a><span class="sd">		path to save the activations to</span>
</span><span id="L-563"><a href="#L-563"><span class="linenos">563</span></a><span class="sd">	- `prompts_path : str`</span>
</span><span id="L-564"><a href="#L-564"><span class="linenos">564</span></a><span class="sd">		path to the prompts file</span>
</span><span id="L-565"><a href="#L-565"><span class="linenos">565</span></a><span class="sd">	- `raw_prompts : bool`</span>
</span><span id="L-566"><a href="#L-566"><span class="linenos">566</span></a><span class="sd">		whether the prompts are raw, not filtered by length. `load_text_data` will be called if `True`, otherwise just load the &quot;text&quot; field from each line in `prompts_path`</span>
</span><span id="L-567"><a href="#L-567"><span class="linenos">567</span></a><span class="sd">	- `min_chars : int`</span>
</span><span id="L-568"><a href="#L-568"><span class="linenos">568</span></a><span class="sd">		minimum number of characters for a prompt</span>
</span><span id="L-569"><a href="#L-569"><span class="linenos">569</span></a><span class="sd">	- `max_chars : int`</span>
</span><span id="L-570"><a href="#L-570"><span class="linenos">570</span></a><span class="sd">		maximum number of characters for a prompt</span>
</span><span id="L-571"><a href="#L-571"><span class="linenos">571</span></a><span class="sd">	- `force : bool`</span>
</span><span id="L-572"><a href="#L-572"><span class="linenos">572</span></a><span class="sd">		whether to overwrite existing files</span>
</span><span id="L-573"><a href="#L-573"><span class="linenos">573</span></a><span class="sd">	- `n_samples : int`</span>
</span><span id="L-574"><a href="#L-574"><span class="linenos">574</span></a><span class="sd">		maximum number of samples to process</span>
</span><span id="L-575"><a href="#L-575"><span class="linenos">575</span></a><span class="sd">	- `no_index_html : bool`</span>
</span><span id="L-576"><a href="#L-576"><span class="linenos">576</span></a><span class="sd">		whether to write an index.html file</span>
</span><span id="L-577"><a href="#L-577"><span class="linenos">577</span></a><span class="sd">	- `shuffle : bool`</span>
</span><span id="L-578"><a href="#L-578"><span class="linenos">578</span></a><span class="sd">		whether to shuffle the prompts</span>
</span><span id="L-579"><a href="#L-579"><span class="linenos">579</span></a><span class="sd">		(defaults to `False`)</span>
</span><span id="L-580"><a href="#L-580"><span class="linenos">580</span></a><span class="sd">	- `stacked_heads : bool`</span>
</span><span id="L-581"><a href="#L-581"><span class="linenos">581</span></a><span class="sd">		whether	to stack the heads in the output tensor. will save as `.npy` instead of `.npz` if `True`</span>
</span><span id="L-582"><a href="#L-582"><span class="linenos">582</span></a><span class="sd">		(defaults to `False`)</span>
</span><span id="L-583"><a href="#L-583"><span class="linenos">583</span></a><span class="sd">	- `device : str | torch.device`</span>
</span><span id="L-584"><a href="#L-584"><span class="linenos">584</span></a><span class="sd">		the device to use. if a string, will be passed to `torch.device`</span>
</span><span id="L-585"><a href="#L-585"><span class="linenos">585</span></a><span class="sd">	- `batch_size : int`</span>
</span><span id="L-586"><a href="#L-586"><span class="linenos">586</span></a><span class="sd">		number of prompts per forward pass. prompts are sorted by token length</span>
</span><span id="L-587"><a href="#L-587"><span class="linenos">587</span></a><span class="sd">		(longest first) and grouped so that similar-length prompts share a batch,</span>
</span><span id="L-588"><a href="#L-588"><span class="linenos">588</span></a><span class="sd">		minimizing padding waste. use `batch_size=1` for one prompt per forward</span>
</span><span id="L-589"><a href="#L-589"><span class="linenos">589</span></a><span class="sd">		pass (largely equivalent to the old sequential behavior, but note: prompts</span>
</span><span id="L-590"><a href="#L-590"><span class="linenos">590</span></a><span class="sd">		are still sorted by length and cache checking uses file-existence only,</span>
</span><span id="L-591"><a href="#L-591"><span class="linenos">591</span></a><span class="sd">		unlike the old path which processed prompts in order and validated cache</span>
</span><span id="L-592"><a href="#L-592"><span class="linenos">592</span></a><span class="sd">		contents via `load_activations`).</span>
</span><span id="L-593"><a href="#L-593"><span class="linenos">593</span></a><span class="sd">		the single-prompt functions `compute_activations` and `get_activations`</span>
</span><span id="L-594"><a href="#L-594"><span class="linenos">594</span></a><span class="sd">		are still available for programmatic use outside of `activations_main`.</span>
</span><span id="L-595"><a href="#L-595"><span class="linenos">595</span></a><span class="sd">		(defaults to `32`)</span>
</span><span id="L-596"><a href="#L-596"><span class="linenos">596</span></a><span class="sd">	&quot;&quot;&quot;</span>
</span><span id="L-597"><a href="#L-597"><span class="linenos">597</span></a>	<span class="c1"># figure out the device to use</span>
</span><span id="L-598"><a href="#L-598"><span class="linenos">598</span></a>	<span class="n">device_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
</span><span id="L-599"><a href="#L-599"><span class="linenos">599</span></a>	<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
</span><span id="L-600"><a href="#L-600"><span class="linenos">600</span></a>		<span class="n">device_</span> <span class="o">=</span> <span class="n">device</span>
</span><span id="L-601"><a href="#L-601"><span class="linenos">601</span></a>	<span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="L-602"><a href="#L-602"><span class="linenos">602</span></a>		<span class="n">device_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span><span id="L-603"><a href="#L-603"><span class="linenos">603</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="L-604"><a href="#L-604"><span class="linenos">604</span></a>		<span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;invalid device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="L-605"><a href="#L-605"><span class="linenos">605</span></a>		<span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span id="L-606"><a href="#L-606"><span class="linenos">606</span></a>
</span><span id="L-607"><a href="#L-607"><span class="linenos">607</span></a>	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;using device: </span><span class="si">{</span><span class="n">device_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-608"><a href="#L-608"><span class="linenos">608</span></a>
</span><span id="L-609"><a href="#L-609"><span class="linenos">609</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;loading model&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">):</span>
</span><span id="L-610"><a href="#L-610"><span class="linenos">610</span></a>		<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">=</span> <span class="n">HookedTransformer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span><span id="L-611"><a href="#L-611"><span class="linenos">611</span></a>			<span class="n">model_name</span><span class="p">,</span>
</span><span id="L-612"><a href="#L-612"><span class="linenos">612</span></a>			<span class="n">device</span><span class="o">=</span><span class="n">device_</span><span class="p">,</span>
</span><span id="L-613"><a href="#L-613"><span class="linenos">613</span></a>		<span class="p">)</span>
</span><span id="L-614"><a href="#L-614"><span class="linenos">614</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>  <span class="c1"># type: ignore[unresolved-attribute]</span>
</span><span id="L-615"><a href="#L-615"><span class="linenos">615</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
</span><span id="L-616"><a href="#L-616"><span class="linenos">616</span></a>		<span class="n">n_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</span><span id="L-617"><a href="#L-617"><span class="linenos">617</span></a>	<span class="nb">print</span><span class="p">(</span>
</span><span id="L-618"><a href="#L-618"><span class="linenos">618</span></a>		<span class="sa">f</span><span class="s2">&quot;loaded </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="n">shorten_numerical_to_str</span><span class="p">(</span><span class="n">n_params</span><span class="p">)</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">n_params</span><span class="si">}</span><span class="s2">) parameters&quot;</span><span class="p">,</span>
</span><span id="L-619"><a href="#L-619"><span class="linenos">619</span></a>	<span class="p">)</span>
</span><span id="L-620"><a href="#L-620"><span class="linenos">620</span></a>	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">model devices: </span><span class="si">{</span><span class="w"> </span><span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span><span class="w"> </span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-621"><a href="#L-621"><span class="linenos">621</span></a>
</span><span id="L-622"><a href="#L-622"><span class="linenos">622</span></a>	<span class="n">save_path_p</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
</span><span id="L-623"><a href="#L-623"><span class="linenos">623</span></a>	<span class="n">save_path_p</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-624"><a href="#L-624"><span class="linenos">624</span></a>	<span class="n">model_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">save_path_p</span> <span class="o">/</span> <span class="n">model_name</span>
</span><span id="L-625"><a href="#L-625"><span class="linenos">625</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span>
</span><span id="L-626"><a href="#L-626"><span class="linenos">626</span></a>		<span class="n">message</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;saving model info to </span><span class="si">{</span><span class="n">_rel_path</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="L-627"><a href="#L-627"><span class="linenos">627</span></a>		<span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="L-628"><a href="#L-628"><span class="linenos">628</span></a>	<span class="p">):</span>
</span><span id="L-629"><a href="#L-629"><span class="linenos">629</span></a>		<span class="n">model_cfg</span><span class="p">:</span> <span class="n">HookedTransformerConfig</span>
</span><span id="L-630"><a href="#L-630"><span class="linenos">630</span></a>		<span class="n">model_cfg</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span>
</span><span id="L-631"><a href="#L-631"><span class="linenos">631</span></a>		<span class="n">model_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-632"><a href="#L-632"><span class="linenos">632</span></a>		<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span> <span class="o">/</span> <span class="s2">&quot;model_cfg.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span id="L-633"><a href="#L-633"><span class="linenos">633</span></a>			<span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">json_serialize</span><span class="p">(</span><span class="n">asdict</span><span class="p">(</span><span class="n">model_cfg</span><span class="p">)),</span> <span class="n">f</span><span class="p">)</span>
</span><span id="L-634"><a href="#L-634"><span class="linenos">634</span></a>
</span><span id="L-635"><a href="#L-635"><span class="linenos">635</span></a>	<span class="c1"># load prompts</span>
</span><span id="L-636"><a href="#L-636"><span class="linenos">636</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span>
</span><span id="L-637"><a href="#L-637"><span class="linenos">637</span></a>		<span class="n">message</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;loading prompts from </span><span class="si">{</span><span class="n">Path</span><span class="p">(</span><span class="n">prompts_path</span><span class="p">)</span><span class="o">.</span><span class="n">as_posix</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="L-638"><a href="#L-638"><span class="linenos">638</span></a>		<span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="L-639"><a href="#L-639"><span class="linenos">639</span></a>	<span class="p">):</span>
</span><span id="L-640"><a href="#L-640"><span class="linenos">640</span></a>		<span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span>
</span><span id="L-641"><a href="#L-641"><span class="linenos">641</span></a>		<span class="k">if</span> <span class="n">raw_prompts</span><span class="p">:</span>
</span><span id="L-642"><a href="#L-642"><span class="linenos">642</span></a>			<span class="n">prompts</span> <span class="o">=</span> <span class="n">load_text_data</span><span class="p">(</span>
</span><span id="L-643"><a href="#L-643"><span class="linenos">643</span></a>				<span class="n">Path</span><span class="p">(</span><span class="n">prompts_path</span><span class="p">),</span>
</span><span id="L-644"><a href="#L-644"><span class="linenos">644</span></a>				<span class="n">min_chars</span><span class="o">=</span><span class="n">min_chars</span><span class="p">,</span>
</span><span id="L-645"><a href="#L-645"><span class="linenos">645</span></a>				<span class="n">max_chars</span><span class="o">=</span><span class="n">max_chars</span><span class="p">,</span>
</span><span id="L-646"><a href="#L-646"><span class="linenos">646</span></a>				<span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
</span><span id="L-647"><a href="#L-647"><span class="linenos">647</span></a>			<span class="p">)</span>
</span><span id="L-648"><a href="#L-648"><span class="linenos">648</span></a>		<span class="k">else</span><span class="p">:</span>
</span><span id="L-649"><a href="#L-649"><span class="linenos">649</span></a>			<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span> <span class="o">/</span> <span class="s2">&quot;prompts.jsonl&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span id="L-650"><a href="#L-650"><span class="linenos">650</span></a>				<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>
</span><span id="L-651"><a href="#L-651"><span class="linenos">651</span></a>		<span class="c1"># truncate to n_samples</span>
</span><span id="L-652"><a href="#L-652"><span class="linenos">652</span></a>		<span class="n">prompts</span> <span class="o">=</span> <span class="n">prompts</span><span class="p">[:</span><span class="n">n_samples</span><span class="p">]</span>
</span><span id="L-653"><a href="#L-653"><span class="linenos">653</span></a>
</span><span id="L-654"><a href="#L-654"><span class="linenos">654</span></a>	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span><span class="si">}</span><span class="s2"> prompts loaded&quot;</span><span class="p">)</span>
</span><span id="L-655"><a href="#L-655"><span class="linenos">655</span></a>
</span><span id="L-656"><a href="#L-656"><span class="linenos">656</span></a>	<span class="c1"># write index.html</span>
</span><span id="L-657"><a href="#L-657"><span class="linenos">657</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span>
</span><span id="L-658"><a href="#L-658"><span class="linenos">658</span></a>		<span class="n">message</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;writing </span><span class="si">{</span><span class="n">_rel_path</span><span class="p">(</span><span class="n">save_path_p</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="s1">&#39;index.html&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="L-659"><a href="#L-659"><span class="linenos">659</span></a>		<span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="L-660"><a href="#L-660"><span class="linenos">660</span></a>	<span class="p">):</span>
</span><span id="L-661"><a href="#L-661"><span class="linenos">661</span></a>		<span class="k">if</span> <span class="ow">not</span> <span class="n">no_index_html</span><span class="p">:</span>
</span><span id="L-662"><a href="#L-662"><span class="linenos">662</span></a>			<span class="n">write_html_index</span><span class="p">(</span><span class="n">save_path_p</span><span class="p">)</span>
</span><span id="L-663"><a href="#L-663"><span class="linenos">663</span></a>
</span><span id="L-664"><a href="#L-664"><span class="linenos">664</span></a>	<span class="c1"># TODO: not implemented yet</span>
</span><span id="L-665"><a href="#L-665"><span class="linenos">665</span></a>	<span class="k">if</span> <span class="n">stacked_heads</span><span class="p">:</span>
</span><span id="L-666"><a href="#L-666"><span class="linenos">666</span></a>		<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;stacked_heads not implemented yet&quot;</span><span class="p">)</span>
</span><span id="L-667"><a href="#L-667"><span class="linenos">667</span></a>
</span><span id="L-668"><a href="#L-668"><span class="linenos">668</span></a>	<span class="c1"># augment all prompts with hashes</span>
</span><span id="L-669"><a href="#L-669"><span class="linenos">669</span></a>	<span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
</span><span id="L-670"><a href="#L-670"><span class="linenos">670</span></a>		<span class="n">augment_prompt_with_hash</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</span><span id="L-671"><a href="#L-671"><span class="linenos">671</span></a>
</span><span id="L-672"><a href="#L-672"><span class="linenos">672</span></a>	<span class="c1"># filter out cached prompts</span>
</span><span id="L-673"><a href="#L-673"><span class="linenos">673</span></a>	<span class="k">if</span> <span class="ow">not</span> <span class="n">force</span><span class="p">:</span>
</span><span id="L-674"><a href="#L-674"><span class="linenos">674</span></a>		<span class="n">uncached</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-675"><a href="#L-675"><span class="linenos">675</span></a>			<span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">activations_exist</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">save_path_p</span><span class="p">)</span>
</span><span id="L-676"><a href="#L-676"><span class="linenos">676</span></a>		<span class="p">]</span>
</span><span id="L-677"><a href="#L-677"><span class="linenos">677</span></a>		<span class="n">n_cached</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">uncached</span><span class="p">)</span>
</span><span id="L-678"><a href="#L-678"><span class="linenos">678</span></a>		<span class="k">if</span> <span class="n">n_cached</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="L-679"><a href="#L-679"><span class="linenos">679</span></a>			<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">n_cached</span><span class="si">}</span><span class="s2"> prompts already cached, </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">uncached</span><span class="p">)</span><span class="si">}</span><span class="s2"> to compute&quot;</span><span class="p">)</span>
</span><span id="L-680"><a href="#L-680"><span class="linenos">680</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="L-681"><a href="#L-681"><span class="linenos">681</span></a>		<span class="n">uncached</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
</span><span id="L-682"><a href="#L-682"><span class="linenos">682</span></a>
</span><span id="L-683"><a href="#L-683"><span class="linenos">683</span></a>	<span class="k">if</span> <span class="n">uncached</span><span class="p">:</span>
</span><span id="L-684"><a href="#L-684"><span class="linenos">684</span></a>		<span class="c1"># sort by token length descending so that:</span>
</span><span id="L-685"><a href="#L-685"><span class="linenos">685</span></a>		<span class="c1"># 1. the longest (slowest, most memory-hungry) batches run first --</span>
</span><span id="L-686"><a href="#L-686"><span class="linenos">686</span></a>		<span class="c1">#    OOM errors surface immediately rather than after all the cheap work,</span>
</span><span id="L-687"><a href="#L-687"><span class="linenos">687</span></a>		<span class="c1">#    and tqdm&#39;s ETA stabilizes early for better progress estimation</span>
</span><span id="L-688"><a href="#L-688"><span class="linenos">688</span></a>		<span class="c1"># 2. similar-length prompts are grouped together, minimizing padding waste</span>
</span><span id="L-689"><a href="#L-689"><span class="linenos">689</span></a>		<span class="c1">#</span>
</span><span id="L-690"><a href="#L-690"><span class="linenos">690</span></a>		<span class="c1"># pre-tokenization is a separate step from compute_activations_batched because</span>
</span><span id="L-691"><a href="#L-691"><span class="linenos">691</span></a>		<span class="c1"># we need token lengths *before* batching to sort and group. the resulting</span>
</span><span id="L-692"><a href="#L-692"><span class="linenos">692</span></a>		<span class="c1"># seq_lens are then passed through so compute_activations_batched can skip</span>
</span><span id="L-693"><a href="#L-693"><span class="linenos">693</span></a>		<span class="c1"># re-tokenizing each prompt internally.</span>
</span><span id="L-694"><a href="#L-694"><span class="linenos">694</span></a>		<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span>
</span><span id="L-695"><a href="#L-695"><span class="linenos">695</span></a>			<span class="n">message</span><span class="o">=</span><span class="s2">&quot;pre-tokenizing prompts for length sorting&quot;</span><span class="p">,</span>
</span><span id="L-696"><a href="#L-696"><span class="linenos">696</span></a>			<span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="L-697"><a href="#L-697"><span class="linenos">697</span></a>		<span class="p">):</span>
</span><span id="L-698"><a href="#L-698"><span class="linenos">698</span></a>			<span class="n">uncached_with_lens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-699"><a href="#L-699"><span class="linenos">699</span></a>				<span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">uncached</span>
</span><span id="L-700"><a href="#L-700"><span class="linenos">700</span></a>			<span class="p">]</span>
</span><span id="L-701"><a href="#L-701"><span class="linenos">701</span></a>			<span class="n">uncached_with_lens</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="L-702"><a href="#L-702"><span class="linenos">702</span></a>			<span class="n">sorted_uncached</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">uncached_with_lens</span><span class="p">]</span>
</span><span id="L-703"><a href="#L-703"><span class="linenos">703</span></a>			<span class="n">sorted_seq_lens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">sl</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">sl</span> <span class="ow">in</span> <span class="n">uncached_with_lens</span><span class="p">]</span>
</span><span id="L-704"><a href="#L-704"><span class="linenos">704</span></a>
</span><span id="L-705"><a href="#L-705"><span class="linenos">705</span></a>		<span class="c1"># process in batches</span>
</span><span id="L-706"><a href="#L-706"><span class="linenos">706</span></a>		<span class="n">n_prompts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sorted_uncached</span><span class="p">)</span>
</span><span id="L-707"><a href="#L-707"><span class="linenos">707</span></a>		<span class="k">with</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span>
</span><span id="L-708"><a href="#L-708"><span class="linenos">708</span></a>			<span class="n">total</span><span class="o">=</span><span class="n">n_prompts</span><span class="p">,</span>
</span><span id="L-709"><a href="#L-709"><span class="linenos">709</span></a>			<span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Computing activations&quot;</span><span class="p">,</span>
</span><span id="L-710"><a href="#L-710"><span class="linenos">710</span></a>			<span class="n">unit</span><span class="o">=</span><span class="s2">&quot;prompt&quot;</span><span class="p">,</span>
</span><span id="L-711"><a href="#L-711"><span class="linenos">711</span></a>		<span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
</span><span id="L-712"><a href="#L-712"><span class="linenos">712</span></a>			<span class="k">for</span> <span class="n">batch_start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_prompts</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
</span><span id="L-713"><a href="#L-713"><span class="linenos">713</span></a>				<span class="n">batch_end</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">batch_start</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_prompts</span><span class="p">)</span>
</span><span id="L-714"><a href="#L-714"><span class="linenos">714</span></a>				<span class="n">batch</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">sorted_uncached</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span>
</span><span id="L-715"><a href="#L-715"><span class="linenos">715</span></a>				<span class="n">batch_seq_lens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">sorted_seq_lens</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span>
</span><span id="L-716"><a href="#L-716"><span class="linenos">716</span></a>				<span class="n">pbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span>
</span><span id="L-717"><a href="#L-717"><span class="linenos">717</span></a>					<span class="n">n_ctx</span><span class="o">=</span><span class="n">batch_seq_lens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="L-718"><a href="#L-718"><span class="linenos">718</span></a>				<span class="p">)</span>  <span class="c1"># longest in batch (sorted descending)</span>
</span><span id="L-719"><a href="#L-719"><span class="linenos">719</span></a>				<span class="n">compute_activations_batched</span><span class="p">(</span>
</span><span id="L-720"><a href="#L-720"><span class="linenos">720</span></a>					<span class="n">prompts</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
</span><span id="L-721"><a href="#L-721"><span class="linenos">721</span></a>					<span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
</span><span id="L-722"><a href="#L-722"><span class="linenos">722</span></a>					<span class="n">save_path</span><span class="o">=</span><span class="n">save_path_p</span><span class="p">,</span>
</span><span id="L-723"><a href="#L-723"><span class="linenos">723</span></a>					<span class="n">seq_lens</span><span class="o">=</span><span class="n">batch_seq_lens</span><span class="p">,</span>
</span><span id="L-724"><a href="#L-724"><span class="linenos">724</span></a>				<span class="p">)</span>
</span><span id="L-725"><a href="#L-725"><span class="linenos">725</span></a>				<span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
</span><span id="L-726"><a href="#L-726"><span class="linenos">726</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="L-727"><a href="#L-727"><span class="linenos">727</span></a>		<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  all prompts cached, nothing to compute&quot;</span><span class="p">)</span>
</span><span id="L-728"><a href="#L-728"><span class="linenos">728</span></a>
</span><span id="L-729"><a href="#L-729"><span class="linenos">729</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span>
</span><span id="L-730"><a href="#L-730"><span class="linenos">730</span></a>		<span class="n">message</span><span class="o">=</span><span class="s2">&quot;updating jsonl metadata for models and prompts&quot;</span><span class="p">,</span>
</span><span id="L-731"><a href="#L-731"><span class="linenos">731</span></a>		<span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="L-732"><a href="#L-732"><span class="linenos">732</span></a>	<span class="p">):</span>
</span><span id="L-733"><a href="#L-733"><span class="linenos">733</span></a>		<span class="n">generate_models_jsonl</span><span class="p">(</span><span class="n">save_path_p</span><span class="p">)</span>
</span><span id="L-734"><a href="#L-734"><span class="linenos">734</span></a>		<span class="n">generate_prompts_jsonl</span><span class="p">(</span><span class="n">save_path_p</span> <span class="o">/</span> <span class="n">model_name</span><span class="p">)</span>
</span><span id="L-735"><a href="#L-735"><span class="linenos">735</span></a>
</span><span id="L-736"><a href="#L-736"><span class="linenos">736</span></a>
</span><span id="L-737"><a href="#L-737"><span class="linenos">737</span></a><span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="L-738"><a href="#L-738"><span class="linenos">738</span></a>	<span class="s2">&quot;generate attention pattern activations for a model and prompts&quot;</span>
</span><span id="L-739"><a href="#L-739"><span class="linenos">739</span></a>	<span class="nb">print</span><span class="p">(</span><span class="n">DIVIDER_S1</span><span class="p">)</span>
</span><span id="L-740"><a href="#L-740"><span class="linenos">740</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;parsing args&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">):</span>
</span><span id="L-741"><a href="#L-741"><span class="linenos">741</span></a>		<span class="n">arg_parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
</span><span id="L-742"><a href="#L-742"><span class="linenos">742</span></a>		<span class="c1"># input and output</span>
</span><span id="L-743"><a href="#L-743"><span class="linenos">743</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-744"><a href="#L-744"><span class="linenos">744</span></a>			<span class="s2">&quot;--model&quot;</span><span class="p">,</span>
</span><span id="L-745"><a href="#L-745"><span class="linenos">745</span></a>			<span class="s2">&quot;-m&quot;</span><span class="p">,</span>
</span><span id="L-746"><a href="#L-746"><span class="linenos">746</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span><span id="L-747"><a href="#L-747"><span class="linenos">747</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="L-748"><a href="#L-748"><span class="linenos">748</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The model name(s) to use. comma separated with no whitespace if multiple&quot;</span><span class="p">,</span>
</span><span id="L-749"><a href="#L-749"><span class="linenos">749</span></a>		<span class="p">)</span>
</span><span id="L-750"><a href="#L-750"><span class="linenos">750</span></a>
</span><span id="L-751"><a href="#L-751"><span class="linenos">751</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-752"><a href="#L-752"><span class="linenos">752</span></a>			<span class="s2">&quot;--prompts&quot;</span><span class="p">,</span>
</span><span id="L-753"><a href="#L-753"><span class="linenos">753</span></a>			<span class="s2">&quot;-p&quot;</span><span class="p">,</span>
</span><span id="L-754"><a href="#L-754"><span class="linenos">754</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span><span id="L-755"><a href="#L-755"><span class="linenos">755</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-756"><a href="#L-756"><span class="linenos">756</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The path to the prompts file (jsonl with &#39;text&#39; key on each line). If `None`, expects that `--figures` is passed and will generate figures for all prompts in the model directory&quot;</span><span class="p">,</span>
</span><span id="L-757"><a href="#L-757"><span class="linenos">757</span></a>			<span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-758"><a href="#L-758"><span class="linenos">758</span></a>		<span class="p">)</span>
</span><span id="L-759"><a href="#L-759"><span class="linenos">759</span></a>
</span><span id="L-760"><a href="#L-760"><span class="linenos">760</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-761"><a href="#L-761"><span class="linenos">761</span></a>			<span class="s2">&quot;--save-path&quot;</span><span class="p">,</span>
</span><span id="L-762"><a href="#L-762"><span class="linenos">762</span></a>			<span class="s2">&quot;-s&quot;</span><span class="p">,</span>
</span><span id="L-763"><a href="#L-763"><span class="linenos">763</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span><span id="L-764"><a href="#L-764"><span class="linenos">764</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-765"><a href="#L-765"><span class="linenos">765</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The path to save the attention patterns&quot;</span><span class="p">,</span>
</span><span id="L-766"><a href="#L-766"><span class="linenos">766</span></a>			<span class="n">default</span><span class="o">=</span><span class="n">DATA_DIR</span><span class="p">,</span>
</span><span id="L-767"><a href="#L-767"><span class="linenos">767</span></a>		<span class="p">)</span>
</span><span id="L-768"><a href="#L-768"><span class="linenos">768</span></a>
</span><span id="L-769"><a href="#L-769"><span class="linenos">769</span></a>		<span class="c1"># min and max prompt lengths</span>
</span><span id="L-770"><a href="#L-770"><span class="linenos">770</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-771"><a href="#L-771"><span class="linenos">771</span></a>			<span class="s2">&quot;--min-chars&quot;</span><span class="p">,</span>
</span><span id="L-772"><a href="#L-772"><span class="linenos">772</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
</span><span id="L-773"><a href="#L-773"><span class="linenos">773</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-774"><a href="#L-774"><span class="linenos">774</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The minimum number of characters for a prompt&quot;</span><span class="p">,</span>
</span><span id="L-775"><a href="#L-775"><span class="linenos">775</span></a>			<span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span><span id="L-776"><a href="#L-776"><span class="linenos">776</span></a>		<span class="p">)</span>
</span><span id="L-777"><a href="#L-777"><span class="linenos">777</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-778"><a href="#L-778"><span class="linenos">778</span></a>			<span class="s2">&quot;--max-chars&quot;</span><span class="p">,</span>
</span><span id="L-779"><a href="#L-779"><span class="linenos">779</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
</span><span id="L-780"><a href="#L-780"><span class="linenos">780</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-781"><a href="#L-781"><span class="linenos">781</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The maximum number of characters for a prompt&quot;</span><span class="p">,</span>
</span><span id="L-782"><a href="#L-782"><span class="linenos">782</span></a>			<span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
</span><span id="L-783"><a href="#L-783"><span class="linenos">783</span></a>		<span class="p">)</span>
</span><span id="L-784"><a href="#L-784"><span class="linenos">784</span></a>
</span><span id="L-785"><a href="#L-785"><span class="linenos">785</span></a>		<span class="c1"># number of samples</span>
</span><span id="L-786"><a href="#L-786"><span class="linenos">786</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-787"><a href="#L-787"><span class="linenos">787</span></a>			<span class="s2">&quot;--n-samples&quot;</span><span class="p">,</span>
</span><span id="L-788"><a href="#L-788"><span class="linenos">788</span></a>			<span class="s2">&quot;-n&quot;</span><span class="p">,</span>
</span><span id="L-789"><a href="#L-789"><span class="linenos">789</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
</span><span id="L-790"><a href="#L-790"><span class="linenos">790</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-791"><a href="#L-791"><span class="linenos">791</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The max number of samples to process, do all in the file if None&quot;</span><span class="p">,</span>
</span><span id="L-792"><a href="#L-792"><span class="linenos">792</span></a>			<span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="L-793"><a href="#L-793"><span class="linenos">793</span></a>		<span class="p">)</span>
</span><span id="L-794"><a href="#L-794"><span class="linenos">794</span></a>
</span><span id="L-795"><a href="#L-795"><span class="linenos">795</span></a>		<span class="c1"># batch size</span>
</span><span id="L-796"><a href="#L-796"><span class="linenos">796</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-797"><a href="#L-797"><span class="linenos">797</span></a>			<span class="s2">&quot;--batch-size&quot;</span><span class="p">,</span>
</span><span id="L-798"><a href="#L-798"><span class="linenos">798</span></a>			<span class="s2">&quot;-b&quot;</span><span class="p">,</span>
</span><span id="L-799"><a href="#L-799"><span class="linenos">799</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
</span><span id="L-800"><a href="#L-800"><span class="linenos">800</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-801"><a href="#L-801"><span class="linenos">801</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;Batch size for computing activations (number of prompts per forward pass)&quot;</span><span class="p">,</span>
</span><span id="L-802"><a href="#L-802"><span class="linenos">802</span></a>			<span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
</span><span id="L-803"><a href="#L-803"><span class="linenos">803</span></a>		<span class="p">)</span>
</span><span id="L-804"><a href="#L-804"><span class="linenos">804</span></a>
</span><span id="L-805"><a href="#L-805"><span class="linenos">805</span></a>		<span class="c1"># force overwrite</span>
</span><span id="L-806"><a href="#L-806"><span class="linenos">806</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-807"><a href="#L-807"><span class="linenos">807</span></a>			<span class="s2">&quot;--force&quot;</span><span class="p">,</span>
</span><span id="L-808"><a href="#L-808"><span class="linenos">808</span></a>			<span class="s2">&quot;-f&quot;</span><span class="p">,</span>
</span><span id="L-809"><a href="#L-809"><span class="linenos">809</span></a>			<span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
</span><span id="L-810"><a href="#L-810"><span class="linenos">810</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;If passed, will overwrite existing files&quot;</span><span class="p">,</span>
</span><span id="L-811"><a href="#L-811"><span class="linenos">811</span></a>		<span class="p">)</span>
</span><span id="L-812"><a href="#L-812"><span class="linenos">812</span></a>
</span><span id="L-813"><a href="#L-813"><span class="linenos">813</span></a>		<span class="c1"># no index html</span>
</span><span id="L-814"><a href="#L-814"><span class="linenos">814</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-815"><a href="#L-815"><span class="linenos">815</span></a>			<span class="s2">&quot;--no-index-html&quot;</span><span class="p">,</span>
</span><span id="L-816"><a href="#L-816"><span class="linenos">816</span></a>			<span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
</span><span id="L-817"><a href="#L-817"><span class="linenos">817</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;If passed, will not write an index.html file for the model&quot;</span><span class="p">,</span>
</span><span id="L-818"><a href="#L-818"><span class="linenos">818</span></a>		<span class="p">)</span>
</span><span id="L-819"><a href="#L-819"><span class="linenos">819</span></a>
</span><span id="L-820"><a href="#L-820"><span class="linenos">820</span></a>		<span class="c1"># raw prompts</span>
</span><span id="L-821"><a href="#L-821"><span class="linenos">821</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-822"><a href="#L-822"><span class="linenos">822</span></a>			<span class="s2">&quot;--raw-prompts&quot;</span><span class="p">,</span>
</span><span id="L-823"><a href="#L-823"><span class="linenos">823</span></a>			<span class="s2">&quot;-r&quot;</span><span class="p">,</span>
</span><span id="L-824"><a href="#L-824"><span class="linenos">824</span></a>			<span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
</span><span id="L-825"><a href="#L-825"><span class="linenos">825</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;pass if the prompts have not been split and tokenized (still needs keys &#39;text&#39; and &#39;meta&#39; for each item)&quot;</span><span class="p">,</span>
</span><span id="L-826"><a href="#L-826"><span class="linenos">826</span></a>		<span class="p">)</span>
</span><span id="L-827"><a href="#L-827"><span class="linenos">827</span></a>
</span><span id="L-828"><a href="#L-828"><span class="linenos">828</span></a>		<span class="c1"># shuffle</span>
</span><span id="L-829"><a href="#L-829"><span class="linenos">829</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-830"><a href="#L-830"><span class="linenos">830</span></a>			<span class="s2">&quot;--shuffle&quot;</span><span class="p">,</span>
</span><span id="L-831"><a href="#L-831"><span class="linenos">831</span></a>			<span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
</span><span id="L-832"><a href="#L-832"><span class="linenos">832</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;If passed, will shuffle the prompts&quot;</span><span class="p">,</span>
</span><span id="L-833"><a href="#L-833"><span class="linenos">833</span></a>		<span class="p">)</span>
</span><span id="L-834"><a href="#L-834"><span class="linenos">834</span></a>
</span><span id="L-835"><a href="#L-835"><span class="linenos">835</span></a>		<span class="c1"># stack heads</span>
</span><span id="L-836"><a href="#L-836"><span class="linenos">836</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-837"><a href="#L-837"><span class="linenos">837</span></a>			<span class="s2">&quot;--stacked-heads&quot;</span><span class="p">,</span>
</span><span id="L-838"><a href="#L-838"><span class="linenos">838</span></a>			<span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
</span><span id="L-839"><a href="#L-839"><span class="linenos">839</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;If passed, will stack the heads in the output tensor&quot;</span><span class="p">,</span>
</span><span id="L-840"><a href="#L-840"><span class="linenos">840</span></a>		<span class="p">)</span>
</span><span id="L-841"><a href="#L-841"><span class="linenos">841</span></a>
</span><span id="L-842"><a href="#L-842"><span class="linenos">842</span></a>		<span class="c1"># device</span>
</span><span id="L-843"><a href="#L-843"><span class="linenos">843</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="L-844"><a href="#L-844"><span class="linenos">844</span></a>			<span class="s2">&quot;--device&quot;</span><span class="p">,</span>
</span><span id="L-845"><a href="#L-845"><span class="linenos">845</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span><span id="L-846"><a href="#L-846"><span class="linenos">846</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="L-847"><a href="#L-847"><span class="linenos">847</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The device to use for the model&quot;</span><span class="p">,</span>
</span><span id="L-848"><a href="#L-848"><span class="linenos">848</span></a>			<span class="n">default</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
</span><span id="L-849"><a href="#L-849"><span class="linenos">849</span></a>		<span class="p">)</span>
</span><span id="L-850"><a href="#L-850"><span class="linenos">850</span></a>
</span><span id="L-851"><a href="#L-851"><span class="linenos">851</span></a>		<span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span> <span class="o">=</span> <span class="n">arg_parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span><span id="L-852"><a href="#L-852"><span class="linenos">852</span></a>
</span><span id="L-853"><a href="#L-853"><span class="linenos">853</span></a>	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;args parsed: </span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-854"><a href="#L-854"><span class="linenos">854</span></a>
</span><span id="L-855"><a href="#L-855"><span class="linenos">855</span></a>	<span class="n">models</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
</span><span id="L-856"><a href="#L-856"><span class="linenos">856</span></a>	<span class="k">if</span> <span class="s2">&quot;,&quot;</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">:</span>
</span><span id="L-857"><a href="#L-857"><span class="linenos">857</span></a>		<span class="n">models</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
</span><span id="L-858"><a href="#L-858"><span class="linenos">858</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="L-859"><a href="#L-859"><span class="linenos">859</span></a>		<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">]</span>
</span><span id="L-860"><a href="#L-860"><span class="linenos">860</span></a>
</span><span id="L-861"><a href="#L-861"><span class="linenos">861</span></a>	<span class="n">n_models</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
</span><span id="L-862"><a href="#L-862"><span class="linenos">862</span></a>	<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
</span><span id="L-863"><a href="#L-863"><span class="linenos">863</span></a>		<span class="nb">print</span><span class="p">(</span><span class="n">DIVIDER_S2</span><span class="p">)</span>
</span><span id="L-864"><a href="#L-864"><span class="linenos">864</span></a>		<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;processing model </span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">n_models</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="L-865"><a href="#L-865"><span class="linenos">865</span></a>		<span class="nb">print</span><span class="p">(</span><span class="n">DIVIDER_S2</span><span class="p">)</span>
</span><span id="L-866"><a href="#L-866"><span class="linenos">866</span></a>
</span><span id="L-867"><a href="#L-867"><span class="linenos">867</span></a>		<span class="n">activations_main</span><span class="p">(</span>
</span><span id="L-868"><a href="#L-868"><span class="linenos">868</span></a>			<span class="n">model_name</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
</span><span id="L-869"><a href="#L-869"><span class="linenos">869</span></a>			<span class="n">save_path</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span>
</span><span id="L-870"><a href="#L-870"><span class="linenos">870</span></a>			<span class="n">prompts_path</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">prompts</span><span class="p">,</span>
</span><span id="L-871"><a href="#L-871"><span class="linenos">871</span></a>			<span class="n">raw_prompts</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">raw_prompts</span><span class="p">,</span>
</span><span id="L-872"><a href="#L-872"><span class="linenos">872</span></a>			<span class="n">min_chars</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">min_chars</span><span class="p">,</span>
</span><span id="L-873"><a href="#L-873"><span class="linenos">873</span></a>			<span class="n">max_chars</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_chars</span><span class="p">,</span>
</span><span id="L-874"><a href="#L-874"><span class="linenos">874</span></a>			<span class="n">force</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">force</span><span class="p">,</span>
</span><span id="L-875"><a href="#L-875"><span class="linenos">875</span></a>			<span class="n">n_samples</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_samples</span><span class="p">,</span>
</span><span id="L-876"><a href="#L-876"><span class="linenos">876</span></a>			<span class="n">no_index_html</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">no_index_html</span><span class="p">,</span>
</span><span id="L-877"><a href="#L-877"><span class="linenos">877</span></a>			<span class="n">shuffle</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">shuffle</span><span class="p">,</span>
</span><span id="L-878"><a href="#L-878"><span class="linenos">878</span></a>			<span class="n">stacked_heads</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">stacked_heads</span><span class="p">,</span>
</span><span id="L-879"><a href="#L-879"><span class="linenos">879</span></a>			<span class="n">device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
</span><span id="L-880"><a href="#L-880"><span class="linenos">880</span></a>			<span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
</span><span id="L-881"><a href="#L-881"><span class="linenos">881</span></a>		<span class="p">)</span>
</span><span id="L-882"><a href="#L-882"><span class="linenos">882</span></a>		<span class="k">del</span> <span class="n">model</span>
</span><span id="L-883"><a href="#L-883"><span class="linenos">883</span></a>
</span><span id="L-884"><a href="#L-884"><span class="linenos">884</span></a>	<span class="nb">print</span><span class="p">(</span><span class="n">DIVIDER_S1</span><span class="p">)</span>
</span><span id="L-885"><a href="#L-885"><span class="linenos">885</span></a>
</span><span id="L-886"><a href="#L-886"><span class="linenos">886</span></a>
</span><span id="L-887"><a href="#L-887"><span class="linenos">887</span></a><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span id="L-888"><a href="#L-888"><span class="linenos">888</span></a>	<span class="n">main</span><span class="p">()</span>
</span></pre></div>


                <br/>
            </section>
                <section id="compute_activations">
                            <input id="compute_activations-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">compute_activations</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span>,</span><span class="param">	<span class="n">model</span><span class="p">:</span> <span class="n">transformer_lens</span><span class="o">.</span><span class="n">HookedTransformer</span><span class="o">.</span><span class="n">HookedTransformer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">save_path</span><span class="p">:</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">_local</span><span class="o">.</span><span class="n">Path</span> <span class="o">=</span> <span class="n">PosixPath</span><span class="p">(</span><span class="s1">&#39;attn_data&#39;</span><span class="p">)</span>,</span><span class="param">	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;blocks</span><span class="se">\\</span><span class="s1">.(</span><span class="se">\\</span><span class="s1">d+)</span><span class="se">\\</span><span class="s1">.attn</span><span class="se">\\</span><span class="s1">.hook_pattern&#39;</span><span class="p">)</span>,</span><span class="param">	<span class="n">return_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;numpy&#39;</span><span class="p">,</span> <span class="s1">&#39;torch&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="s1">&#39;torch&#39;</span>,</span><span class="param">	<span class="n">stack_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">pathlib</span><span class="o">.</span><span class="n">_local</span><span class="o">.</span><span class="n">Path</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">|</span> <span class="n">transformer_lens</span><span class="o">.</span><span class="n">ActivationCache</span><span class="o">.</span><span class="n">ActivationCache</span> <span class="o">|</span> <span class="n">jaxtyping</span><span class="o">.</span><span class="n">Float</span><span class="p">[</span><span class="n">ndarray</span><span class="p">,</span> <span class="s1">&#39;n_layers n_heads n_ctx n_ctx&#39;</span><span class="p">]</span> <span class="o">|</span> <span class="n">jaxtyping</span><span class="o">.</span><span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s1">&#39;n_layers n_heads n_ctx n_ctx&#39;</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]</span>:</span></span>

                <div class="source-button-container">
            <label class="pdoc-button view-source-button" for="compute_activations-view-source"><span>View Source</span></label>
            <div class="github-button-wrapper">
                <a class="pdoc-button github-link-button" href="https://github.com/mivanit/pattern-lens/blob/0.6.0activations.py#L128-L292" target="_blank">
                    <span>View on GitHub</span>
                </a>
            </div>
        </div>

    </div>
    <a class="headerlink" href="#compute_activations"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="compute_activations-129"><a href="#compute_activations-129"><span class="linenos">129</span></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_activations</span><span class="p">(</span>  <span class="c1"># noqa: PLR0915</span>
</span><span id="compute_activations-130"><a href="#compute_activations-130"><span class="linenos">130</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="compute_activations-131"><a href="#compute_activations-131"><span class="linenos">131</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="compute_activations-132"><a href="#compute_activations-132"><span class="linenos">132</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="compute_activations-133"><a href="#compute_activations-133"><span class="linenos">133</span></a>	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">ATTN_PATTERN_REGEX</span><span class="p">,</span>
</span><span id="compute_activations-134"><a href="#compute_activations-134"><span class="linenos">134</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="n">ReturnCache</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span><span class="p">,</span>
</span><span id="compute_activations-135"><a href="#compute_activations-135"><span class="linenos">135</span></a>	<span class="n">stack_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="compute_activations-136"><a href="#compute_activations-136"><span class="linenos">136</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span>
</span><span id="compute_activations-137"><a href="#compute_activations-137"><span class="linenos">137</span></a>	<span class="n">Path</span><span class="p">,</span>
</span><span id="compute_activations-138"><a href="#compute_activations-138"><span class="linenos">138</span></a>	<span class="n">ActivationCacheNp</span>
</span><span id="compute_activations-139"><a href="#compute_activations-139"><span class="linenos">139</span></a>	<span class="o">|</span> <span class="n">ActivationCache</span>
</span><span id="compute_activations-140"><a href="#compute_activations-140"><span class="linenos">140</span></a>	<span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s2">&quot;n_layers n_heads n_ctx n_ctx&quot;</span><span class="p">]</span>
</span><span id="compute_activations-141"><a href="#compute_activations-141"><span class="linenos">141</span></a>	<span class="o">|</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;n_layers n_heads n_ctx n_ctx&quot;</span><span class="p">]</span>
</span><span id="compute_activations-142"><a href="#compute_activations-142"><span class="linenos">142</span></a>	<span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="compute_activations-143"><a href="#compute_activations-143"><span class="linenos">143</span></a><span class="p">]:</span>
</span><span id="compute_activations-144"><a href="#compute_activations-144"><span class="linenos">144</span></a><span class="w">	</span><span class="sd">&quot;&quot;&quot;compute activations for a single prompt and save to disk</span>
</span><span id="compute_activations-145"><a href="#compute_activations-145"><span class="linenos">145</span></a>
</span><span id="compute_activations-146"><a href="#compute_activations-146"><span class="linenos">146</span></a><span class="sd">	always runs a forward pass -- does NOT load from disk cache.</span>
</span><span id="compute_activations-147"><a href="#compute_activations-147"><span class="linenos">147</span></a><span class="sd">	for cache-aware loading, use `get_activations` which tries disk first.</span>
</span><span id="compute_activations-148"><a href="#compute_activations-148"><span class="linenos">148</span></a>
</span><span id="compute_activations-149"><a href="#compute_activations-149"><span class="linenos">149</span></a><span class="sd">	# Parameters:</span>
</span><span id="compute_activations-150"><a href="#compute_activations-150"><span class="linenos">150</span></a><span class="sd">	- `prompt : dict | None`</span>
</span><span id="compute_activations-151"><a href="#compute_activations-151"><span class="linenos">151</span></a><span class="sd">		(defaults to `None`)</span>
</span><span id="compute_activations-152"><a href="#compute_activations-152"><span class="linenos">152</span></a><span class="sd">	- `model : HookedTransformer`</span>
</span><span id="compute_activations-153"><a href="#compute_activations-153"><span class="linenos">153</span></a><span class="sd">	- `save_path : Path`</span>
</span><span id="compute_activations-154"><a href="#compute_activations-154"><span class="linenos">154</span></a><span class="sd">		(defaults to `Path(DATA_DIR)`)</span>
</span><span id="compute_activations-155"><a href="#compute_activations-155"><span class="linenos">155</span></a><span class="sd">	- `names_filter : Callable[[str], bool]|re.Pattern`</span>
</span><span id="compute_activations-156"><a href="#compute_activations-156"><span class="linenos">156</span></a><span class="sd">		a filter for the names of the activations to return. if an `re.Pattern`, will use `lambda key: names_filter.match(key) is not None`</span>
</span><span id="compute_activations-157"><a href="#compute_activations-157"><span class="linenos">157</span></a><span class="sd">		(defaults to `ATTN_PATTERN_REGEX`)</span>
</span><span id="compute_activations-158"><a href="#compute_activations-158"><span class="linenos">158</span></a><span class="sd">	- `return_cache : Literal[None, &quot;numpy&quot;, &quot;torch&quot;]`</span>
</span><span id="compute_activations-159"><a href="#compute_activations-159"><span class="linenos">159</span></a><span class="sd">		will return `None` as the second element if `None`, otherwise will return the cache in the specified tensor format. `stack_heads` still affects whether it will be a dict (False) or a single tensor (True)</span>
</span><span id="compute_activations-160"><a href="#compute_activations-160"><span class="linenos">160</span></a><span class="sd">		(defaults to `None`)</span>
</span><span id="compute_activations-161"><a href="#compute_activations-161"><span class="linenos">161</span></a><span class="sd">	- `stack_heads : bool`</span>
</span><span id="compute_activations-162"><a href="#compute_activations-162"><span class="linenos">162</span></a><span class="sd">		whether the heads should be stacked in the output. this causes a number of changes:</span>
</span><span id="compute_activations-163"><a href="#compute_activations-163"><span class="linenos">163</span></a><span class="sd">	- `npy` file with a single `(n_layers, n_heads, n_ctx, n_ctx)` tensor saved for each prompt instead of `npz` file with dict by layer</span>
</span><span id="compute_activations-164"><a href="#compute_activations-164"><span class="linenos">164</span></a><span class="sd">	- `cache` will be a single `(n_layers, n_heads, n_ctx, n_ctx)` tensor instead of a dict by layer if `return_cache` is `True`</span>
</span><span id="compute_activations-165"><a href="#compute_activations-165"><span class="linenos">165</span></a><span class="sd">		will assert that everything in the activation cache is only attention patterns, and is all of the attention patterns. raises an exception if not.</span>
</span><span id="compute_activations-166"><a href="#compute_activations-166"><span class="linenos">166</span></a>
</span><span id="compute_activations-167"><a href="#compute_activations-167"><span class="linenos">167</span></a><span class="sd">	# Returns:</span>
</span><span id="compute_activations-168"><a href="#compute_activations-168"><span class="linenos">168</span></a><span class="sd">	```</span>
</span><span id="compute_activations-169"><a href="#compute_activations-169"><span class="linenos">169</span></a><span class="sd">	tuple[</span>
</span><span id="compute_activations-170"><a href="#compute_activations-170"><span class="linenos">170</span></a><span class="sd">		Path,</span>
</span><span id="compute_activations-171"><a href="#compute_activations-171"><span class="linenos">171</span></a><span class="sd">		Union[</span>
</span><span id="compute_activations-172"><a href="#compute_activations-172"><span class="linenos">172</span></a><span class="sd">			None,</span>
</span><span id="compute_activations-173"><a href="#compute_activations-173"><span class="linenos">173</span></a><span class="sd">			ActivationCacheNp, ActivationCache,</span>
</span><span id="compute_activations-174"><a href="#compute_activations-174"><span class="linenos">174</span></a><span class="sd">			Float[np.ndarray, &quot;n_layers n_heads n_ctx n_ctx&quot;], Float[torch.Tensor, &quot;n_layers n_heads n_ctx n_ctx&quot;],</span>
</span><span id="compute_activations-175"><a href="#compute_activations-175"><span class="linenos">175</span></a><span class="sd">		]</span>
</span><span id="compute_activations-176"><a href="#compute_activations-176"><span class="linenos">176</span></a><span class="sd">	]</span>
</span><span id="compute_activations-177"><a href="#compute_activations-177"><span class="linenos">177</span></a><span class="sd">	```</span>
</span><span id="compute_activations-178"><a href="#compute_activations-178"><span class="linenos">178</span></a><span class="sd">	&quot;&quot;&quot;</span>
</span><span id="compute_activations-179"><a href="#compute_activations-179"><span class="linenos">179</span></a>	<span class="c1"># check inputs</span>
</span><span id="compute_activations-180"><a href="#compute_activations-180"><span class="linenos">180</span></a>	<span class="k">assert</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;model must be passed&quot;</span>
</span><span id="compute_activations-181"><a href="#compute_activations-181"><span class="linenos">181</span></a>	<span class="k">assert</span> <span class="s2">&quot;text&quot;</span> <span class="ow">in</span> <span class="n">prompt</span><span class="p">,</span> <span class="s2">&quot;prompt must contain &#39;text&#39; key&quot;</span>
</span><span id="compute_activations-182"><a href="#compute_activations-182"><span class="linenos">182</span></a>	<span class="n">prompt_str</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
</span><span id="compute_activations-183"><a href="#compute_activations-183"><span class="linenos">183</span></a>
</span><span id="compute_activations-184"><a href="#compute_activations-184"><span class="linenos">184</span></a>	<span class="c1"># compute or get prompt metadata</span>
</span><span id="compute_activations-185"><a href="#compute_activations-185"><span class="linenos">185</span></a>	<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="compute_activations-186"><a href="#compute_activations-186"><span class="linenos">186</span></a>	<span class="n">prompt_tokenized</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
</span><span id="compute_activations-187"><a href="#compute_activations-187"><span class="linenos">187</span></a>		<span class="s2">&quot;tokens&quot;</span><span class="p">,</span>
</span><span id="compute_activations-188"><a href="#compute_activations-188"><span class="linenos">188</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">prompt_str</span><span class="p">),</span>
</span><span id="compute_activations-189"><a href="#compute_activations-189"><span class="linenos">189</span></a>	<span class="p">)</span>
</span><span id="compute_activations-190"><a href="#compute_activations-190"><span class="linenos">190</span></a>	<span class="c1"># n_tokens counts subword tokens (no BOS); attention patterns include BOS</span>
</span><span id="compute_activations-191"><a href="#compute_activations-191"><span class="linenos">191</span></a>	<span class="c1"># so have dim n_tokens+1. see also compute_activations_batched Phase B.</span>
</span><span id="compute_activations-192"><a href="#compute_activations-192"><span class="linenos">192</span></a>	<span class="n">prompt</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
</span><span id="compute_activations-193"><a href="#compute_activations-193"><span class="linenos">193</span></a>		<span class="nb">dict</span><span class="p">(</span>
</span><span id="compute_activations-194"><a href="#compute_activations-194"><span class="linenos">194</span></a>			<span class="n">n_tokens</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokenized</span><span class="p">),</span>
</span><span id="compute_activations-195"><a href="#compute_activations-195"><span class="linenos">195</span></a>			<span class="n">tokens</span><span class="o">=</span><span class="n">prompt_tokenized</span><span class="p">,</span>
</span><span id="compute_activations-196"><a href="#compute_activations-196"><span class="linenos">196</span></a>		<span class="p">),</span>
</span><span id="compute_activations-197"><a href="#compute_activations-197"><span class="linenos">197</span></a>	<span class="p">)</span>
</span><span id="compute_activations-198"><a href="#compute_activations-198"><span class="linenos">198</span></a>
</span><span id="compute_activations-199"><a href="#compute_activations-199"><span class="linenos">199</span></a>	<span class="c1"># save metadata</span>
</span><span id="compute_activations-200"><a href="#compute_activations-200"><span class="linenos">200</span></a>	<span class="n">prompt_dir</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">save_path</span> <span class="o">/</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">model_name</span> <span class="o">/</span> <span class="s2">&quot;prompts&quot;</span> <span class="o">/</span> <span class="n">prompt</span><span class="p">[</span><span class="s2">&quot;hash&quot;</span><span class="p">]</span>
</span><span id="compute_activations-201"><a href="#compute_activations-201"><span class="linenos">201</span></a>	<span class="n">prompt_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="compute_activations-202"><a href="#compute_activations-202"><span class="linenos">202</span></a>	<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">prompt_dir</span> <span class="o">/</span> <span class="s2">&quot;prompt.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span id="compute_activations-203"><a href="#compute_activations-203"><span class="linenos">203</span></a>		<span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</span><span id="compute_activations-204"><a href="#compute_activations-204"><span class="linenos">204</span></a>
</span><span id="compute_activations-205"><a href="#compute_activations-205"><span class="linenos">205</span></a>	<span class="c1"># set up names filter</span>
</span><span id="compute_activations-206"><a href="#compute_activations-206"><span class="linenos">206</span></a>	<span class="n">names_filter_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span>
</span><span id="compute_activations-207"><a href="#compute_activations-207"><span class="linenos">207</span></a>	<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names_filter</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span><span class="p">):</span>
</span><span id="compute_activations-208"><a href="#compute_activations-208"><span class="linenos">208</span></a>		<span class="n">names_filter_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">key</span><span class="p">:</span> <span class="n">names_filter</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># noqa: E731</span>
</span><span id="compute_activations-209"><a href="#compute_activations-209"><span class="linenos">209</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="compute_activations-210"><a href="#compute_activations-210"><span class="linenos">210</span></a>		<span class="n">names_filter_fn</span> <span class="o">=</span> <span class="n">names_filter</span>
</span><span id="compute_activations-211"><a href="#compute_activations-211"><span class="linenos">211</span></a>
</span><span id="compute_activations-212"><a href="#compute_activations-212"><span class="linenos">212</span></a>	<span class="c1"># compute activations</span>
</span><span id="compute_activations-213"><a href="#compute_activations-213"><span class="linenos">213</span></a>	<span class="c1"># NOTE: no padding_side kwarg here -- it&#39;s only meaningful for multi-sequence</span>
</span><span id="compute_activations-214"><a href="#compute_activations-214"><span class="linenos">214</span></a>	<span class="c1"># batches where padding is needed. single-string input has no padding.</span>
</span><span id="compute_activations-215"><a href="#compute_activations-215"><span class="linenos">215</span></a>	<span class="c1"># see compute_activations_batched for the batched path that passes padding_side=&quot;right&quot;.</span>
</span><span id="compute_activations-216"><a href="#compute_activations-216"><span class="linenos">216</span></a>	<span class="n">cache_torch</span><span class="p">:</span> <span class="n">ActivationCache</span>
</span><span id="compute_activations-217"><a href="#compute_activations-217"><span class="linenos">217</span></a>	<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span><span id="compute_activations-218"><a href="#compute_activations-218"><span class="linenos">218</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="compute_activations-219"><a href="#compute_activations-219"><span class="linenos">219</span></a>		<span class="n">_</span><span class="p">,</span> <span class="n">cache_torch</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">run_with_cache</span><span class="p">(</span>
</span><span id="compute_activations-220"><a href="#compute_activations-220"><span class="linenos">220</span></a>			<span class="n">prompt_str</span><span class="p">,</span>
</span><span id="compute_activations-221"><a href="#compute_activations-221"><span class="linenos">221</span></a>			<span class="n">names_filter</span><span class="o">=</span><span class="n">names_filter_fn</span><span class="p">,</span>
</span><span id="compute_activations-222"><a href="#compute_activations-222"><span class="linenos">222</span></a>			<span class="n">return_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="compute_activations-223"><a href="#compute_activations-223"><span class="linenos">223</span></a>		<span class="p">)</span>
</span><span id="compute_activations-224"><a href="#compute_activations-224"><span class="linenos">224</span></a>
</span><span id="compute_activations-225"><a href="#compute_activations-225"><span class="linenos">225</span></a>	<span class="n">activations_path</span><span class="p">:</span> <span class="n">Path</span>
</span><span id="compute_activations-226"><a href="#compute_activations-226"><span class="linenos">226</span></a>	<span class="c1"># saving and returning</span>
</span><span id="compute_activations-227"><a href="#compute_activations-227"><span class="linenos">227</span></a>	<span class="k">if</span> <span class="n">stack_heads</span><span class="p">:</span>
</span><span id="compute_activations-228"><a href="#compute_activations-228"><span class="linenos">228</span></a>		<span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_layers</span>
</span><span id="compute_activations-229"><a href="#compute_activations-229"><span class="linenos">229</span></a>		<span class="n">key_pattern</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;blocks.</span><span class="si">{i}</span><span class="s2">.attn.hook_pattern&quot;</span>
</span><span id="compute_activations-230"><a href="#compute_activations-230"><span class="linenos">230</span></a>		<span class="c1"># NOTE: this only works for stacking heads at the moment</span>
</span><span id="compute_activations-231"><a href="#compute_activations-231"><span class="linenos">231</span></a>		<span class="c1"># activations_specifier: str = key_pattern.format(i=f&#39;0-{n_layers}&#39;)</span>
</span><span id="compute_activations-232"><a href="#compute_activations-232"><span class="linenos">232</span></a>		<span class="n">activations_specifier</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">key_pattern</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
</span><span id="compute_activations-233"><a href="#compute_activations-233"><span class="linenos">233</span></a>		<span class="n">activations_path</span> <span class="o">=</span> <span class="n">prompt_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;activations-</span><span class="si">{</span><span class="n">activations_specifier</span><span class="si">}</span><span class="s2">.npy&quot;</span>
</span><span id="compute_activations-234"><a href="#compute_activations-234"><span class="linenos">234</span></a>
</span><span id="compute_activations-235"><a href="#compute_activations-235"><span class="linenos">235</span></a>		<span class="c1"># check the keys are only attention heads</span>
</span><span id="compute_activations-236"><a href="#compute_activations-236"><span class="linenos">236</span></a>		<span class="n">head_keys</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">key_pattern</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)]</span>
</span><span id="compute_activations-237"><a href="#compute_activations-237"><span class="linenos">237</span></a>		<span class="n">cache_torch_keys_set</span><span class="p">:</span> <span class="nb">set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">cache_torch</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span><span id="compute_activations-238"><a href="#compute_activations-238"><span class="linenos">238</span></a>		<span class="k">assert</span> <span class="n">cache_torch_keys_set</span> <span class="o">==</span> <span class="nb">set</span><span class="p">(</span><span class="n">head_keys</span><span class="p">),</span> <span class="p">(</span>
</span><span id="compute_activations-239"><a href="#compute_activations-239"><span class="linenos">239</span></a>			<span class="sa">f</span><span class="s2">&quot;unexpected keys!</span><span class="se">\n</span><span class="si">{</span><span class="nb">set</span><span class="p">(</span><span class="n">head_keys</span><span class="p">)</span><span class="o">.</span><span class="n">symmetric_difference</span><span class="p">(</span><span class="n">cache_torch_keys_set</span><span class="p">)</span><span class="w"> </span><span class="si">= }</span><span class="se">\n</span><span class="si">{</span><span class="n">cache_torch_keys_set</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="nb">set</span><span class="p">(</span><span class="n">head_keys</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="compute_activations-240"><a href="#compute_activations-240"><span class="linenos">240</span></a>		<span class="p">)</span>
</span><span id="compute_activations-241"><a href="#compute_activations-241"><span class="linenos">241</span></a>
</span><span id="compute_activations-242"><a href="#compute_activations-242"><span class="linenos">242</span></a>		<span class="c1"># stack heads</span>
</span><span id="compute_activations-243"><a href="#compute_activations-243"><span class="linenos">243</span></a>		<span class="n">patterns_stacked</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;n_layers n_heads n_ctx n_ctx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="compute_activations-244"><a href="#compute_activations-244"><span class="linenos">244</span></a>			<span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">cache_torch</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">head_keys</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="compute_activations-245"><a href="#compute_activations-245"><span class="linenos">245</span></a>		<span class="p">)</span>
</span><span id="compute_activations-246"><a href="#compute_activations-246"><span class="linenos">246</span></a>		<span class="c1"># check shape</span>
</span><span id="compute_activations-247"><a href="#compute_activations-247"><span class="linenos">247</span></a>		<span class="n">pattern_shape_no_ctx</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">patterns_stacked</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>
</span><span id="compute_activations-248"><a href="#compute_activations-248"><span class="linenos">248</span></a>		<span class="k">assert</span> <span class="n">pattern_shape_no_ctx</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span> <span class="p">(</span>
</span><span id="compute_activations-249"><a href="#compute_activations-249"><span class="linenos">249</span></a>			<span class="sa">f</span><span class="s2">&quot;unexpected shape: </span><span class="si">{</span><span class="n">patterns_stacked</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span><span class="w"> </span><span class="si">= }</span><span class="s2"> (</span><span class="si">{</span><span class="n">pattern_shape_no_ctx</span><span class="w"> </span><span class="si">= }</span><span class="s2">), expected </span><span class="si">{</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">n_layers</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">n_heads</span><span class="p">)</span><span class="w"> </span><span class="si">= }</span><span class="s2">&quot;</span>
</span><span id="compute_activations-250"><a href="#compute_activations-250"><span class="linenos">250</span></a>		<span class="p">)</span>
</span><span id="compute_activations-251"><a href="#compute_activations-251"><span class="linenos">251</span></a>
</span><span id="compute_activations-252"><a href="#compute_activations-252"><span class="linenos">252</span></a>		<span class="n">patterns_stacked_np</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s2">&quot;n_layers n_heads n_ctx n_ctx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="compute_activations-253"><a href="#compute_activations-253"><span class="linenos">253</span></a>			<span class="n">patterns_stacked</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span><span id="compute_activations-254"><a href="#compute_activations-254"><span class="linenos">254</span></a>		<span class="p">)</span>
</span><span id="compute_activations-255"><a href="#compute_activations-255"><span class="linenos">255</span></a>
</span><span id="compute_activations-256"><a href="#compute_activations-256"><span class="linenos">256</span></a>		<span class="c1"># save</span>
</span><span id="compute_activations-257"><a href="#compute_activations-257"><span class="linenos">257</span></a>		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">activations_path</span><span class="p">,</span> <span class="n">patterns_stacked_np</span><span class="p">)</span>
</span><span id="compute_activations-258"><a href="#compute_activations-258"><span class="linenos">258</span></a>
</span><span id="compute_activations-259"><a href="#compute_activations-259"><span class="linenos">259</span></a>		<span class="c1"># return</span>
</span><span id="compute_activations-260"><a href="#compute_activations-260"><span class="linenos">260</span></a>		<span class="k">match</span> <span class="n">return_cache</span><span class="p">:</span>
</span><span id="compute_activations-261"><a href="#compute_activations-261"><span class="linenos">261</span></a>			<span class="k">case</span> <span class="s2">&quot;numpy&quot;</span><span class="p">:</span>
</span><span id="compute_activations-262"><a href="#compute_activations-262"><span class="linenos">262</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="n">patterns_stacked_np</span>
</span><span id="compute_activations-263"><a href="#compute_activations-263"><span class="linenos">263</span></a>			<span class="k">case</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
</span><span id="compute_activations-264"><a href="#compute_activations-264"><span class="linenos">264</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="n">patterns_stacked</span>
</span><span id="compute_activations-265"><a href="#compute_activations-265"><span class="linenos">265</span></a>			<span class="k">case</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="compute_activations-266"><a href="#compute_activations-266"><span class="linenos">266</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="compute_activations-267"><a href="#compute_activations-267"><span class="linenos">267</span></a>			<span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>
</span><span id="compute_activations-268"><a href="#compute_activations-268"><span class="linenos">268</span></a>				<span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;invalid return_cache: </span><span class="si">{</span><span class="n">return_cache</span><span class="w"> </span><span class="si">= }</span><span class="s2">&quot;</span>
</span><span id="compute_activations-269"><a href="#compute_activations-269"><span class="linenos">269</span></a>				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span id="compute_activations-270"><a href="#compute_activations-270"><span class="linenos">270</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="compute_activations-271"><a href="#compute_activations-271"><span class="linenos">271</span></a>		<span class="n">activations_path</span> <span class="o">=</span> <span class="n">prompt_dir</span> <span class="o">/</span> <span class="s2">&quot;activations.npz&quot;</span>
</span><span id="compute_activations-272"><a href="#compute_activations-272"><span class="linenos">272</span></a>
</span><span id="compute_activations-273"><a href="#compute_activations-273"><span class="linenos">273</span></a>		<span class="c1"># save</span>
</span><span id="compute_activations-274"><a href="#compute_activations-274"><span class="linenos">274</span></a>		<span class="n">cache_np</span><span class="p">:</span> <span class="n">ActivationCacheNp</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="compute_activations-275"><a href="#compute_activations-275"><span class="linenos">275</span></a>			<span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cache_torch</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
</span><span id="compute_activations-276"><a href="#compute_activations-276"><span class="linenos">276</span></a>		<span class="p">}</span>
</span><span id="compute_activations-277"><a href="#compute_activations-277"><span class="linenos">277</span></a>
</span><span id="compute_activations-278"><a href="#compute_activations-278"><span class="linenos">278</span></a>		<span class="n">np</span><span class="o">.</span><span class="n">savez_compressed</span><span class="p">(</span>
</span><span id="compute_activations-279"><a href="#compute_activations-279"><span class="linenos">279</span></a>			<span class="n">activations_path</span><span class="p">,</span>
</span><span id="compute_activations-280"><a href="#compute_activations-280"><span class="linenos">280</span></a>			<span class="o">**</span><span class="n">cache_np</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
</span><span id="compute_activations-281"><a href="#compute_activations-281"><span class="linenos">281</span></a>		<span class="p">)</span>
</span><span id="compute_activations-282"><a href="#compute_activations-282"><span class="linenos">282</span></a>
</span><span id="compute_activations-283"><a href="#compute_activations-283"><span class="linenos">283</span></a>		<span class="c1"># return</span>
</span><span id="compute_activations-284"><a href="#compute_activations-284"><span class="linenos">284</span></a>		<span class="k">match</span> <span class="n">return_cache</span><span class="p">:</span>
</span><span id="compute_activations-285"><a href="#compute_activations-285"><span class="linenos">285</span></a>			<span class="k">case</span> <span class="s2">&quot;numpy&quot;</span><span class="p">:</span>
</span><span id="compute_activations-286"><a href="#compute_activations-286"><span class="linenos">286</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="n">cache_np</span>
</span><span id="compute_activations-287"><a href="#compute_activations-287"><span class="linenos">287</span></a>			<span class="k">case</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
</span><span id="compute_activations-288"><a href="#compute_activations-288"><span class="linenos">288</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="n">cache_torch</span>
</span><span id="compute_activations-289"><a href="#compute_activations-289"><span class="linenos">289</span></a>			<span class="k">case</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="compute_activations-290"><a href="#compute_activations-290"><span class="linenos">290</span></a>				<span class="k">return</span> <span class="n">activations_path</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="compute_activations-291"><a href="#compute_activations-291"><span class="linenos">291</span></a>			<span class="k">case</span><span class="w"> </span><span class="k">_</span><span class="p">:</span>
</span><span id="compute_activations-292"><a href="#compute_activations-292"><span class="linenos">292</span></a>				<span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;invalid return_cache: </span><span class="si">{</span><span class="n">return_cache</span><span class="w"> </span><span class="si">= }</span><span class="s2">&quot;</span>
</span><span id="compute_activations-293"><a href="#compute_activations-293"><span class="linenos">293</span></a>				<span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>compute activations for a single prompt and save to disk</p>

<p>always runs a forward pass -- does NOT load from disk cache.
for cache-aware loading, use <code><a href="#get_activations">get_activations</a></code> which tries disk first.</p>

<h1 id="parameters">Parameters:</h1>

<ul>
<li><code>prompt : dict | None</code>
(defaults to <code>None</code>)</li>
<li><code>model : HookedTransformer</code></li>
<li><code>save_path : Path</code>
(defaults to <code>Path(DATA_DIR)</code>)</li>
<li><code>names_filter : Callable[[str], bool]|re.Pattern</code>
a filter for the names of the activations to return. if an <code>re.Pattern</code>, will use <code>lambda key: names_filter.match(key) is not None</code>
(defaults to <code>ATTN_PATTERN_REGEX</code>)</li>
<li><code>return_cache : Literal[None, "numpy", "torch"]</code>
will return <code>None</code> as the second element if <code>None</code>, otherwise will return the cache in the specified tensor format. <code>stack_heads</code> still affects whether it will be a dict (False) or a single tensor (True)
(defaults to <code>None</code>)</li>
<li><code>stack_heads : bool</code>
whether the heads should be stacked in the output. this causes a number of changes:</li>
<li><code>npy</code> file with a single <code>(n_layers, n_heads, n_ctx, n_ctx)</code> tensor saved for each prompt instead of <code>npz</code> file with dict by layer</li>
<li><code>cache</code> will be a single <code>(n_layers, n_heads, n_ctx, n_ctx)</code> tensor instead of a dict by layer if <code>return_cache</code> is <code>True</code>
will assert that everything in the activation cache is only attention patterns, and is all of the attention patterns. raises an exception if not.</li>
</ul>

<h1 id="returns">Returns:</h1>

<pre><code>tuple[
        Path,
        Union[
                None,
                ActivationCacheNp, ActivationCache,
                Float[np.ndarray, "n_layers n_heads n_ctx n_ctx"], Float[torch.Tensor, "n_layers n_heads n_ctx n_ctx"],
        ]
]
</code></pre>
</div>


                </section>
                <section id="compute_activations_batched">
                            <input id="compute_activations_batched-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">compute_activations_batched</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span>,</span><span class="param">	<span class="n">model</span><span class="p">:</span> <span class="n">transformer_lens</span><span class="o">.</span><span class="n">HookedTransformer</span><span class="o">.</span><span class="n">HookedTransformer</span>,</span><span class="param">	<span class="n">save_path</span><span class="p">:</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">_local</span><span class="o">.</span><span class="n">Path</span> <span class="o">=</span> <span class="n">PosixPath</span><span class="p">(</span><span class="s1">&#39;attn_data&#39;</span><span class="p">)</span>,</span><span class="param">	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;blocks</span><span class="se">\\</span><span class="s1">.(</span><span class="se">\\</span><span class="s1">d+)</span><span class="se">\\</span><span class="s1">.attn</span><span class="se">\\</span><span class="s1">.hook_pattern&#39;</span><span class="p">)</span>,</span><span class="param">	<span class="n">seq_lens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span></span><span class="return-annotation">) -> <span class="nb">list</span><span class="p">[</span><span class="n">pathlib</span><span class="o">.</span><span class="n">_local</span><span class="o">.</span><span class="n">Path</span><span class="p">]</span>:</span></span>

                <div class="source-button-container">
            <label class="pdoc-button view-source-button" for="compute_activations_batched-view-source"><span>View Source</span></label>
            <div class="github-button-wrapper">
                <a class="pdoc-button github-link-button" href="https://github.com/mivanit/pattern-lens/blob/0.6.0activations.py#L295-L438" target="_blank">
                    <span>View on GitHub</span>
                </a>
            </div>
        </div>

    </div>
    <a class="headerlink" href="#compute_activations_batched"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="compute_activations_batched-296"><a href="#compute_activations_batched-296"><span class="linenos">296</span></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_activations_batched</span><span class="p">(</span>
</span><span id="compute_activations_batched-297"><a href="#compute_activations_batched-297"><span class="linenos">297</span></a>	<span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span>
</span><span id="compute_activations_batched-298"><a href="#compute_activations_batched-298"><span class="linenos">298</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span><span class="p">,</span>
</span><span id="compute_activations_batched-299"><a href="#compute_activations_batched-299"><span class="linenos">299</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="compute_activations_batched-300"><a href="#compute_activations_batched-300"><span class="linenos">300</span></a>	<span class="n">names_filter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">|</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span> <span class="o">=</span> <span class="n">ATTN_PATTERN_REGEX</span><span class="p">,</span>
</span><span id="compute_activations_batched-301"><a href="#compute_activations_batched-301"><span class="linenos">301</span></a>	<span class="n">seq_lens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="compute_activations_batched-302"><a href="#compute_activations_batched-302"><span class="linenos">302</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Path</span><span class="p">]:</span>
</span><span id="compute_activations_batched-303"><a href="#compute_activations_batched-303"><span class="linenos">303</span></a><span class="w">	</span><span class="sd">&quot;&quot;&quot;compute and save activations for a batch of prompts in a single forward pass</span>
</span><span id="compute_activations_batched-304"><a href="#compute_activations_batched-304"><span class="linenos">304</span></a>
</span><span id="compute_activations_batched-305"><a href="#compute_activations_batched-305"><span class="linenos">305</span></a><span class="sd">	Batched companion to `compute_activations` -- instead of one forward pass per</span>
</span><span id="compute_activations_batched-306"><a href="#compute_activations_batched-306"><span class="linenos">306</span></a><span class="sd">	prompt, this runs a single `model.run_with_cache(list_of_strings)` call for the</span>
</span><span id="compute_activations_batched-307"><a href="#compute_activations_batched-307"><span class="linenos">307</span></a><span class="sd">	whole batch. TransformerLens tokenizes and right-pads automatically. Each prompt&#39;s</span>
</span><span id="compute_activations_batched-308"><a href="#compute_activations_batched-308"><span class="linenos">308</span></a><span class="sd">	attention patterns are then trimmed to their actual (unpadded) size and saved</span>
</span><span id="compute_activations_batched-309"><a href="#compute_activations_batched-309"><span class="linenos">309</span></a><span class="sd">	individually, producing files identical to the single-prompt path.</span>
</span><span id="compute_activations_batched-310"><a href="#compute_activations_batched-310"><span class="linenos">310</span></a>
</span><span id="compute_activations_batched-311"><a href="#compute_activations_batched-311"><span class="linenos">311</span></a><span class="sd">	Does not support `stack_heads` or `return_cache` -- this function is intended for</span>
</span><span id="compute_activations_batched-312"><a href="#compute_activations_batched-312"><span class="linenos">312</span></a><span class="sd">	the bulk processing path in `activations_main`, not for interactive use. Use</span>
</span><span id="compute_activations_batched-313"><a href="#compute_activations_batched-313"><span class="linenos">313</span></a><span class="sd">	`compute_activations` directly for single-prompt use cases that need those features.</span>
</span><span id="compute_activations_batched-314"><a href="#compute_activations_batched-314"><span class="linenos">314</span></a>
</span><span id="compute_activations_batched-315"><a href="#compute_activations_batched-315"><span class="linenos">315</span></a><span class="sd">	## Why right-padding makes trimming correct without an explicit attention mask</span>
</span><span id="compute_activations_batched-316"><a href="#compute_activations_batched-316"><span class="linenos">316</span></a>
</span><span id="compute_activations_batched-317"><a href="#compute_activations_batched-317"><span class="linenos">317</span></a><span class="sd">	With right-padding, pad tokens sit at positions seq_len, seq_len+1, ...,</span>
</span><span id="compute_activations_batched-318"><a href="#compute_activations_batched-318"><span class="linenos">318</span></a><span class="sd">	max_seq_len-1 (higher than any real token). The causal attention mask prevents</span>
</span><span id="compute_activations_batched-319"><a href="#compute_activations_batched-319"><span class="linenos">319</span></a><span class="sd">	position i from attending to any j &gt; i. So for real tokens at positions</span>
</span><span id="compute_activations_batched-320"><a href="#compute_activations_batched-320"><span class="linenos">320</span></a><span class="sd">	0..seq_len-1, they can only attend to 0..i -- all real tokens. The softmax is computed over the same set of positions</span>
</span><span id="compute_activations_batched-321"><a href="#compute_activations_batched-321"><span class="linenos">321</span></a><span class="sd">	as in single-prompt inference, producing identical attention patterns.</span>
</span><span id="compute_activations_batched-322"><a href="#compute_activations_batched-322"><span class="linenos">322</span></a>
</span><span id="compute_activations_batched-323"><a href="#compute_activations_batched-323"><span class="linenos">323</span></a><span class="sd">	We explicitly pass `padding_side=&quot;right&quot;` to `run_with_cache` to guarantee this</span>
</span><span id="compute_activations_batched-324"><a href="#compute_activations_batched-324"><span class="linenos">324</span></a><span class="sd">	regardless of the model&#39;s default padding side.</span>
</span><span id="compute_activations_batched-325"><a href="#compute_activations_batched-325"><span class="linenos">325</span></a>
</span><span id="compute_activations_batched-326"><a href="#compute_activations_batched-326"><span class="linenos">326</span></a><span class="sd">	# Parameters:</span>
</span><span id="compute_activations_batched-327"><a href="#compute_activations_batched-327"><span class="linenos">327</span></a><span class="sd">	- `prompts : list[dict]`</span>
</span><span id="compute_activations_batched-328"><a href="#compute_activations_batched-328"><span class="linenos">328</span></a><span class="sd">		each prompt must contain &#39;text&#39; and &#39;hash&#39; keys. call</span>
</span><span id="compute_activations_batched-329"><a href="#compute_activations_batched-329"><span class="linenos">329</span></a><span class="sd">		`augment_prompt_with_hash` on each prompt before passing them here.</span>
</span><span id="compute_activations_batched-330"><a href="#compute_activations_batched-330"><span class="linenos">330</span></a><span class="sd">	- `model : HookedTransformer`</span>
</span><span id="compute_activations_batched-331"><a href="#compute_activations_batched-331"><span class="linenos">331</span></a><span class="sd">		the model to compute activations with</span>
</span><span id="compute_activations_batched-332"><a href="#compute_activations_batched-332"><span class="linenos">332</span></a><span class="sd">	- `save_path : Path`</span>
</span><span id="compute_activations_batched-333"><a href="#compute_activations_batched-333"><span class="linenos">333</span></a><span class="sd">		path to save the activations to</span>
</span><span id="compute_activations_batched-334"><a href="#compute_activations_batched-334"><span class="linenos">334</span></a><span class="sd">		(defaults to `Path(DATA_DIR)`)</span>
</span><span id="compute_activations_batched-335"><a href="#compute_activations_batched-335"><span class="linenos">335</span></a><span class="sd">	- `names_filter : Callable[[str], bool] | re.Pattern`</span>
</span><span id="compute_activations_batched-336"><a href="#compute_activations_batched-336"><span class="linenos">336</span></a><span class="sd">		filter for which activations to save. must only match activations with</span>
</span><span id="compute_activations_batched-337"><a href="#compute_activations_batched-337"><span class="linenos">337</span></a><span class="sd">		4D shape `[batch, n_heads, seq, seq]` (e.g. attention patterns).</span>
</span><span id="compute_activations_batched-338"><a href="#compute_activations_batched-338"><span class="linenos">338</span></a><span class="sd">		non-attention activations will cause incorrect trimming.</span>
</span><span id="compute_activations_batched-339"><a href="#compute_activations_batched-339"><span class="linenos">339</span></a><span class="sd">		(defaults to `ATTN_PATTERN_REGEX`)</span>
</span><span id="compute_activations_batched-340"><a href="#compute_activations_batched-340"><span class="linenos">340</span></a><span class="sd">	- `seq_lens : list[int] | None`</span>
</span><span id="compute_activations_batched-341"><a href="#compute_activations_batched-341"><span class="linenos">341</span></a><span class="sd">		pre-computed model sequence lengths per prompt (from `model.to_tokens`).</span>
</span><span id="compute_activations_batched-342"><a href="#compute_activations_batched-342"><span class="linenos">342</span></a><span class="sd">		if `None`, will be computed internally. pass this to avoid redundant</span>
</span><span id="compute_activations_batched-343"><a href="#compute_activations_batched-343"><span class="linenos">343</span></a><span class="sd">		tokenization when lengths are already known (e.g. from length-sorting).</span>
</span><span id="compute_activations_batched-344"><a href="#compute_activations_batched-344"><span class="linenos">344</span></a><span class="sd">		**important**: these must be from `model.to_tokens()` (includes BOS),</span>
</span><span id="compute_activations_batched-345"><a href="#compute_activations_batched-345"><span class="linenos">345</span></a><span class="sd">		NOT from `model.tokenizer.tokenize()` (excludes BOS).</span>
</span><span id="compute_activations_batched-346"><a href="#compute_activations_batched-346"><span class="linenos">346</span></a><span class="sd">		(defaults to `None`)</span>
</span><span id="compute_activations_batched-347"><a href="#compute_activations_batched-347"><span class="linenos">347</span></a>
</span><span id="compute_activations_batched-348"><a href="#compute_activations_batched-348"><span class="linenos">348</span></a><span class="sd">	# Returns:</span>
</span><span id="compute_activations_batched-349"><a href="#compute_activations_batched-349"><span class="linenos">349</span></a><span class="sd">	- `list[Path]`</span>
</span><span id="compute_activations_batched-350"><a href="#compute_activations_batched-350"><span class="linenos">350</span></a><span class="sd">		paths to the saved activations files, one per prompt</span>
</span><span id="compute_activations_batched-351"><a href="#compute_activations_batched-351"><span class="linenos">351</span></a>
</span><span id="compute_activations_batched-352"><a href="#compute_activations_batched-352"><span class="linenos">352</span></a><span class="sd">	# Modifies:</span>
</span><span id="compute_activations_batched-353"><a href="#compute_activations_batched-353"><span class="linenos">353</span></a><span class="sd">	each prompt dict in `prompts` -- adds/overwrites `n_tokens` and `tokens` keys</span>
</span><span id="compute_activations_batched-354"><a href="#compute_activations_batched-354"><span class="linenos">354</span></a><span class="sd">	with tokenization metadata (same mutation as `compute_activations`).</span>
</span><span id="compute_activations_batched-355"><a href="#compute_activations_batched-355"><span class="linenos">355</span></a><span class="sd">	&quot;&quot;&quot;</span>
</span><span id="compute_activations_batched-356"><a href="#compute_activations_batched-356"><span class="linenos">356</span></a>	<span class="k">assert</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;model must be passed&quot;</span>
</span><span id="compute_activations_batched-357"><a href="#compute_activations_batched-357"><span class="linenos">357</span></a>	<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;prompts must not be empty&quot;</span>
</span><span id="compute_activations_batched-358"><a href="#compute_activations_batched-358"><span class="linenos">358</span></a>	<span class="k">assert</span> <span class="s2">&quot;text&quot;</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;prompt must contain &#39;text&#39; key: </span><span class="si">{</span><span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="compute_activations_batched-359"><a href="#compute_activations_batched-359"><span class="linenos">359</span></a>	<span class="k">assert</span> <span class="s2">&quot;hash&quot;</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span>
</span><span id="compute_activations_batched-360"><a href="#compute_activations_batched-360"><span class="linenos">360</span></a>		<span class="sa">f</span><span class="s2">&quot;prompt must contain &#39;hash&#39; key (call augment_prompt_with_hash first): </span><span class="si">{</span><span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="compute_activations_batched-361"><a href="#compute_activations_batched-361"><span class="linenos">361</span></a>	<span class="p">)</span>
</span><span id="compute_activations_batched-362"><a href="#compute_activations_batched-362"><span class="linenos">362</span></a>
</span><span id="compute_activations_batched-363"><a href="#compute_activations_batched-363"><span class="linenos">363</span></a>	<span class="c1"># --- Phase A: get actual model sequence lengths ---</span>
</span><span id="compute_activations_batched-364"><a href="#compute_activations_batched-364"><span class="linenos">364</span></a>	<span class="c1"># model.to_tokens() includes BOS if applicable, matching the attention pattern dims</span>
</span><span id="compute_activations_batched-365"><a href="#compute_activations_batched-365"><span class="linenos">365</span></a>	<span class="c1"># model.tokenizer.tokenize() gives subword strings WITHOUT BOS, used for metadata</span>
</span><span id="compute_activations_batched-366"><a href="#compute_activations_batched-366"><span class="linenos">366</span></a>	<span class="c1"># these differ by 1 when BOS is prepended -- using the wrong one for trimming</span>
</span><span id="compute_activations_batched-367"><a href="#compute_activations_batched-367"><span class="linenos">367</span></a>	<span class="c1"># would silently truncate or include garbage</span>
</span><span id="compute_activations_batched-368"><a href="#compute_activations_batched-368"><span class="linenos">368</span></a>	<span class="k">if</span> <span class="n">seq_lens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="compute_activations_batched-369"><a href="#compute_activations_batched-369"><span class="linenos">369</span></a>		<span class="n">seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
</span><span id="compute_activations_batched-370"><a href="#compute_activations_batched-370"><span class="linenos">370</span></a>	<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">),</span> <span class="p">(</span>
</span><span id="compute_activations_batched-371"><a href="#compute_activations_batched-371"><span class="linenos">371</span></a>		<span class="sa">f</span><span class="s2">&quot;seq_lens length mismatch: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">)</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="compute_activations_batched-372"><a href="#compute_activations_batched-372"><span class="linenos">372</span></a>	<span class="p">)</span>
</span><span id="compute_activations_batched-373"><a href="#compute_activations_batched-373"><span class="linenos">373</span></a>
</span><span id="compute_activations_batched-374"><a href="#compute_activations_batched-374"><span class="linenos">374</span></a>	<span class="c1"># --- Phase B: save prompt metadata (mirrors compute_activations&#39;s metadata logic) ---</span>
</span><span id="compute_activations_batched-375"><a href="#compute_activations_batched-375"><span class="linenos">375</span></a>	<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span><span id="compute_activations_batched-376"><a href="#compute_activations_batched-376"><span class="linenos">376</span></a>	<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
</span><span id="compute_activations_batched-377"><a href="#compute_activations_batched-377"><span class="linenos">377</span></a>		<span class="n">prompt_str</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
</span><span id="compute_activations_batched-378"><a href="#compute_activations_batched-378"><span class="linenos">378</span></a>		<span class="n">prompt_tokenized</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
</span><span id="compute_activations_batched-379"><a href="#compute_activations_batched-379"><span class="linenos">379</span></a>			<span class="s2">&quot;tokens&quot;</span><span class="p">,</span>
</span><span id="compute_activations_batched-380"><a href="#compute_activations_batched-380"><span class="linenos">380</span></a>			<span class="n">model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">prompt_str</span><span class="p">),</span>
</span><span id="compute_activations_batched-381"><a href="#compute_activations_batched-381"><span class="linenos">381</span></a>		<span class="p">)</span>
</span><span id="compute_activations_batched-382"><a href="#compute_activations_batched-382"><span class="linenos">382</span></a>		<span class="c1"># n_tokens counts subword tokens (no BOS); attention patterns include BOS so have dim n_tokens+1</span>
</span><span id="compute_activations_batched-383"><a href="#compute_activations_batched-383"><span class="linenos">383</span></a>		<span class="n">p</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
</span><span id="compute_activations_batched-384"><a href="#compute_activations_batched-384"><span class="linenos">384</span></a>			<span class="nb">dict</span><span class="p">(</span>
</span><span id="compute_activations_batched-385"><a href="#compute_activations_batched-385"><span class="linenos">385</span></a>				<span class="n">n_tokens</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_tokenized</span><span class="p">),</span>
</span><span id="compute_activations_batched-386"><a href="#compute_activations_batched-386"><span class="linenos">386</span></a>				<span class="n">tokens</span><span class="o">=</span><span class="n">prompt_tokenized</span><span class="p">,</span>
</span><span id="compute_activations_batched-387"><a href="#compute_activations_batched-387"><span class="linenos">387</span></a>			<span class="p">),</span>
</span><span id="compute_activations_batched-388"><a href="#compute_activations_batched-388"><span class="linenos">388</span></a>		<span class="p">)</span>
</span><span id="compute_activations_batched-389"><a href="#compute_activations_batched-389"><span class="linenos">389</span></a>		<span class="n">prompt_dir</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">save_path</span> <span class="o">/</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">model_name</span> <span class="o">/</span> <span class="s2">&quot;prompts&quot;</span> <span class="o">/</span> <span class="n">p</span><span class="p">[</span><span class="s2">&quot;hash&quot;</span><span class="p">]</span>
</span><span id="compute_activations_batched-390"><a href="#compute_activations_batched-390"><span class="linenos">390</span></a>		<span class="n">prompt_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="compute_activations_batched-391"><a href="#compute_activations_batched-391"><span class="linenos">391</span></a>		<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">prompt_dir</span> <span class="o">/</span> <span class="s2">&quot;prompt.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span id="compute_activations_batched-392"><a href="#compute_activations_batched-392"><span class="linenos">392</span></a>			<span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</span><span id="compute_activations_batched-393"><a href="#compute_activations_batched-393"><span class="linenos">393</span></a>
</span><span id="compute_activations_batched-394"><a href="#compute_activations_batched-394"><span class="linenos">394</span></a>	<span class="c1"># --- Phase C: batched forward pass ---</span>
</span><span id="compute_activations_batched-395"><a href="#compute_activations_batched-395"><span class="linenos">395</span></a>	<span class="n">names_filter_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span>
</span><span id="compute_activations_batched-396"><a href="#compute_activations_batched-396"><span class="linenos">396</span></a>	<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names_filter</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span><span class="p">):</span>
</span><span id="compute_activations_batched-397"><a href="#compute_activations_batched-397"><span class="linenos">397</span></a>		<span class="n">names_filter_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">key</span><span class="p">:</span> <span class="n">names_filter</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># noqa: E731</span>
</span><span id="compute_activations_batched-398"><a href="#compute_activations_batched-398"><span class="linenos">398</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="compute_activations_batched-399"><a href="#compute_activations_batched-399"><span class="linenos">399</span></a>		<span class="n">names_filter_fn</span> <span class="o">=</span> <span class="n">names_filter</span>
</span><span id="compute_activations_batched-400"><a href="#compute_activations_batched-400"><span class="linenos">400</span></a>
</span><span id="compute_activations_batched-401"><a href="#compute_activations_batched-401"><span class="linenos">401</span></a>	<span class="n">texts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
</span><span id="compute_activations_batched-402"><a href="#compute_activations_batched-402"><span class="linenos">402</span></a>	<span class="n">cache_torch</span><span class="p">:</span> <span class="n">ActivationCache</span>
</span><span id="compute_activations_batched-403"><a href="#compute_activations_batched-403"><span class="linenos">403</span></a>	<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span><span id="compute_activations_batched-404"><a href="#compute_activations_batched-404"><span class="linenos">404</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="compute_activations_batched-405"><a href="#compute_activations_batched-405"><span class="linenos">405</span></a>		<span class="n">_</span><span class="p">,</span> <span class="n">cache_torch</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">run_with_cache</span><span class="p">(</span>
</span><span id="compute_activations_batched-406"><a href="#compute_activations_batched-406"><span class="linenos">406</span></a>			<span class="n">texts</span><span class="p">,</span>
</span><span id="compute_activations_batched-407"><a href="#compute_activations_batched-407"><span class="linenos">407</span></a>			<span class="n">names_filter</span><span class="o">=</span><span class="n">names_filter_fn</span><span class="p">,</span>
</span><span id="compute_activations_batched-408"><a href="#compute_activations_batched-408"><span class="linenos">408</span></a>			<span class="n">return_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="compute_activations_batched-409"><a href="#compute_activations_batched-409"><span class="linenos">409</span></a>			<span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span>
</span><span id="compute_activations_batched-410"><a href="#compute_activations_batched-410"><span class="linenos">410</span></a>		<span class="p">)</span>
</span><span id="compute_activations_batched-411"><a href="#compute_activations_batched-411"><span class="linenos">411</span></a>
</span><span id="compute_activations_batched-412"><a href="#compute_activations_batched-412"><span class="linenos">412</span></a>	<span class="c1"># --- Phase D: split, trim padding, and save per-prompt ---</span>
</span><span id="compute_activations_batched-413"><a href="#compute_activations_batched-413"><span class="linenos">413</span></a>	<span class="c1"># For each prompt i with actual sequence length seq_len_i:</span>
</span><span id="compute_activations_batched-414"><a href="#compute_activations_batched-414"><span class="linenos">414</span></a>	<span class="c1">#   v[i : i+1, :, :seq_len_i, :seq_len_i]</span>
</span><span id="compute_activations_batched-415"><a href="#compute_activations_batched-415"><span class="linenos">415</span></a>	<span class="c1">#     ^^^^^^^                               i:i+1 not i -- keeps batch dim [1,...] for</span>
</span><span id="compute_activations_batched-416"><a href="#compute_activations_batched-416"><span class="linenos">416</span></a>	<span class="c1">#                                           format compatibility with compute_activations</span>
</span><span id="compute_activations_batched-417"><a href="#compute_activations_batched-417"><span class="linenos">417</span></a>	<span class="c1">#              ^^                           all attention heads</span>
</span><span id="compute_activations_batched-418"><a href="#compute_activations_batched-418"><span class="linenos">418</span></a>	<span class="c1">#                  ^^^^^^^^^^  ^^^^^^^^^^   trim both query and key dims to actual length,</span>
</span><span id="compute_activations_batched-419"><a href="#compute_activations_batched-419"><span class="linenos">419</span></a>	<span class="c1">#                                           discarding meaningless padding positions</span>
</span><span id="compute_activations_batched-420"><a href="#compute_activations_batched-420"><span class="linenos">420</span></a>	<span class="n">paths</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Path</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="compute_activations_batched-421"><a href="#compute_activations_batched-421"><span class="linenos">421</span></a>	<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">seq_lens</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)):</span>
</span><span id="compute_activations_batched-422"><a href="#compute_activations_batched-422"><span class="linenos">422</span></a>		<span class="n">prompt_dir</span> <span class="o">=</span> <span class="n">save_path</span> <span class="o">/</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">model_name</span> <span class="o">/</span> <span class="s2">&quot;prompts&quot;</span> <span class="o">/</span> <span class="n">prompt</span><span class="p">[</span><span class="s2">&quot;hash&quot;</span><span class="p">]</span>
</span><span id="compute_activations_batched-423"><a href="#compute_activations_batched-423"><span class="linenos">423</span></a>		<span class="n">activations_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">prompt_dir</span> <span class="o">/</span> <span class="s2">&quot;activations.npz&quot;</span>
</span><span id="compute_activations_batched-424"><a href="#compute_activations_batched-424"><span class="linenos">424</span></a>		<span class="n">cache_np</span><span class="p">:</span> <span class="n">ActivationCacheNp</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="compute_activations_batched-425"><a href="#compute_activations_batched-425"><span class="linenos">425</span></a>		<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cache_torch</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="compute_activations_batched-426"><a href="#compute_activations_batched-426"><span class="linenos">426</span></a>			<span class="k">assert</span> <span class="n">v</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span>  <span class="c1"># noqa: PLR2004</span>
</span><span id="compute_activations_batched-427"><a href="#compute_activations_batched-427"><span class="linenos">427</span></a>				<span class="sa">f</span><span class="s2">&quot;expected 4D attention pattern tensor for </span><span class="si">{</span><span class="n">k</span><span class="si">!r}</span><span class="s2">, &quot;</span>
</span><span id="compute_activations_batched-428"><a href="#compute_activations_batched-428"><span class="linenos">428</span></a>				<span class="sa">f</span><span class="s2">&quot;got shape </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. names_filter must only match &quot;</span>
</span><span id="compute_activations_batched-429"><a href="#compute_activations_batched-429"><span class="linenos">429</span></a>				<span class="sa">f</span><span class="s2">&quot;attention pattern activations [batch, n_heads, seq, seq]&quot;</span>
</span><span id="compute_activations_batched-430"><a href="#compute_activations_batched-430"><span class="linenos">430</span></a>			<span class="p">)</span>
</span><span id="compute_activations_batched-431"><a href="#compute_activations_batched-431"><span class="linenos">431</span></a>			<span class="n">cache_np</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span><span id="compute_activations_batched-432"><a href="#compute_activations_batched-432"><span class="linenos">432</span></a>
</span><span id="compute_activations_batched-433"><a href="#compute_activations_batched-433"><span class="linenos">433</span></a>		<span class="n">np</span><span class="o">.</span><span class="n">savez_compressed</span><span class="p">(</span>
</span><span id="compute_activations_batched-434"><a href="#compute_activations_batched-434"><span class="linenos">434</span></a>			<span class="n">activations_path</span><span class="p">,</span>
</span><span id="compute_activations_batched-435"><a href="#compute_activations_batched-435"><span class="linenos">435</span></a>			<span class="o">**</span><span class="n">cache_np</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
</span><span id="compute_activations_batched-436"><a href="#compute_activations_batched-436"><span class="linenos">436</span></a>		<span class="p">)</span>
</span><span id="compute_activations_batched-437"><a href="#compute_activations_batched-437"><span class="linenos">437</span></a>		<span class="n">paths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activations_path</span><span class="p">)</span>
</span><span id="compute_activations_batched-438"><a href="#compute_activations_batched-438"><span class="linenos">438</span></a>
</span><span id="compute_activations_batched-439"><a href="#compute_activations_batched-439"><span class="linenos">439</span></a>	<span class="k">return</span> <span class="n">paths</span>
</span></pre></div>


            <div class="docstring"><p>compute and save activations for a batch of prompts in a single forward pass</p>

<p>Batched companion to <code><a href="#compute_activations">compute_activations</a></code> -- instead of one forward pass per
prompt, this runs a single <code>model.run_with_cache(list_of_strings)</code> call for the
whole batch. TransformerLens tokenizes and right-pads automatically. Each prompt's
attention patterns are then trimmed to their actual (unpadded) size and saved
individually, producing files identical to the single-prompt path.</p>

<p>Does not support <code>stack_heads</code> or <code>return_cache</code> -- this function is intended for
the bulk processing path in <code><a href="#activations_main">activations_main</a></code>, not for interactive use. Use
<code><a href="#compute_activations">compute_activations</a></code> directly for single-prompt use cases that need those features.</p>

<h2 id="why-right-padding-makes-trimming-correct-without-an-explicit-attention-mask">Why right-padding makes trimming correct without an explicit attention mask</h2>

<p>With right-padding, pad tokens sit at positions seq_len, seq_len+1, ...,
max_seq_len-1 (higher than any real token). The causal attention mask prevents
position i from attending to any j &gt; i. So for real tokens at positions
0..seq_len-1, they can only attend to 0..i -- all real tokens. The softmax is computed over the same set of positions
as in single-prompt inference, producing identical attention patterns.</p>

<p>We explicitly pass <code>padding_side="right"</code> to <code>run_with_cache</code> to guarantee this
regardless of the model's default padding side.</p>

<h1 id="parameters">Parameters:</h1>

<ul>
<li><code>prompts : list[dict]</code>
each prompt must contain 'text' and 'hash' keys. call
<code>augment_prompt_with_hash</code> on each prompt before passing them here.</li>
<li><code>model : HookedTransformer</code>
the model to compute activations with</li>
<li><code>save_path : Path</code>
path to save the activations to
(defaults to <code>Path(DATA_DIR)</code>)</li>
<li><code>names_filter : Callable[[str], bool] | re.Pattern</code>
filter for which activations to save. must only match activations with
4D shape <code>[batch, n_heads, seq, seq]</code> (e.g. attention patterns).
non-attention activations will cause incorrect trimming.
(defaults to <code>ATTN_PATTERN_REGEX</code>)</li>
<li><code>seq_lens : list[int] | None</code>
pre-computed model sequence lengths per prompt (from <code>model.to_tokens</code>).
if <code>None</code>, will be computed internally. pass this to avoid redundant
tokenization when lengths are already known (e.g. from length-sorting).
<strong>important</strong>: these must be from <code>model.to_tokens()</code> (includes BOS),
NOT from <code>model.tokenizer.tokenize()</code> (excludes BOS).
(defaults to <code>None</code>)</li>
</ul>

<h1 id="returns">Returns:</h1>

<ul>
<li><code>list[Path]</code>
paths to the saved activations files, one per prompt</li>
</ul>

<h1 id="modifies">Modifies:</h1>

<p>each prompt dict in <code>prompts</code> -- adds/overwrites <code>n_tokens</code> and <code>tokens</code> keys
with tokenization metadata (same mutation as <code><a href="#compute_activations">compute_activations</a></code>).</p>
</div>


                </section>
                <section id="get_activations">
                            <input id="get_activations-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_activations</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span>,</span><span class="param">	<span class="n">model</span><span class="p">:</span> <span class="n">transformer_lens</span><span class="o">.</span><span class="n">HookedTransformer</span><span class="o">.</span><span class="n">HookedTransformer</span> <span class="o">|</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">save_path</span><span class="p">:</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">_local</span><span class="o">.</span><span class="n">Path</span> <span class="o">=</span> <span class="n">PosixPath</span><span class="p">(</span><span class="s1">&#39;attn_data&#39;</span><span class="p">)</span>,</span><span class="param">	<span class="n">allow_disk_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">return_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s1">&#39;numpy&#39;</span><span class="p">,</span> <span class="s1">&#39;torch&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="s1">&#39;numpy&#39;</span></span><span class="return-annotation">) -> <span class="nb">tuple</span><span class="p">[</span><span class="n">pathlib</span><span class="o">.</span><span class="n">_local</span><span class="o">.</span><span class="n">Path</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">|</span> <span class="n">transformer_lens</span><span class="o">.</span><span class="n">ActivationCache</span><span class="o">.</span><span class="n">ActivationCache</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]</span>:</span></span>

                <div class="source-button-container">
            <label class="pdoc-button view-source-button" for="get_activations-view-source"><span>View Source</span></label>
            <div class="github-button-wrapper">
                <a class="pdoc-button github-link-button" href="https://github.com/mivanit/pattern-lens/blob/0.6.0activations.py#L465-L533" target="_blank">
                    <span>View on GitHub</span>
                </a>
            </div>
        </div>

    </div>
    <a class="headerlink" href="#get_activations"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="get_activations-466"><a href="#get_activations-466"><span class="linenos">466</span></a><span class="k">def</span><span class="w"> </span><span class="nf">get_activations</span><span class="p">(</span>
</span><span id="get_activations-467"><a href="#get_activations-467"><span class="linenos">467</span></a>	<span class="n">prompt</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
</span><span id="get_activations-468"><a href="#get_activations-468"><span class="linenos">468</span></a>	<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">|</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="get_activations-469"><a href="#get_activations-469"><span class="linenos">469</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">),</span>
</span><span id="get_activations-470"><a href="#get_activations-470"><span class="linenos">470</span></a>	<span class="n">allow_disk_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="get_activations-471"><a href="#get_activations-471"><span class="linenos">471</span></a>	<span class="n">return_cache</span><span class="p">:</span> <span class="n">ReturnCache</span> <span class="o">=</span> <span class="s2">&quot;numpy&quot;</span><span class="p">,</span>
</span><span id="get_activations-472"><a href="#get_activations-472"><span class="linenos">472</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="n">ActivationCacheNp</span> <span class="o">|</span> <span class="n">ActivationCache</span> <span class="o">|</span> <span class="kc">None</span><span class="p">]:</span>
</span><span id="get_activations-473"><a href="#get_activations-473"><span class="linenos">473</span></a><span class="w">	</span><span class="sd">&quot;&quot;&quot;given a prompt and a model, save or load activations</span>
</span><span id="get_activations-474"><a href="#get_activations-474"><span class="linenos">474</span></a>
</span><span id="get_activations-475"><a href="#get_activations-475"><span class="linenos">475</span></a><span class="sd">	# Parameters:</span>
</span><span id="get_activations-476"><a href="#get_activations-476"><span class="linenos">476</span></a><span class="sd">	- `prompt : dict`</span>
</span><span id="get_activations-477"><a href="#get_activations-477"><span class="linenos">477</span></a><span class="sd">		expected to contain the &#39;text&#39; key</span>
</span><span id="get_activations-478"><a href="#get_activations-478"><span class="linenos">478</span></a><span class="sd">	- `model : HookedTransformer | str`</span>
</span><span id="get_activations-479"><a href="#get_activations-479"><span class="linenos">479</span></a><span class="sd">		either a `HookedTransformer` or a string model name, to be loaded with `HookedTransformer.from_pretrained`</span>
</span><span id="get_activations-480"><a href="#get_activations-480"><span class="linenos">480</span></a><span class="sd">	- `save_path : Path`</span>
</span><span id="get_activations-481"><a href="#get_activations-481"><span class="linenos">481</span></a><span class="sd">		path to save the activations to (and load from)</span>
</span><span id="get_activations-482"><a href="#get_activations-482"><span class="linenos">482</span></a><span class="sd">		(defaults to `Path(DATA_DIR)`)</span>
</span><span id="get_activations-483"><a href="#get_activations-483"><span class="linenos">483</span></a><span class="sd">	- `allow_disk_cache : bool`</span>
</span><span id="get_activations-484"><a href="#get_activations-484"><span class="linenos">484</span></a><span class="sd">		whether to allow loading from disk cache</span>
</span><span id="get_activations-485"><a href="#get_activations-485"><span class="linenos">485</span></a><span class="sd">		(defaults to `True`)</span>
</span><span id="get_activations-486"><a href="#get_activations-486"><span class="linenos">486</span></a><span class="sd">	- `return_cache : Literal[None, &quot;numpy&quot;, &quot;torch&quot;]`</span>
</span><span id="get_activations-487"><a href="#get_activations-487"><span class="linenos">487</span></a><span class="sd">		whether to return the cache, and in what format</span>
</span><span id="get_activations-488"><a href="#get_activations-488"><span class="linenos">488</span></a><span class="sd">		(defaults to `&quot;numpy&quot;`)</span>
</span><span id="get_activations-489"><a href="#get_activations-489"><span class="linenos">489</span></a>
</span><span id="get_activations-490"><a href="#get_activations-490"><span class="linenos">490</span></a><span class="sd">	# Returns:</span>
</span><span id="get_activations-491"><a href="#get_activations-491"><span class="linenos">491</span></a><span class="sd">	- `tuple[Path, ActivationCacheNp | ActivationCache | None]`</span>
</span><span id="get_activations-492"><a href="#get_activations-492"><span class="linenos">492</span></a><span class="sd">		the path to the activations and the cache if `return_cache is not None`</span>
</span><span id="get_activations-493"><a href="#get_activations-493"><span class="linenos">493</span></a>
</span><span id="get_activations-494"><a href="#get_activations-494"><span class="linenos">494</span></a><span class="sd">	&quot;&quot;&quot;</span>
</span><span id="get_activations-495"><a href="#get_activations-495"><span class="linenos">495</span></a>	<span class="c1"># add hash to prompt</span>
</span><span id="get_activations-496"><a href="#get_activations-496"><span class="linenos">496</span></a>	<span class="n">augment_prompt_with_hash</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</span><span id="get_activations-497"><a href="#get_activations-497"><span class="linenos">497</span></a>
</span><span id="get_activations-498"><a href="#get_activations-498"><span class="linenos">498</span></a>	<span class="c1"># get the model</span>
</span><span id="get_activations-499"><a href="#get_activations-499"><span class="linenos">499</span></a>	<span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="get_activations-500"><a href="#get_activations-500"><span class="linenos">500</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">model_name</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">HookedTransformer</span><span class="p">)</span> <span class="k">else</span> <span class="n">model</span>
</span><span id="get_activations-501"><a href="#get_activations-501"><span class="linenos">501</span></a>	<span class="p">)</span>
</span><span id="get_activations-502"><a href="#get_activations-502"><span class="linenos">502</span></a>
</span><span id="get_activations-503"><a href="#get_activations-503"><span class="linenos">503</span></a>	<span class="c1"># from cache</span>
</span><span id="get_activations-504"><a href="#get_activations-504"><span class="linenos">504</span></a>	<span class="k">if</span> <span class="n">allow_disk_cache</span><span class="p">:</span>
</span><span id="get_activations-505"><a href="#get_activations-505"><span class="linenos">505</span></a>		<span class="k">if</span> <span class="n">return_cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="get_activations-506"><a href="#get_activations-506"><span class="linenos">506</span></a>			<span class="c1"># fast path: check file existence without loading data into memory.</span>
</span><span id="get_activations-507"><a href="#get_activations-507"><span class="linenos">507</span></a>			<span class="c1"># activations_exist just calls .exists() on two paths, whereas</span>
</span><span id="get_activations-508"><a href="#get_activations-508"><span class="linenos">508</span></a>			<span class="c1"># load_activations would decompress the full .npz into numpy arrays</span>
</span><span id="get_activations-509"><a href="#get_activations-509"><span class="linenos">509</span></a>			<span class="c1"># only for us to discard them immediately.</span>
</span><span id="get_activations-510"><a href="#get_activations-510"><span class="linenos">510</span></a>			<span class="k">if</span> <span class="n">activations_exist</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">save_path</span><span class="p">):</span>
</span><span id="get_activations-511"><a href="#get_activations-511"><span class="linenos">511</span></a>				<span class="n">prompt_dir</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">save_path</span> <span class="o">/</span> <span class="n">model_name</span> <span class="o">/</span> <span class="s2">&quot;prompts&quot;</span> <span class="o">/</span> <span class="n">prompt</span><span class="p">[</span><span class="s2">&quot;hash&quot;</span><span class="p">]</span>
</span><span id="get_activations-512"><a href="#get_activations-512"><span class="linenos">512</span></a>				<span class="k">return</span> <span class="n">prompt_dir</span> <span class="o">/</span> <span class="s2">&quot;activations.npz&quot;</span><span class="p">,</span> <span class="kc">None</span>
</span><span id="get_activations-513"><a href="#get_activations-513"><span class="linenos">513</span></a>		<span class="k">else</span><span class="p">:</span>
</span><span id="get_activations-514"><a href="#get_activations-514"><span class="linenos">514</span></a>			<span class="k">try</span><span class="p">:</span>
</span><span id="get_activations-515"><a href="#get_activations-515"><span class="linenos">515</span></a>				<span class="n">path</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">load_activations</span><span class="p">(</span>
</span><span id="get_activations-516"><a href="#get_activations-516"><span class="linenos">516</span></a>					<span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
</span><span id="get_activations-517"><a href="#get_activations-517"><span class="linenos">517</span></a>					<span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
</span><span id="get_activations-518"><a href="#get_activations-518"><span class="linenos">518</span></a>					<span class="n">save_path</span><span class="o">=</span><span class="n">save_path</span><span class="p">,</span>
</span><span id="get_activations-519"><a href="#get_activations-519"><span class="linenos">519</span></a>				<span class="p">)</span>
</span><span id="get_activations-520"><a href="#get_activations-520"><span class="linenos">520</span></a>			<span class="k">except</span> <span class="n">ActivationsMissingError</span><span class="p">:</span>
</span><span id="get_activations-521"><a href="#get_activations-521"><span class="linenos">521</span></a>				<span class="k">pass</span>
</span><span id="get_activations-522"><a href="#get_activations-522"><span class="linenos">522</span></a>			<span class="k">else</span><span class="p">:</span>
</span><span id="get_activations-523"><a href="#get_activations-523"><span class="linenos">523</span></a>				<span class="k">return</span> <span class="n">path</span><span class="p">,</span> <span class="n">cache</span>
</span><span id="get_activations-524"><a href="#get_activations-524"><span class="linenos">524</span></a>
</span><span id="get_activations-525"><a href="#get_activations-525"><span class="linenos">525</span></a>	<span class="c1"># compute them</span>
</span><span id="get_activations-526"><a href="#get_activations-526"><span class="linenos">526</span></a>	<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="get_activations-527"><a href="#get_activations-527"><span class="linenos">527</span></a>		<span class="n">model</span> <span class="o">=</span> <span class="n">HookedTransformer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</span><span id="get_activations-528"><a href="#get_activations-528"><span class="linenos">528</span></a>
</span><span id="get_activations-529"><a href="#get_activations-529"><span class="linenos">529</span></a>	<span class="k">return</span> <span class="n">compute_activations</span><span class="p">(</span>  <span class="c1"># type: ignore[return-value]</span>
</span><span id="get_activations-530"><a href="#get_activations-530"><span class="linenos">530</span></a>		<span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
</span><span id="get_activations-531"><a href="#get_activations-531"><span class="linenos">531</span></a>		<span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
</span><span id="get_activations-532"><a href="#get_activations-532"><span class="linenos">532</span></a>		<span class="n">save_path</span><span class="o">=</span><span class="n">save_path</span><span class="p">,</span>
</span><span id="get_activations-533"><a href="#get_activations-533"><span class="linenos">533</span></a>		<span class="n">return_cache</span><span class="o">=</span><span class="n">return_cache</span><span class="p">,</span>
</span><span id="get_activations-534"><a href="#get_activations-534"><span class="linenos">534</span></a>	<span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>given a prompt and a model, save or load activations</p>

<h1 id="parameters">Parameters:</h1>

<ul>
<li><code>prompt : dict</code>
expected to contain the 'text' key</li>
<li><code>model : HookedTransformer | str</code>
either a <code>HookedTransformer</code> or a string model name, to be loaded with <code>HookedTransformer.from_pretrained</code></li>
<li><code>save_path : Path</code>
path to save the activations to (and load from)
(defaults to <code>Path(DATA_DIR)</code>)</li>
<li><code>allow_disk_cache : bool</code>
whether to allow loading from disk cache
(defaults to <code>True</code>)</li>
<li><code>return_cache : Literal[None, "numpy", "torch"]</code>
whether to return the cache, and in what format
(defaults to <code>"numpy"</code>)</li>
</ul>

<h1 id="returns">Returns:</h1>

<ul>
<li><code>tuple[Path, ActivationCacheNp | ActivationCache | None]</code>
the path to the activations and the cache if <code>return_cache is not None</code></li>
</ul>
</div>


                </section>
                <section id="DEFAULT_DEVICE">
                    <div class="attr variable">
            <span class="name">DEFAULT_DEVICE</span><span class="annotation">: torch.device</span>        =
<span class="default_value">device(type=&#39;cuda&#39;)</span>

        
    </div>
    <a class="headerlink" href="#DEFAULT_DEVICE"></a>
    
    

                </section>
                <section id="activations_main">
                            <input id="activations_main-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">activations_main</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">save_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">_local</span><span class="o">.</span><span class="n">Path</span>,</span><span class="param">	<span class="n">prompts_path</span><span class="p">:</span> <span class="nb">str</span>,</span><span class="param">	<span class="n">raw_prompts</span><span class="p">:</span> <span class="nb">bool</span>,</span><span class="param">	<span class="n">min_chars</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">max_chars</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">force</span><span class="p">:</span> <span class="nb">bool</span>,</span><span class="param">	<span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">no_index_html</span><span class="p">:</span> <span class="nb">bool</span>,</span><span class="param">	<span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">stacked_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>,</span><span class="param">	<span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span></span><span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <div class="source-button-container">
            <label class="pdoc-button view-source-button" for="activations_main-view-source"><span>View Source</span></label>
            <div class="github-button-wrapper">
                <a class="pdoc-button github-link-button" href="https://github.com/mivanit/pattern-lens/blob/0.6.0activations.py#L541-L734" target="_blank">
                    <span>View on GitHub</span>
                </a>
            </div>
        </div>

    </div>
    <a class="headerlink" href="#activations_main"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="activations_main-542"><a href="#activations_main-542"><span class="linenos">542</span></a><span class="k">def</span><span class="w"> </span><span class="nf">activations_main</span><span class="p">(</span>  <span class="c1"># noqa: C901, PLR0912, PLR0915</span>
</span><span id="activations_main-543"><a href="#activations_main-543"><span class="linenos">543</span></a>	<span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="activations_main-544"><a href="#activations_main-544"><span class="linenos">544</span></a>	<span class="n">save_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Path</span><span class="p">,</span>
</span><span id="activations_main-545"><a href="#activations_main-545"><span class="linenos">545</span></a>	<span class="n">prompts_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="activations_main-546"><a href="#activations_main-546"><span class="linenos">546</span></a>	<span class="n">raw_prompts</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="activations_main-547"><a href="#activations_main-547"><span class="linenos">547</span></a>	<span class="n">min_chars</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="activations_main-548"><a href="#activations_main-548"><span class="linenos">548</span></a>	<span class="n">max_chars</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="activations_main-549"><a href="#activations_main-549"><span class="linenos">549</span></a>	<span class="n">force</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="activations_main-550"><a href="#activations_main-550"><span class="linenos">550</span></a>	<span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="activations_main-551"><a href="#activations_main-551"><span class="linenos">551</span></a>	<span class="n">no_index_html</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
</span><span id="activations_main-552"><a href="#activations_main-552"><span class="linenos">552</span></a>	<span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="activations_main-553"><a href="#activations_main-553"><span class="linenos">553</span></a>	<span class="n">stacked_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="activations_main-554"><a href="#activations_main-554"><span class="linenos">554</span></a>	<span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">DEFAULT_DEVICE</span><span class="p">,</span>
</span><span id="activations_main-555"><a href="#activations_main-555"><span class="linenos">555</span></a>	<span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
</span><span id="activations_main-556"><a href="#activations_main-556"><span class="linenos">556</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="activations_main-557"><a href="#activations_main-557"><span class="linenos">557</span></a><span class="w">	</span><span class="sd">&quot;&quot;&quot;main function for computing activations</span>
</span><span id="activations_main-558"><a href="#activations_main-558"><span class="linenos">558</span></a>
</span><span id="activations_main-559"><a href="#activations_main-559"><span class="linenos">559</span></a><span class="sd">	# Parameters:</span>
</span><span id="activations_main-560"><a href="#activations_main-560"><span class="linenos">560</span></a><span class="sd">	- `model_name : str`</span>
</span><span id="activations_main-561"><a href="#activations_main-561"><span class="linenos">561</span></a><span class="sd">		name of a model to load with `HookedTransformer.from_pretrained`</span>
</span><span id="activations_main-562"><a href="#activations_main-562"><span class="linenos">562</span></a><span class="sd">	- `save_path : str | Path`</span>
</span><span id="activations_main-563"><a href="#activations_main-563"><span class="linenos">563</span></a><span class="sd">		path to save the activations to</span>
</span><span id="activations_main-564"><a href="#activations_main-564"><span class="linenos">564</span></a><span class="sd">	- `prompts_path : str`</span>
</span><span id="activations_main-565"><a href="#activations_main-565"><span class="linenos">565</span></a><span class="sd">		path to the prompts file</span>
</span><span id="activations_main-566"><a href="#activations_main-566"><span class="linenos">566</span></a><span class="sd">	- `raw_prompts : bool`</span>
</span><span id="activations_main-567"><a href="#activations_main-567"><span class="linenos">567</span></a><span class="sd">		whether the prompts are raw, not filtered by length. `load_text_data` will be called if `True`, otherwise just load the &quot;text&quot; field from each line in `prompts_path`</span>
</span><span id="activations_main-568"><a href="#activations_main-568"><span class="linenos">568</span></a><span class="sd">	- `min_chars : int`</span>
</span><span id="activations_main-569"><a href="#activations_main-569"><span class="linenos">569</span></a><span class="sd">		minimum number of characters for a prompt</span>
</span><span id="activations_main-570"><a href="#activations_main-570"><span class="linenos">570</span></a><span class="sd">	- `max_chars : int`</span>
</span><span id="activations_main-571"><a href="#activations_main-571"><span class="linenos">571</span></a><span class="sd">		maximum number of characters for a prompt</span>
</span><span id="activations_main-572"><a href="#activations_main-572"><span class="linenos">572</span></a><span class="sd">	- `force : bool`</span>
</span><span id="activations_main-573"><a href="#activations_main-573"><span class="linenos">573</span></a><span class="sd">		whether to overwrite existing files</span>
</span><span id="activations_main-574"><a href="#activations_main-574"><span class="linenos">574</span></a><span class="sd">	- `n_samples : int`</span>
</span><span id="activations_main-575"><a href="#activations_main-575"><span class="linenos">575</span></a><span class="sd">		maximum number of samples to process</span>
</span><span id="activations_main-576"><a href="#activations_main-576"><span class="linenos">576</span></a><span class="sd">	- `no_index_html : bool`</span>
</span><span id="activations_main-577"><a href="#activations_main-577"><span class="linenos">577</span></a><span class="sd">		whether to write an index.html file</span>
</span><span id="activations_main-578"><a href="#activations_main-578"><span class="linenos">578</span></a><span class="sd">	- `shuffle : bool`</span>
</span><span id="activations_main-579"><a href="#activations_main-579"><span class="linenos">579</span></a><span class="sd">		whether to shuffle the prompts</span>
</span><span id="activations_main-580"><a href="#activations_main-580"><span class="linenos">580</span></a><span class="sd">		(defaults to `False`)</span>
</span><span id="activations_main-581"><a href="#activations_main-581"><span class="linenos">581</span></a><span class="sd">	- `stacked_heads : bool`</span>
</span><span id="activations_main-582"><a href="#activations_main-582"><span class="linenos">582</span></a><span class="sd">		whether	to stack the heads in the output tensor. will save as `.npy` instead of `.npz` if `True`</span>
</span><span id="activations_main-583"><a href="#activations_main-583"><span class="linenos">583</span></a><span class="sd">		(defaults to `False`)</span>
</span><span id="activations_main-584"><a href="#activations_main-584"><span class="linenos">584</span></a><span class="sd">	- `device : str | torch.device`</span>
</span><span id="activations_main-585"><a href="#activations_main-585"><span class="linenos">585</span></a><span class="sd">		the device to use. if a string, will be passed to `torch.device`</span>
</span><span id="activations_main-586"><a href="#activations_main-586"><span class="linenos">586</span></a><span class="sd">	- `batch_size : int`</span>
</span><span id="activations_main-587"><a href="#activations_main-587"><span class="linenos">587</span></a><span class="sd">		number of prompts per forward pass. prompts are sorted by token length</span>
</span><span id="activations_main-588"><a href="#activations_main-588"><span class="linenos">588</span></a><span class="sd">		(longest first) and grouped so that similar-length prompts share a batch,</span>
</span><span id="activations_main-589"><a href="#activations_main-589"><span class="linenos">589</span></a><span class="sd">		minimizing padding waste. use `batch_size=1` for one prompt per forward</span>
</span><span id="activations_main-590"><a href="#activations_main-590"><span class="linenos">590</span></a><span class="sd">		pass (largely equivalent to the old sequential behavior, but note: prompts</span>
</span><span id="activations_main-591"><a href="#activations_main-591"><span class="linenos">591</span></a><span class="sd">		are still sorted by length and cache checking uses file-existence only,</span>
</span><span id="activations_main-592"><a href="#activations_main-592"><span class="linenos">592</span></a><span class="sd">		unlike the old path which processed prompts in order and validated cache</span>
</span><span id="activations_main-593"><a href="#activations_main-593"><span class="linenos">593</span></a><span class="sd">		contents via `load_activations`).</span>
</span><span id="activations_main-594"><a href="#activations_main-594"><span class="linenos">594</span></a><span class="sd">		the single-prompt functions `compute_activations` and `get_activations`</span>
</span><span id="activations_main-595"><a href="#activations_main-595"><span class="linenos">595</span></a><span class="sd">		are still available for programmatic use outside of `activations_main`.</span>
</span><span id="activations_main-596"><a href="#activations_main-596"><span class="linenos">596</span></a><span class="sd">		(defaults to `32`)</span>
</span><span id="activations_main-597"><a href="#activations_main-597"><span class="linenos">597</span></a><span class="sd">	&quot;&quot;&quot;</span>
</span><span id="activations_main-598"><a href="#activations_main-598"><span class="linenos">598</span></a>	<span class="c1"># figure out the device to use</span>
</span><span id="activations_main-599"><a href="#activations_main-599"><span class="linenos">599</span></a>	<span class="n">device_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
</span><span id="activations_main-600"><a href="#activations_main-600"><span class="linenos">600</span></a>	<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
</span><span id="activations_main-601"><a href="#activations_main-601"><span class="linenos">601</span></a>		<span class="n">device_</span> <span class="o">=</span> <span class="n">device</span>
</span><span id="activations_main-602"><a href="#activations_main-602"><span class="linenos">602</span></a>	<span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
</span><span id="activations_main-603"><a href="#activations_main-603"><span class="linenos">603</span></a>		<span class="n">device_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span><span id="activations_main-604"><a href="#activations_main-604"><span class="linenos">604</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="activations_main-605"><a href="#activations_main-605"><span class="linenos">605</span></a>		<span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;invalid device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="activations_main-606"><a href="#activations_main-606"><span class="linenos">606</span></a>		<span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</span><span id="activations_main-607"><a href="#activations_main-607"><span class="linenos">607</span></a>
</span><span id="activations_main-608"><a href="#activations_main-608"><span class="linenos">608</span></a>	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;using device: </span><span class="si">{</span><span class="n">device_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="activations_main-609"><a href="#activations_main-609"><span class="linenos">609</span></a>
</span><span id="activations_main-610"><a href="#activations_main-610"><span class="linenos">610</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;loading model&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">):</span>
</span><span id="activations_main-611"><a href="#activations_main-611"><span class="linenos">611</span></a>		<span class="n">model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">=</span> <span class="n">HookedTransformer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span><span id="activations_main-612"><a href="#activations_main-612"><span class="linenos">612</span></a>			<span class="n">model_name</span><span class="p">,</span>
</span><span id="activations_main-613"><a href="#activations_main-613"><span class="linenos">613</span></a>			<span class="n">device</span><span class="o">=</span><span class="n">device_</span><span class="p">,</span>
</span><span id="activations_main-614"><a href="#activations_main-614"><span class="linenos">614</span></a>		<span class="p">)</span>
</span><span id="activations_main-615"><a href="#activations_main-615"><span class="linenos">615</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>  <span class="c1"># type: ignore[unresolved-attribute]</span>
</span><span id="activations_main-616"><a href="#activations_main-616"><span class="linenos">616</span></a>		<span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
</span><span id="activations_main-617"><a href="#activations_main-617"><span class="linenos">617</span></a>		<span class="n">n_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</span><span id="activations_main-618"><a href="#activations_main-618"><span class="linenos">618</span></a>	<span class="nb">print</span><span class="p">(</span>
</span><span id="activations_main-619"><a href="#activations_main-619"><span class="linenos">619</span></a>		<span class="sa">f</span><span class="s2">&quot;loaded </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="n">shorten_numerical_to_str</span><span class="p">(</span><span class="n">n_params</span><span class="p">)</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">n_params</span><span class="si">}</span><span class="s2">) parameters&quot;</span><span class="p">,</span>
</span><span id="activations_main-620"><a href="#activations_main-620"><span class="linenos">620</span></a>	<span class="p">)</span>
</span><span id="activations_main-621"><a href="#activations_main-621"><span class="linenos">621</span></a>	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">model devices: </span><span class="si">{</span><span class="w"> </span><span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span><span class="w"> </span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="activations_main-622"><a href="#activations_main-622"><span class="linenos">622</span></a>
</span><span id="activations_main-623"><a href="#activations_main-623"><span class="linenos">623</span></a>	<span class="n">save_path_p</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
</span><span id="activations_main-624"><a href="#activations_main-624"><span class="linenos">624</span></a>	<span class="n">save_path_p</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="activations_main-625"><a href="#activations_main-625"><span class="linenos">625</span></a>	<span class="n">model_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">save_path_p</span> <span class="o">/</span> <span class="n">model_name</span>
</span><span id="activations_main-626"><a href="#activations_main-626"><span class="linenos">626</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span>
</span><span id="activations_main-627"><a href="#activations_main-627"><span class="linenos">627</span></a>		<span class="n">message</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;saving model info to </span><span class="si">{</span><span class="n">_rel_path</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="activations_main-628"><a href="#activations_main-628"><span class="linenos">628</span></a>		<span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="activations_main-629"><a href="#activations_main-629"><span class="linenos">629</span></a>	<span class="p">):</span>
</span><span id="activations_main-630"><a href="#activations_main-630"><span class="linenos">630</span></a>		<span class="n">model_cfg</span><span class="p">:</span> <span class="n">HookedTransformerConfig</span>
</span><span id="activations_main-631"><a href="#activations_main-631"><span class="linenos">631</span></a>		<span class="n">model_cfg</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cfg</span>
</span><span id="activations_main-632"><a href="#activations_main-632"><span class="linenos">632</span></a>		<span class="n">model_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="activations_main-633"><a href="#activations_main-633"><span class="linenos">633</span></a>		<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span> <span class="o">/</span> <span class="s2">&quot;model_cfg.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span id="activations_main-634"><a href="#activations_main-634"><span class="linenos">634</span></a>			<span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">json_serialize</span><span class="p">(</span><span class="n">asdict</span><span class="p">(</span><span class="n">model_cfg</span><span class="p">)),</span> <span class="n">f</span><span class="p">)</span>
</span><span id="activations_main-635"><a href="#activations_main-635"><span class="linenos">635</span></a>
</span><span id="activations_main-636"><a href="#activations_main-636"><span class="linenos">636</span></a>	<span class="c1"># load prompts</span>
</span><span id="activations_main-637"><a href="#activations_main-637"><span class="linenos">637</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span>
</span><span id="activations_main-638"><a href="#activations_main-638"><span class="linenos">638</span></a>		<span class="n">message</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;loading prompts from </span><span class="si">{</span><span class="n">Path</span><span class="p">(</span><span class="n">prompts_path</span><span class="p">)</span><span class="o">.</span><span class="n">as_posix</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="activations_main-639"><a href="#activations_main-639"><span class="linenos">639</span></a>		<span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="activations_main-640"><a href="#activations_main-640"><span class="linenos">640</span></a>	<span class="p">):</span>
</span><span id="activations_main-641"><a href="#activations_main-641"><span class="linenos">641</span></a>		<span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span>
</span><span id="activations_main-642"><a href="#activations_main-642"><span class="linenos">642</span></a>		<span class="k">if</span> <span class="n">raw_prompts</span><span class="p">:</span>
</span><span id="activations_main-643"><a href="#activations_main-643"><span class="linenos">643</span></a>			<span class="n">prompts</span> <span class="o">=</span> <span class="n">load_text_data</span><span class="p">(</span>
</span><span id="activations_main-644"><a href="#activations_main-644"><span class="linenos">644</span></a>				<span class="n">Path</span><span class="p">(</span><span class="n">prompts_path</span><span class="p">),</span>
</span><span id="activations_main-645"><a href="#activations_main-645"><span class="linenos">645</span></a>				<span class="n">min_chars</span><span class="o">=</span><span class="n">min_chars</span><span class="p">,</span>
</span><span id="activations_main-646"><a href="#activations_main-646"><span class="linenos">646</span></a>				<span class="n">max_chars</span><span class="o">=</span><span class="n">max_chars</span><span class="p">,</span>
</span><span id="activations_main-647"><a href="#activations_main-647"><span class="linenos">647</span></a>				<span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
</span><span id="activations_main-648"><a href="#activations_main-648"><span class="linenos">648</span></a>			<span class="p">)</span>
</span><span id="activations_main-649"><a href="#activations_main-649"><span class="linenos">649</span></a>		<span class="k">else</span><span class="p">:</span>
</span><span id="activations_main-650"><a href="#activations_main-650"><span class="linenos">650</span></a>			<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span> <span class="o">/</span> <span class="s2">&quot;prompts.jsonl&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span id="activations_main-651"><a href="#activations_main-651"><span class="linenos">651</span></a>				<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>
</span><span id="activations_main-652"><a href="#activations_main-652"><span class="linenos">652</span></a>		<span class="c1"># truncate to n_samples</span>
</span><span id="activations_main-653"><a href="#activations_main-653"><span class="linenos">653</span></a>		<span class="n">prompts</span> <span class="o">=</span> <span class="n">prompts</span><span class="p">[:</span><span class="n">n_samples</span><span class="p">]</span>
</span><span id="activations_main-654"><a href="#activations_main-654"><span class="linenos">654</span></a>
</span><span id="activations_main-655"><a href="#activations_main-655"><span class="linenos">655</span></a>	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span><span class="si">}</span><span class="s2"> prompts loaded&quot;</span><span class="p">)</span>
</span><span id="activations_main-656"><a href="#activations_main-656"><span class="linenos">656</span></a>
</span><span id="activations_main-657"><a href="#activations_main-657"><span class="linenos">657</span></a>	<span class="c1"># write index.html</span>
</span><span id="activations_main-658"><a href="#activations_main-658"><span class="linenos">658</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span>
</span><span id="activations_main-659"><a href="#activations_main-659"><span class="linenos">659</span></a>		<span class="n">message</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;writing </span><span class="si">{</span><span class="n">_rel_path</span><span class="p">(</span><span class="n">save_path_p</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="s1">&#39;index.html&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
</span><span id="activations_main-660"><a href="#activations_main-660"><span class="linenos">660</span></a>		<span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="activations_main-661"><a href="#activations_main-661"><span class="linenos">661</span></a>	<span class="p">):</span>
</span><span id="activations_main-662"><a href="#activations_main-662"><span class="linenos">662</span></a>		<span class="k">if</span> <span class="ow">not</span> <span class="n">no_index_html</span><span class="p">:</span>
</span><span id="activations_main-663"><a href="#activations_main-663"><span class="linenos">663</span></a>			<span class="n">write_html_index</span><span class="p">(</span><span class="n">save_path_p</span><span class="p">)</span>
</span><span id="activations_main-664"><a href="#activations_main-664"><span class="linenos">664</span></a>
</span><span id="activations_main-665"><a href="#activations_main-665"><span class="linenos">665</span></a>	<span class="c1"># TODO: not implemented yet</span>
</span><span id="activations_main-666"><a href="#activations_main-666"><span class="linenos">666</span></a>	<span class="k">if</span> <span class="n">stacked_heads</span><span class="p">:</span>
</span><span id="activations_main-667"><a href="#activations_main-667"><span class="linenos">667</span></a>		<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;stacked_heads not implemented yet&quot;</span><span class="p">)</span>
</span><span id="activations_main-668"><a href="#activations_main-668"><span class="linenos">668</span></a>
</span><span id="activations_main-669"><a href="#activations_main-669"><span class="linenos">669</span></a>	<span class="c1"># augment all prompts with hashes</span>
</span><span id="activations_main-670"><a href="#activations_main-670"><span class="linenos">670</span></a>	<span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
</span><span id="activations_main-671"><a href="#activations_main-671"><span class="linenos">671</span></a>		<span class="n">augment_prompt_with_hash</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</span><span id="activations_main-672"><a href="#activations_main-672"><span class="linenos">672</span></a>
</span><span id="activations_main-673"><a href="#activations_main-673"><span class="linenos">673</span></a>	<span class="c1"># filter out cached prompts</span>
</span><span id="activations_main-674"><a href="#activations_main-674"><span class="linenos">674</span></a>	<span class="k">if</span> <span class="ow">not</span> <span class="n">force</span><span class="p">:</span>
</span><span id="activations_main-675"><a href="#activations_main-675"><span class="linenos">675</span></a>		<span class="n">uncached</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="activations_main-676"><a href="#activations_main-676"><span class="linenos">676</span></a>			<span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">activations_exist</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">save_path_p</span><span class="p">)</span>
</span><span id="activations_main-677"><a href="#activations_main-677"><span class="linenos">677</span></a>		<span class="p">]</span>
</span><span id="activations_main-678"><a href="#activations_main-678"><span class="linenos">678</span></a>		<span class="n">n_cached</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">uncached</span><span class="p">)</span>
</span><span id="activations_main-679"><a href="#activations_main-679"><span class="linenos">679</span></a>		<span class="k">if</span> <span class="n">n_cached</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="activations_main-680"><a href="#activations_main-680"><span class="linenos">680</span></a>			<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">n_cached</span><span class="si">}</span><span class="s2"> prompts already cached, </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">uncached</span><span class="p">)</span><span class="si">}</span><span class="s2"> to compute&quot;</span><span class="p">)</span>
</span><span id="activations_main-681"><a href="#activations_main-681"><span class="linenos">681</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="activations_main-682"><a href="#activations_main-682"><span class="linenos">682</span></a>		<span class="n">uncached</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
</span><span id="activations_main-683"><a href="#activations_main-683"><span class="linenos">683</span></a>
</span><span id="activations_main-684"><a href="#activations_main-684"><span class="linenos">684</span></a>	<span class="k">if</span> <span class="n">uncached</span><span class="p">:</span>
</span><span id="activations_main-685"><a href="#activations_main-685"><span class="linenos">685</span></a>		<span class="c1"># sort by token length descending so that:</span>
</span><span id="activations_main-686"><a href="#activations_main-686"><span class="linenos">686</span></a>		<span class="c1"># 1. the longest (slowest, most memory-hungry) batches run first --</span>
</span><span id="activations_main-687"><a href="#activations_main-687"><span class="linenos">687</span></a>		<span class="c1">#    OOM errors surface immediately rather than after all the cheap work,</span>
</span><span id="activations_main-688"><a href="#activations_main-688"><span class="linenos">688</span></a>		<span class="c1">#    and tqdm&#39;s ETA stabilizes early for better progress estimation</span>
</span><span id="activations_main-689"><a href="#activations_main-689"><span class="linenos">689</span></a>		<span class="c1"># 2. similar-length prompts are grouped together, minimizing padding waste</span>
</span><span id="activations_main-690"><a href="#activations_main-690"><span class="linenos">690</span></a>		<span class="c1">#</span>
</span><span id="activations_main-691"><a href="#activations_main-691"><span class="linenos">691</span></a>		<span class="c1"># pre-tokenization is a separate step from compute_activations_batched because</span>
</span><span id="activations_main-692"><a href="#activations_main-692"><span class="linenos">692</span></a>		<span class="c1"># we need token lengths *before* batching to sort and group. the resulting</span>
</span><span id="activations_main-693"><a href="#activations_main-693"><span class="linenos">693</span></a>		<span class="c1"># seq_lens are then passed through so compute_activations_batched can skip</span>
</span><span id="activations_main-694"><a href="#activations_main-694"><span class="linenos">694</span></a>		<span class="c1"># re-tokenizing each prompt internally.</span>
</span><span id="activations_main-695"><a href="#activations_main-695"><span class="linenos">695</span></a>		<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span>
</span><span id="activations_main-696"><a href="#activations_main-696"><span class="linenos">696</span></a>			<span class="n">message</span><span class="o">=</span><span class="s2">&quot;pre-tokenizing prompts for length sorting&quot;</span><span class="p">,</span>
</span><span id="activations_main-697"><a href="#activations_main-697"><span class="linenos">697</span></a>			<span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="activations_main-698"><a href="#activations_main-698"><span class="linenos">698</span></a>		<span class="p">):</span>
</span><span id="activations_main-699"><a href="#activations_main-699"><span class="linenos">699</span></a>			<span class="n">uncached_with_lens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="activations_main-700"><a href="#activations_main-700"><span class="linenos">700</span></a>				<span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">to_tokens</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">uncached</span>
</span><span id="activations_main-701"><a href="#activations_main-701"><span class="linenos">701</span></a>			<span class="p">]</span>
</span><span id="activations_main-702"><a href="#activations_main-702"><span class="linenos">702</span></a>			<span class="n">uncached_with_lens</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="activations_main-703"><a href="#activations_main-703"><span class="linenos">703</span></a>			<span class="n">sorted_uncached</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">uncached_with_lens</span><span class="p">]</span>
</span><span id="activations_main-704"><a href="#activations_main-704"><span class="linenos">704</span></a>			<span class="n">sorted_seq_lens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">sl</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">sl</span> <span class="ow">in</span> <span class="n">uncached_with_lens</span><span class="p">]</span>
</span><span id="activations_main-705"><a href="#activations_main-705"><span class="linenos">705</span></a>
</span><span id="activations_main-706"><a href="#activations_main-706"><span class="linenos">706</span></a>		<span class="c1"># process in batches</span>
</span><span id="activations_main-707"><a href="#activations_main-707"><span class="linenos">707</span></a>		<span class="n">n_prompts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sorted_uncached</span><span class="p">)</span>
</span><span id="activations_main-708"><a href="#activations_main-708"><span class="linenos">708</span></a>		<span class="k">with</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span>
</span><span id="activations_main-709"><a href="#activations_main-709"><span class="linenos">709</span></a>			<span class="n">total</span><span class="o">=</span><span class="n">n_prompts</span><span class="p">,</span>
</span><span id="activations_main-710"><a href="#activations_main-710"><span class="linenos">710</span></a>			<span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Computing activations&quot;</span><span class="p">,</span>
</span><span id="activations_main-711"><a href="#activations_main-711"><span class="linenos">711</span></a>			<span class="n">unit</span><span class="o">=</span><span class="s2">&quot;prompt&quot;</span><span class="p">,</span>
</span><span id="activations_main-712"><a href="#activations_main-712"><span class="linenos">712</span></a>		<span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
</span><span id="activations_main-713"><a href="#activations_main-713"><span class="linenos">713</span></a>			<span class="k">for</span> <span class="n">batch_start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_prompts</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
</span><span id="activations_main-714"><a href="#activations_main-714"><span class="linenos">714</span></a>				<span class="n">batch_end</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">batch_start</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_prompts</span><span class="p">)</span>
</span><span id="activations_main-715"><a href="#activations_main-715"><span class="linenos">715</span></a>				<span class="n">batch</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="n">sorted_uncached</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span>
</span><span id="activations_main-716"><a href="#activations_main-716"><span class="linenos">716</span></a>				<span class="n">batch_seq_lens</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">sorted_seq_lens</span><span class="p">[</span><span class="n">batch_start</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]</span>
</span><span id="activations_main-717"><a href="#activations_main-717"><span class="linenos">717</span></a>				<span class="n">pbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span>
</span><span id="activations_main-718"><a href="#activations_main-718"><span class="linenos">718</span></a>					<span class="n">n_ctx</span><span class="o">=</span><span class="n">batch_seq_lens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span><span id="activations_main-719"><a href="#activations_main-719"><span class="linenos">719</span></a>				<span class="p">)</span>  <span class="c1"># longest in batch (sorted descending)</span>
</span><span id="activations_main-720"><a href="#activations_main-720"><span class="linenos">720</span></a>				<span class="n">compute_activations_batched</span><span class="p">(</span>
</span><span id="activations_main-721"><a href="#activations_main-721"><span class="linenos">721</span></a>					<span class="n">prompts</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
</span><span id="activations_main-722"><a href="#activations_main-722"><span class="linenos">722</span></a>					<span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
</span><span id="activations_main-723"><a href="#activations_main-723"><span class="linenos">723</span></a>					<span class="n">save_path</span><span class="o">=</span><span class="n">save_path_p</span><span class="p">,</span>
</span><span id="activations_main-724"><a href="#activations_main-724"><span class="linenos">724</span></a>					<span class="n">seq_lens</span><span class="o">=</span><span class="n">batch_seq_lens</span><span class="p">,</span>
</span><span id="activations_main-725"><a href="#activations_main-725"><span class="linenos">725</span></a>				<span class="p">)</span>
</span><span id="activations_main-726"><a href="#activations_main-726"><span class="linenos">726</span></a>				<span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
</span><span id="activations_main-727"><a href="#activations_main-727"><span class="linenos">727</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="activations_main-728"><a href="#activations_main-728"><span class="linenos">728</span></a>		<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  all prompts cached, nothing to compute&quot;</span><span class="p">)</span>
</span><span id="activations_main-729"><a href="#activations_main-729"><span class="linenos">729</span></a>
</span><span id="activations_main-730"><a href="#activations_main-730"><span class="linenos">730</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span>
</span><span id="activations_main-731"><a href="#activations_main-731"><span class="linenos">731</span></a>		<span class="n">message</span><span class="o">=</span><span class="s2">&quot;updating jsonl metadata for models and prompts&quot;</span><span class="p">,</span>
</span><span id="activations_main-732"><a href="#activations_main-732"><span class="linenos">732</span></a>		<span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">,</span>
</span><span id="activations_main-733"><a href="#activations_main-733"><span class="linenos">733</span></a>	<span class="p">):</span>
</span><span id="activations_main-734"><a href="#activations_main-734"><span class="linenos">734</span></a>		<span class="n">generate_models_jsonl</span><span class="p">(</span><span class="n">save_path_p</span><span class="p">)</span>
</span><span id="activations_main-735"><a href="#activations_main-735"><span class="linenos">735</span></a>		<span class="n">generate_prompts_jsonl</span><span class="p">(</span><span class="n">save_path_p</span> <span class="o">/</span> <span class="n">model_name</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>main function for computing activations</p>

<h1 id="parameters">Parameters:</h1>

<ul>
<li><code>model_name : str</code>
name of a model to load with <code>HookedTransformer.from_pretrained</code></li>
<li><code>save_path : str | Path</code>
path to save the activations to</li>
<li><code>prompts_path : str</code>
path to the prompts file</li>
<li><code>raw_prompts : bool</code>
whether the prompts are raw, not filtered by length. <code>load_text_data</code> will be called if <code>True</code>, otherwise just load the "text" field from each line in <code>prompts_path</code></li>
<li><code>min_chars : int</code>
minimum number of characters for a prompt</li>
<li><code>max_chars : int</code>
maximum number of characters for a prompt</li>
<li><code>force : bool</code>
whether to overwrite existing files</li>
<li><code>n_samples : int</code>
maximum number of samples to process</li>
<li><code>no_index_html : bool</code>
whether to write an index.html file</li>
<li><code>shuffle : bool</code>
whether to shuffle the prompts
(defaults to <code>False</code>)</li>
<li><code>stacked_heads : bool</code>
whether to stack the heads in the output tensor. will save as <code>.npy</code> instead of <code>.npz</code> if <code>True</code>
(defaults to <code>False</code>)</li>
<li><code>device : str | torch.device</code>
the device to use. if a string, will be passed to <code>torch.device</code></li>
<li><code>batch_size : int</code>
number of prompts per forward pass. prompts are sorted by token length
(longest first) and grouped so that similar-length prompts share a batch,
minimizing padding waste. use <code>batch_size=1</code> for one prompt per forward
pass (largely equivalent to the old sequential behavior, but note: prompts
are still sorted by length and cache checking uses file-existence only,
unlike the old path which processed prompts in order and validated cache
contents via <code>load_activations</code>).
the single-prompt functions <code><a href="#compute_activations">compute_activations</a></code> and <code><a href="#get_activations">get_activations</a></code>
are still available for programmatic use outside of <code><a href="#activations_main">activations_main</a></code>.
(defaults to <code>32</code>)</li>
</ul>
</div>


                </section>
                <section id="main">
                            <input id="main-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">main</span><span class="signature pdoc-code condensed">(<span class="return-annotation">) -> <span class="kc">None</span>:</span></span>

                <div class="source-button-container">
            <label class="pdoc-button view-source-button" for="main-view-source"><span>View Source</span></label>
            <div class="github-button-wrapper">
                <a class="pdoc-button github-link-button" href="https://github.com/mivanit/pattern-lens/blob/0.6.0activations.py#L737-L884" target="_blank">
                    <span>View on GitHub</span>
                </a>
            </div>
        </div>

    </div>
    <a class="headerlink" href="#main"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="main-738"><a href="#main-738"><span class="linenos">738</span></a><span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="main-739"><a href="#main-739"><span class="linenos">739</span></a>	<span class="s2">&quot;generate attention pattern activations for a model and prompts&quot;</span>
</span><span id="main-740"><a href="#main-740"><span class="linenos">740</span></a>	<span class="nb">print</span><span class="p">(</span><span class="n">DIVIDER_S1</span><span class="p">)</span>
</span><span id="main-741"><a href="#main-741"><span class="linenos">741</span></a>	<span class="k">with</span> <span class="n">SpinnerContext</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;parsing args&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">SPINNER_KWARGS</span><span class="p">):</span>
</span><span id="main-742"><a href="#main-742"><span class="linenos">742</span></a>		<span class="n">arg_parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
</span><span id="main-743"><a href="#main-743"><span class="linenos">743</span></a>		<span class="c1"># input and output</span>
</span><span id="main-744"><a href="#main-744"><span class="linenos">744</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-745"><a href="#main-745"><span class="linenos">745</span></a>			<span class="s2">&quot;--model&quot;</span><span class="p">,</span>
</span><span id="main-746"><a href="#main-746"><span class="linenos">746</span></a>			<span class="s2">&quot;-m&quot;</span><span class="p">,</span>
</span><span id="main-747"><a href="#main-747"><span class="linenos">747</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span><span id="main-748"><a href="#main-748"><span class="linenos">748</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="main-749"><a href="#main-749"><span class="linenos">749</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The model name(s) to use. comma separated with no whitespace if multiple&quot;</span><span class="p">,</span>
</span><span id="main-750"><a href="#main-750"><span class="linenos">750</span></a>		<span class="p">)</span>
</span><span id="main-751"><a href="#main-751"><span class="linenos">751</span></a>
</span><span id="main-752"><a href="#main-752"><span class="linenos">752</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-753"><a href="#main-753"><span class="linenos">753</span></a>			<span class="s2">&quot;--prompts&quot;</span><span class="p">,</span>
</span><span id="main-754"><a href="#main-754"><span class="linenos">754</span></a>			<span class="s2">&quot;-p&quot;</span><span class="p">,</span>
</span><span id="main-755"><a href="#main-755"><span class="linenos">755</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span><span id="main-756"><a href="#main-756"><span class="linenos">756</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="main-757"><a href="#main-757"><span class="linenos">757</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The path to the prompts file (jsonl with &#39;text&#39; key on each line). If `None`, expects that `--figures` is passed and will generate figures for all prompts in the model directory&quot;</span><span class="p">,</span>
</span><span id="main-758"><a href="#main-758"><span class="linenos">758</span></a>			<span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="main-759"><a href="#main-759"><span class="linenos">759</span></a>		<span class="p">)</span>
</span><span id="main-760"><a href="#main-760"><span class="linenos">760</span></a>
</span><span id="main-761"><a href="#main-761"><span class="linenos">761</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-762"><a href="#main-762"><span class="linenos">762</span></a>			<span class="s2">&quot;--save-path&quot;</span><span class="p">,</span>
</span><span id="main-763"><a href="#main-763"><span class="linenos">763</span></a>			<span class="s2">&quot;-s&quot;</span><span class="p">,</span>
</span><span id="main-764"><a href="#main-764"><span class="linenos">764</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span><span id="main-765"><a href="#main-765"><span class="linenos">765</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="main-766"><a href="#main-766"><span class="linenos">766</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The path to save the attention patterns&quot;</span><span class="p">,</span>
</span><span id="main-767"><a href="#main-767"><span class="linenos">767</span></a>			<span class="n">default</span><span class="o">=</span><span class="n">DATA_DIR</span><span class="p">,</span>
</span><span id="main-768"><a href="#main-768"><span class="linenos">768</span></a>		<span class="p">)</span>
</span><span id="main-769"><a href="#main-769"><span class="linenos">769</span></a>
</span><span id="main-770"><a href="#main-770"><span class="linenos">770</span></a>		<span class="c1"># min and max prompt lengths</span>
</span><span id="main-771"><a href="#main-771"><span class="linenos">771</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-772"><a href="#main-772"><span class="linenos">772</span></a>			<span class="s2">&quot;--min-chars&quot;</span><span class="p">,</span>
</span><span id="main-773"><a href="#main-773"><span class="linenos">773</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
</span><span id="main-774"><a href="#main-774"><span class="linenos">774</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="main-775"><a href="#main-775"><span class="linenos">775</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The minimum number of characters for a prompt&quot;</span><span class="p">,</span>
</span><span id="main-776"><a href="#main-776"><span class="linenos">776</span></a>			<span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span><span id="main-777"><a href="#main-777"><span class="linenos">777</span></a>		<span class="p">)</span>
</span><span id="main-778"><a href="#main-778"><span class="linenos">778</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-779"><a href="#main-779"><span class="linenos">779</span></a>			<span class="s2">&quot;--max-chars&quot;</span><span class="p">,</span>
</span><span id="main-780"><a href="#main-780"><span class="linenos">780</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
</span><span id="main-781"><a href="#main-781"><span class="linenos">781</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="main-782"><a href="#main-782"><span class="linenos">782</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The maximum number of characters for a prompt&quot;</span><span class="p">,</span>
</span><span id="main-783"><a href="#main-783"><span class="linenos">783</span></a>			<span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
</span><span id="main-784"><a href="#main-784"><span class="linenos">784</span></a>		<span class="p">)</span>
</span><span id="main-785"><a href="#main-785"><span class="linenos">785</span></a>
</span><span id="main-786"><a href="#main-786"><span class="linenos">786</span></a>		<span class="c1"># number of samples</span>
</span><span id="main-787"><a href="#main-787"><span class="linenos">787</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-788"><a href="#main-788"><span class="linenos">788</span></a>			<span class="s2">&quot;--n-samples&quot;</span><span class="p">,</span>
</span><span id="main-789"><a href="#main-789"><span class="linenos">789</span></a>			<span class="s2">&quot;-n&quot;</span><span class="p">,</span>
</span><span id="main-790"><a href="#main-790"><span class="linenos">790</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
</span><span id="main-791"><a href="#main-791"><span class="linenos">791</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="main-792"><a href="#main-792"><span class="linenos">792</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The max number of samples to process, do all in the file if None&quot;</span><span class="p">,</span>
</span><span id="main-793"><a href="#main-793"><span class="linenos">793</span></a>			<span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="main-794"><a href="#main-794"><span class="linenos">794</span></a>		<span class="p">)</span>
</span><span id="main-795"><a href="#main-795"><span class="linenos">795</span></a>
</span><span id="main-796"><a href="#main-796"><span class="linenos">796</span></a>		<span class="c1"># batch size</span>
</span><span id="main-797"><a href="#main-797"><span class="linenos">797</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-798"><a href="#main-798"><span class="linenos">798</span></a>			<span class="s2">&quot;--batch-size&quot;</span><span class="p">,</span>
</span><span id="main-799"><a href="#main-799"><span class="linenos">799</span></a>			<span class="s2">&quot;-b&quot;</span><span class="p">,</span>
</span><span id="main-800"><a href="#main-800"><span class="linenos">800</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
</span><span id="main-801"><a href="#main-801"><span class="linenos">801</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="main-802"><a href="#main-802"><span class="linenos">802</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;Batch size for computing activations (number of prompts per forward pass)&quot;</span><span class="p">,</span>
</span><span id="main-803"><a href="#main-803"><span class="linenos">803</span></a>			<span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
</span><span id="main-804"><a href="#main-804"><span class="linenos">804</span></a>		<span class="p">)</span>
</span><span id="main-805"><a href="#main-805"><span class="linenos">805</span></a>
</span><span id="main-806"><a href="#main-806"><span class="linenos">806</span></a>		<span class="c1"># force overwrite</span>
</span><span id="main-807"><a href="#main-807"><span class="linenos">807</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-808"><a href="#main-808"><span class="linenos">808</span></a>			<span class="s2">&quot;--force&quot;</span><span class="p">,</span>
</span><span id="main-809"><a href="#main-809"><span class="linenos">809</span></a>			<span class="s2">&quot;-f&quot;</span><span class="p">,</span>
</span><span id="main-810"><a href="#main-810"><span class="linenos">810</span></a>			<span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
</span><span id="main-811"><a href="#main-811"><span class="linenos">811</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;If passed, will overwrite existing files&quot;</span><span class="p">,</span>
</span><span id="main-812"><a href="#main-812"><span class="linenos">812</span></a>		<span class="p">)</span>
</span><span id="main-813"><a href="#main-813"><span class="linenos">813</span></a>
</span><span id="main-814"><a href="#main-814"><span class="linenos">814</span></a>		<span class="c1"># no index html</span>
</span><span id="main-815"><a href="#main-815"><span class="linenos">815</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-816"><a href="#main-816"><span class="linenos">816</span></a>			<span class="s2">&quot;--no-index-html&quot;</span><span class="p">,</span>
</span><span id="main-817"><a href="#main-817"><span class="linenos">817</span></a>			<span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
</span><span id="main-818"><a href="#main-818"><span class="linenos">818</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;If passed, will not write an index.html file for the model&quot;</span><span class="p">,</span>
</span><span id="main-819"><a href="#main-819"><span class="linenos">819</span></a>		<span class="p">)</span>
</span><span id="main-820"><a href="#main-820"><span class="linenos">820</span></a>
</span><span id="main-821"><a href="#main-821"><span class="linenos">821</span></a>		<span class="c1"># raw prompts</span>
</span><span id="main-822"><a href="#main-822"><span class="linenos">822</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-823"><a href="#main-823"><span class="linenos">823</span></a>			<span class="s2">&quot;--raw-prompts&quot;</span><span class="p">,</span>
</span><span id="main-824"><a href="#main-824"><span class="linenos">824</span></a>			<span class="s2">&quot;-r&quot;</span><span class="p">,</span>
</span><span id="main-825"><a href="#main-825"><span class="linenos">825</span></a>			<span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
</span><span id="main-826"><a href="#main-826"><span class="linenos">826</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;pass if the prompts have not been split and tokenized (still needs keys &#39;text&#39; and &#39;meta&#39; for each item)&quot;</span><span class="p">,</span>
</span><span id="main-827"><a href="#main-827"><span class="linenos">827</span></a>		<span class="p">)</span>
</span><span id="main-828"><a href="#main-828"><span class="linenos">828</span></a>
</span><span id="main-829"><a href="#main-829"><span class="linenos">829</span></a>		<span class="c1"># shuffle</span>
</span><span id="main-830"><a href="#main-830"><span class="linenos">830</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-831"><a href="#main-831"><span class="linenos">831</span></a>			<span class="s2">&quot;--shuffle&quot;</span><span class="p">,</span>
</span><span id="main-832"><a href="#main-832"><span class="linenos">832</span></a>			<span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
</span><span id="main-833"><a href="#main-833"><span class="linenos">833</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;If passed, will shuffle the prompts&quot;</span><span class="p">,</span>
</span><span id="main-834"><a href="#main-834"><span class="linenos">834</span></a>		<span class="p">)</span>
</span><span id="main-835"><a href="#main-835"><span class="linenos">835</span></a>
</span><span id="main-836"><a href="#main-836"><span class="linenos">836</span></a>		<span class="c1"># stack heads</span>
</span><span id="main-837"><a href="#main-837"><span class="linenos">837</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-838"><a href="#main-838"><span class="linenos">838</span></a>			<span class="s2">&quot;--stacked-heads&quot;</span><span class="p">,</span>
</span><span id="main-839"><a href="#main-839"><span class="linenos">839</span></a>			<span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
</span><span id="main-840"><a href="#main-840"><span class="linenos">840</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;If passed, will stack the heads in the output tensor&quot;</span><span class="p">,</span>
</span><span id="main-841"><a href="#main-841"><span class="linenos">841</span></a>		<span class="p">)</span>
</span><span id="main-842"><a href="#main-842"><span class="linenos">842</span></a>
</span><span id="main-843"><a href="#main-843"><span class="linenos">843</span></a>		<span class="c1"># device</span>
</span><span id="main-844"><a href="#main-844"><span class="linenos">844</span></a>		<span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span><span id="main-845"><a href="#main-845"><span class="linenos">845</span></a>			<span class="s2">&quot;--device&quot;</span><span class="p">,</span>
</span><span id="main-846"><a href="#main-846"><span class="linenos">846</span></a>			<span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
</span><span id="main-847"><a href="#main-847"><span class="linenos">847</span></a>			<span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="main-848"><a href="#main-848"><span class="linenos">848</span></a>			<span class="n">help</span><span class="o">=</span><span class="s2">&quot;The device to use for the model&quot;</span><span class="p">,</span>
</span><span id="main-849"><a href="#main-849"><span class="linenos">849</span></a>			<span class="n">default</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
</span><span id="main-850"><a href="#main-850"><span class="linenos">850</span></a>		<span class="p">)</span>
</span><span id="main-851"><a href="#main-851"><span class="linenos">851</span></a>
</span><span id="main-852"><a href="#main-852"><span class="linenos">852</span></a>		<span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span> <span class="o">=</span> <span class="n">arg_parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span><span id="main-853"><a href="#main-853"><span class="linenos">853</span></a>
</span><span id="main-854"><a href="#main-854"><span class="linenos">854</span></a>	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;args parsed: </span><span class="si">{</span><span class="n">args</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="main-855"><a href="#main-855"><span class="linenos">855</span></a>
</span><span id="main-856"><a href="#main-856"><span class="linenos">856</span></a>	<span class="n">models</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
</span><span id="main-857"><a href="#main-857"><span class="linenos">857</span></a>	<span class="k">if</span> <span class="s2">&quot;,&quot;</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">:</span>
</span><span id="main-858"><a href="#main-858"><span class="linenos">858</span></a>		<span class="n">models</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
</span><span id="main-859"><a href="#main-859"><span class="linenos">859</span></a>	<span class="k">else</span><span class="p">:</span>
</span><span id="main-860"><a href="#main-860"><span class="linenos">860</span></a>		<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">]</span>
</span><span id="main-861"><a href="#main-861"><span class="linenos">861</span></a>
</span><span id="main-862"><a href="#main-862"><span class="linenos">862</span></a>	<span class="n">n_models</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
</span><span id="main-863"><a href="#main-863"><span class="linenos">863</span></a>	<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
</span><span id="main-864"><a href="#main-864"><span class="linenos">864</span></a>		<span class="nb">print</span><span class="p">(</span><span class="n">DIVIDER_S2</span><span class="p">)</span>
</span><span id="main-865"><a href="#main-865"><span class="linenos">865</span></a>		<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;processing model </span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">n_models</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="main-866"><a href="#main-866"><span class="linenos">866</span></a>		<span class="nb">print</span><span class="p">(</span><span class="n">DIVIDER_S2</span><span class="p">)</span>
</span><span id="main-867"><a href="#main-867"><span class="linenos">867</span></a>
</span><span id="main-868"><a href="#main-868"><span class="linenos">868</span></a>		<span class="n">activations_main</span><span class="p">(</span>
</span><span id="main-869"><a href="#main-869"><span class="linenos">869</span></a>			<span class="n">model_name</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
</span><span id="main-870"><a href="#main-870"><span class="linenos">870</span></a>			<span class="n">save_path</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span>
</span><span id="main-871"><a href="#main-871"><span class="linenos">871</span></a>			<span class="n">prompts_path</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">prompts</span><span class="p">,</span>
</span><span id="main-872"><a href="#main-872"><span class="linenos">872</span></a>			<span class="n">raw_prompts</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">raw_prompts</span><span class="p">,</span>
</span><span id="main-873"><a href="#main-873"><span class="linenos">873</span></a>			<span class="n">min_chars</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">min_chars</span><span class="p">,</span>
</span><span id="main-874"><a href="#main-874"><span class="linenos">874</span></a>			<span class="n">max_chars</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_chars</span><span class="p">,</span>
</span><span id="main-875"><a href="#main-875"><span class="linenos">875</span></a>			<span class="n">force</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">force</span><span class="p">,</span>
</span><span id="main-876"><a href="#main-876"><span class="linenos">876</span></a>			<span class="n">n_samples</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_samples</span><span class="p">,</span>
</span><span id="main-877"><a href="#main-877"><span class="linenos">877</span></a>			<span class="n">no_index_html</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">no_index_html</span><span class="p">,</span>
</span><span id="main-878"><a href="#main-878"><span class="linenos">878</span></a>			<span class="n">shuffle</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">shuffle</span><span class="p">,</span>
</span><span id="main-879"><a href="#main-879"><span class="linenos">879</span></a>			<span class="n">stacked_heads</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">stacked_heads</span><span class="p">,</span>
</span><span id="main-880"><a href="#main-880"><span class="linenos">880</span></a>			<span class="n">device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
</span><span id="main-881"><a href="#main-881"><span class="linenos">881</span></a>			<span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
</span><span id="main-882"><a href="#main-882"><span class="linenos">882</span></a>		<span class="p">)</span>
</span><span id="main-883"><a href="#main-883"><span class="linenos">883</span></a>		<span class="k">del</span> <span class="n">model</span>
</span><span id="main-884"><a href="#main-884"><span class="linenos">884</span></a>
</span><span id="main-885"><a href="#main-885"><span class="linenos">885</span></a>	<span class="nb">print</span><span class="p">(</span><span class="n">DIVIDER_S1</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>generate attention pattern activations for a model and prompts</p>
</div>


                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>